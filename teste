import streamlit as st
import pandas as pd
import json
import re
from datetime import datetime, timedelta
import numpy as np
from typing import Dict, List, Any, Optional
import time
import warnings
import os
import requests

# Optional imports with fallbacks
try:
    from streamlit_echarts import st_echarts
    ECHARTS_AVAILABLE = True
except ImportError:
    ECHARTS_AVAILABLE = False
    st_echarts = None

try:
    from databricks import sql as dbsql
    DATABRICKS_AVAILABLE = True
except ImportError:
    DATABRICKS_AVAILABLE = False

try:
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
except ImportError:
    pass

# AWS Bedrock dispon√≠vel
AWS_AVAILABLE = True

# Constantes de configura√ß√£o
MAX_ROWS_FOR_ANALYSIS = 10000  # Limite de linhas para an√°lise
MAX_RETRIES_DEFAULT = 2  # Tentativas padr√£o para queries
MAX_SQL_LENGTH = 50000  # Tamanho m√°ximo de SQL permitido
REQUEST_TIMEOUT = 120  # Timeout para requisi√ß√µes em segundos
MAX_EXPLORATION_COLUMNS = 30  # Limite de colunas para explora√ß√£o

try:
    from app.tools.vector_db_tool import clear_all_documents
    VECTOR_DB_AVAILABLE = True
except ImportError:
    VECTOR_DB_AVAILABLE = False

try:
    from pygwalker.api.streamlit import init_streamlit_comm, get_streamlit_html
    import streamlit.components.v1 as components
    PYGWALKER_AVAILABLE = True
    # Suppress PyGWalker deprecation warnings
    warnings.filterwarnings("ignore", message=".*st.experimental_user.*")
except ImportError:
    PYGWALKER_AVAILABLE = False

# CLASSE BEDROCK CLIENT INTEGRADA
class BedrockClient:
    """Cliente para comunica√ß√£o com AWS Bedrock usando Bearer Token.
    
    Args:
        bearer_token: Token Bearer para autentica√ß√£o (priorit√°rio sobre vari√°vel de ambiente)
        
    Raises:
        ValueError: Se o token n√£o for fornecido ou for inv√°lido
    """
    
    def __init__(self, bearer_token: Optional[str] = None) -> None:
        # Prioridade: 1) par√¢metro direto, 2) vari√°vel de ambiente
        self.bearer_token = (bearer_token or os.getenv('AWS_BEARER_TOKEN_BEDROCK', '')).strip()
        self.model_id = "anthropic.claude-sonnet-4-20250514-v1:0"
        self.region = "us-east-1"
        self.base_url = "https://bedrock-runtime.us-east-1.amazonaws.com"
        
        # Valida√ß√µes de seguran√ßa
        if not self.bearer_token:
            raise ValueError(
                "Token AWS Bedrock n√£o configurado. "
                "Configure na aba 'Configura√ß√£o AWS Bedrock' no menu lateral."
            )
        
        if len(self.bearer_token) < 20:  # Valida√ß√£o b√°sica de tamanho m√≠nimo
            raise ValueError("Token AWS Bedrock parece inv√°lido (muito curto)")
    
    def invoke_model(self, modelId: str, body: str, **kwargs) -> Dict[str, Any]:
        """Invoca modelo no Bedrock via API REST usando Bearer Token.
        
        Args:
            modelId: ID do modelo a ser invocado
            body: Corpo da requisi√ß√£o (JSON string ou dict)
            **kwargs: Argumentos adicionais (n√£o utilizados atualmente)
            
        Returns:
            Resposta da API no formato compat√≠vel com boto3
            
        Raises:
            ValueError: Se o modelId ou body forem inv√°lidos
            requests.exceptions.RequestException: Erros de rede/API
        """
        # Valida√ß√µes de entrada
        if not modelId or not isinstance(modelId, str):
            raise ValueError("modelId deve ser uma string n√£o vazia")
        
        if not body:
            raise ValueError("body n√£o pode ser vazio")
        
        print(f"[BEDROCK] ü§ñ Invocando modelo: {modelId}")
        
        invoke_url = f"{self.base_url}/model/{modelId}/invoke"
        
        headers = {
            "Content-Type": "application/json",
            "Accept": "application/json",
            "Authorization": f"Bearer {self.bearer_token}",
            "User-Agent": "BRIAN-Streamlit/1.0"
        }
        
        try:
            # Parse body se for string
            if isinstance(body, str):
                try:
                    payload = json.loads(body)
                except json.JSONDecodeError as e:
                    raise ValueError(f"Body JSON inv√°lido: {str(e)}")
            else:
                payload = body
            
            # Validar tamanho do payload
            payload_size = len(json.dumps(payload))
            if payload_size > MAX_SQL_LENGTH:
                print(f"[BEDROCK] ‚ö†Ô∏è Payload muito grande: {payload_size} bytes")
            
            print(f"[BEDROCK] üì§ Enviando requisi√ß√£o (tamanho: {payload_size} bytes)...")
            response = requests.post(
                invoke_url,
                json=payload,
                headers=headers,
                timeout=REQUEST_TIMEOUT
            )
            
            print(f"[BEDROCK] üì• Status: {response.status_code}")
            
            if response.status_code == 200:
                try:
                    response_data = response.json()
                except json.JSONDecodeError as e:
                    print(f"[BEDROCK] ‚ùå Erro ao decodificar resposta JSON: {str(e)}")
                    raise ValueError(f"Resposta inv√°lida do servidor: {str(e)}")
                
                # Simular formato boto3
                return {
                    'body': MockResponseBody(response_data),
                    'ResponseMetadata': {
                        'HTTPStatusCode': 200
                    }
                }
            else:
                error_msg = f"HTTP {response.status_code}: {response.text[:500]}"  # Limitar tamanho
                print(f"[BEDROCK] ‚ùå Erro na API: {error_msg}")
                raise Exception(error_msg)
                
        except requests.exceptions.Timeout:
            print(f"[BEDROCK] ‚ùå Timeout ap√≥s {REQUEST_TIMEOUT}s")
            raise Exception(f"Timeout na requisi√ß√£o (>{REQUEST_TIMEOUT}s)")
        except requests.exceptions.RequestException as e:
            print(f"[BEDROCK] ‚ùå Erro de rede: {str(e)}")
            raise Exception(f"Erro de comunica√ß√£o com AWS Bedrock: {str(e)}")
        except Exception as e:
            print(f"[BEDROCK] ‚ùå Erro inesperado: {str(e)}")
            raise

class MockResponseBody:
    """Mock da resposta body do boto3 para compatibilidade"""
    def __init__(self, data):
        self.data = data
    
    def read(self):
        return json.dumps(self.data).encode()

def get_bedrock_client():
    """
    Retorna cliente Bedrock configurado com Bearer Token do session_state
    """
    try:
        print(f"[BEDROCK] üîß Inicializando cliente Bedrock...")
        
        # Obt√©m o token do session_state (configurado no frontend)
        bearer_token = st.session_state.get('aws_bedrock_token', '').strip()
        
        if not bearer_token:
            st.error("‚ùå Token AWS Bedrock n√£o configurado. Configure na aba 'üîë Configura√ß√£o AWS Bedrock' no menu lateral.")
            return None
        
        print(f"[BEDROCK] üîë Usando token do frontend (session_state)")
        client = BedrockClient(bearer_token=bearer_token)
        print(f"[BEDROCK] ‚úÖ Cliente inicializado com sucesso")
        return client
    except Exception as e:
        print(f"[BEDROCK] ‚ùå Erro ao inicializar cliente: {str(e)}")
        st.error(f"‚ùå Erro na configura√ß√£o do Bedrock: {str(e)}")
        return None


def refine_query_with_additional_context(original_user_query: str, original_sql: str, current_df: pd.DataFrame, additional_context: str, bedrock_client, get_context_func, exploration_cache) -> Dict[str, Any]:
    """
    NOVA FUN√á√ÉO DE REFINAMENTO - Gera nova SQL considerando:
    - Pergunta original do usu√°rio
    - SQL anterior que foi executada
    - Dados atuais obtidos
    - Contexto/pedidos adicionais do usu√°rio
    """
    print(f"[REFINE] üîß Iniciando refinamento de query...")

    if not bedrock_client:
        return {
            "refined_sql": original_sql,
            "explanation": "Bedrock n√£o dispon√≠vel para refinamento.",
            "changes": []
        }

    data_summary = f"Linhas retornadas: {len(current_df)}\n"
    data_summary += f"Colunas: {', '.join(current_df.columns.tolist())}\n"
    if not current_df.empty:
        data_summary += f"\nAmostra dos dados (primeiras 3 linhas):\n{current_df.head(3).to_string()}\n"
        try:
            data_summary += f"\nEstat√≠sticas num√©ricas:\n{current_df.describe().to_string()}"
        except Exception:
            pass

    exploration_context = ""
    if exploration_cache:
        if exploration_cache.get('flag_columns'):
            exploration_context += "\nüö© FLAGS DISPON√çVEIS:\n"
            for col, values in list(exploration_cache['flag_columns'].items())[:10]:
                exploration_context += f"  - {col}: {values}\n"

        if exploration_cache.get('description_columns'):
            exploration_context += "\nüìù DESCRI√á√ïES DISPON√çVEIS:\n"
            for col, values in list(exploration_cache['description_columns'].items())[:10]:
                exploration_context += f"  - {col}: {len(values)} valores | Exemplos: {values[:3]}\n"

        if exploration_cache.get('code_columns'):
            exploration_context += "\nüî¢ C√ìDIGOS DISPON√çVEIS:\n"
            for col, values in list(exploration_cache['code_columns'].items())[:10]:
                exploration_context += f"  - {col}: {len(values)} valores | Exemplos: {values[:3]}\n"

    prompt = f"""
Voc√™ √© um ESPECIALISTA em SQL para Databricks.

Tarefa: REFINAR a SQL existente considerando pedidos adicionais do usu√°rio.

üìã CONTEXTO DA TABELA:
{get_context_func()}
{exploration_context}

üìä CONSULTA ORIGINAL:
Pergunta do usu√°rio: {original_user_query}

SQL anterior:
{original_sql}

üìà RESULTADOS ATUAIS:
{data_summary}

‚ûï PEDIDO ADICIONAL DO USU√ÅRIO:
{additional_context}

üéØ SUA TAREFA:
1. Analise a SQL anterior e os resultados obtidos
2. Incorpore o pedido adicional do usu√°rio
3. Gere uma NOVA SQL que mant√©m a consulta original MAS adiciona/modifica conforme solicitado
4. Exemplos de refinamento:
   - "adicionar filtro por per√≠odo" ‚Üí adiciona WHERE com data
   - "agrupar por categoria" ‚Üí adiciona GROUP BY
   - "incluir apenas ativos" ‚Üí adiciona filtro flag_ativo = 1
   - "√∫ltimos 30 dias" ‚Üí adiciona filtro de data

Responda APENAS em JSON:
{{
  "refined_sql": "<SQL completa e refinada pronta para executar>",
  "explanation": "<explica√ß√£o curta do que foi modificado>",
  "changes": ["<mudan√ßa 1>", "<mudan√ßa 2>"]
}}

REGRAS:
- A SQL deve ser COMPLETA (n√£o incremental)
- Use sintaxe Databricks SQL
- Use APENAS colunas que existem no contexto
- Use valores exatos da explora√ß√£o quando dispon√≠vel
- N√£o retorne texto fora do JSON
"""

    try:
        print(f"[REFINE] üì§ Enviando para Bedrock...")
        response = bedrock_client.invoke_model(
            modelId="us.anthropic.claude-sonnet-4-20250514-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 8192,
                "temperature": 0.3,
                "messages": [{"role": "user", "content": prompt}]
            })
        )

        response_body = json.loads(response['body'].read())
        response_text = response_body['content'][0]['text'].strip()

        json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response_text, re.DOTALL)
        if json_match:
            result = json.loads(json_match.group())
            print(f"[REFINE] ‚úÖ SQL refinada gerada com sucesso")
            return result
        result = json.loads(response_text)
        return result
    except Exception as e:
        print(f"[REFINE] ‚ùå Erro: {str(e)}")
        return {
            "refined_sql": original_sql,
            "explanation": f"Erro ao refinar: {str(e)}",
            "changes": []
        }


def regenerate_analysis_with_feedback(analysis_type: str, original_query: str, df: pd.DataFrame, original_analysis: str, user_feedback: str, bedrock_client) -> Dict[str, Any]:
    """
    NOVA FUN√á√ÉO DE REPORTAR PROBLEMA - Regenera an√°lise considerando feedback do usu√°rio.
    """
    print(f"[FEEDBACK] üîç Regenerando an√°lise com feedback do usu√°rio...")

    if not bedrock_client:
        return {
            "corrected_analysis": original_analysis,
            "explanation": "Bedrock n√£o dispon√≠vel.",
            "issues_found": []
        }

    data_summary = f"Linhas: {len(df)}, Colunas: {len(df.columns)}\n"
    if not df.empty:
        data_summary += f"\nAmostra dos dados:\n{df.head(5).to_string()}\n"
        try:
            data_summary += f"\nEstat√≠sticas:\n{df.describe().to_string()}"
        except Exception:
            pass

    analysis_names = {
        "direct": "Resposta Direta √† Pergunta",
        "additional": "An√°lises Adicionais + Resumo Executivo",
        "causes": "Causas Prov√°veis + Novas An√°lises"
    }
    analysis_name = analysis_names.get(analysis_type, "An√°lise")

    prompt = f"""
Voc√™ √© um ANALISTA DE DADOS SENIOR.

Tarefa: CORRIGIR uma an√°lise baseada no feedback do usu√°rio.

üìä CONTEXTO:
Pergunta original: {original_query}

Tipo de an√°lise: {analysis_name}

Dados dispon√≠veis:
{data_summary}

üìù AN√ÅLISE GERADA ANTERIORMENTE:
{original_analysis}

üêõ PROBLEMA REPORTADO PELO USU√ÅRIO:
{user_feedback}

üéØ SUA TAREFA:
1. Identifique o problema espec√≠fico mencionado pelo usu√°rio
2. Analise por que a an√°lise anterior estava incorreta ou incompleta
3. Gere uma NOVA an√°lise CORRIGIDA que resolve o problema
4. A nova an√°lise deve:
   - Ser completa e standalone (n√£o dizer "vou corrigir...")
   - Usar os dados reais do DataFrame
   - Ser factual e precisa
   - Abordar diretamente o problema reportado

Responda APENAS em JSON:
{{
  "corrected_analysis": "<an√°lise completa e corrigida em markdown>",
  "explanation": "<explica√ß√£o curta do que foi corrigido>",
  "issues_found": ["<problema 1>", "<problema 2>"]
}}

N√£o retorne texto fora do JSON.
"""

    try:
        print(f"[FEEDBACK] üì§ Enviando para Bedrock...")
        response = bedrock_client.invoke_model(
            modelId="us.anthropic.claude-sonnet-4-20250514-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 8192,
                "temperature": 0.5,
                "messages": [{"role": "user", "content": prompt}]
            })
        )

        response_body = json.loads(response['body'].read())
        response_text = response_body['content'][0]['text'].strip()

        json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response_text, re.DOTALL)
        if json_match:
            result = json.loads(json_match.group())
            print(f"[FEEDBACK] ‚úÖ An√°lise corrigida gerada")
            return result
        result = json.loads(response_text)
        return result
    except Exception as e:
        print(f"[FEEDBACK] ‚ùå Erro: {str(e)}")
        return {
            "corrected_analysis": original_analysis,
            "explanation": f"Erro ao regenerar: {str(e)}",
            "issues_found": []
        }


# ============================================
# ECHARTS VISUALIZATION FUNCTIONS
# ============================================

def _convert_to_native(value):
    """Converte valores para tipos Python nativos (int, float, str)"""
    from decimal import Decimal
    import numpy as np
    
    if pd.isna(value) or value is None:
        return 0
    if isinstance(value, (Decimal, np.integer, np.floating)):
        return float(value)
    if isinstance(value, (int, float)):
        return value
    if isinstance(value, str):
        v = value.strip()
        # Remover % e substituir v√≠rgula por ponto
        v_clean = v.replace('%', '').replace(',', '.').strip()
        try:
            return float(v_clean)
        except ValueError:
            return value
    return str(value)

def _convert_series_to_native(series):
    """Converte s√©rie pandas para lista de tipos Python nativos"""
    return [_convert_to_native(v) for v in series]


def _extract_colors_from_query(query_str: str, default: str = "#5470c6") -> str:
    """Extrai cor mencionada no texto (nome ou hex)."""
    import re
    query_lower = query_str.lower()
    hex_match = re.search(r"#(?:[0-9a-fA-F]{6}|[0-9a-fA-F]{3})", query_str)
    if hex_match:
        return hex_match.group(0)
    color_map = {
        'vermelho': '#FF0000', 'vermelha': '#FF0000', 'red': '#FF0000',
        'azul': '#0000FF', 'azuis': '#0000FF', 'blue': '#0000FF',
        'verde': '#00AA00', 'green': '#00AA00',
        'amarelo': '#FFFF00', 'amarela': '#FFFF00', 'yellow': '#FFFF00',
        'laranja': '#FFA500', 'orange': '#FFA500',
        'roxo': '#800080', 'roxa': '#800080', 'purple': '#800080',
        'rosa': '#FFC0CB', 'pink': '#FFC0CB',
        'marrom': '#8B4513', 'brown': '#8B4513',
        'cinza': '#808080', 'cizento': '#808080', 'gray': '#808080', 'grey': '#808080',
        'preto': '#000000', 'preta': '#000000', 'black': '#000000',
    }
    for color_name, hex_code in color_map.items():
        if color_name in query_lower:
            return hex_code
    return default


def _add_save_as_image(option: Dict[str, Any]) -> Dict[str, Any]:
    """Garante toolbox com saveAsImage habilitado."""
    if not isinstance(option, dict):
        return option
    toolbox = option.setdefault("toolbox", {})
    feature = toolbox.setdefault("feature", {})
    feature.setdefault("saveAsImage", {"show": True})
    return option




# ============================================
# AI-GENERATED ECHARTS OPTION (SANITIZE/VALIDATE)
# ============================================

def _sanitize_option_types(obj):
    """Sanitiza qualquer estrutura para tipos nativos JSON-safe."""
    if isinstance(obj, dict):
        return {k: _sanitize_option_types(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [_sanitize_option_types(v) for v in obj]
    return _convert_to_native(obj)


def _enforce_label_visibility(option: Dict[str, Any], user_color: str = "#5470c6") -> Dict[str, Any]:
    """Garante r√≥tulos/eixos vis√≠veis e aplica cor padr√£o/usu√°rio nas s√©ries se faltarem."""
    def ensure_axis(axis_obj):
        if isinstance(axis_obj, list):
            return [ensure_axis(a) for a in axis_obj]
        if not isinstance(axis_obj, dict):
            return axis_obj
        axis_obj.setdefault("axisLabel", {})
        axis_obj["axisLabel"].setdefault("show", True)
        axis_obj["axisLabel"].setdefault("color", "#fff")
        axis_obj.setdefault("axisLine", {"show": True, "lineStyle": {"color": "#ffffff"}})
        axis_obj.setdefault("axisTick", {"show": True, "lineStyle": {"color": "#ffffff"}})
        axis_obj.setdefault("splitLine", {"lineStyle": {"color": "#444"}})
        return axis_obj

    if "xAxis" in option:
        option["xAxis"] = ensure_axis(option["xAxis"])
    if "yAxis" in option:
        option["yAxis"] = ensure_axis(option["yAxis"])

    if "series" in option and isinstance(option["series"], list):
        for s in option["series"]:
            if not isinstance(s, dict):
                continue
            s.setdefault("label", {})
            s["label"].setdefault("show", True)
            s["label"].setdefault("formatter", "{c}")
            s["label"].setdefault("color", "#ffffff")
            s.setdefault("labelLayout", {})
            s["labelLayout"].setdefault("hideOverlap", False)
            s.setdefault("itemStyle", {})
            if "color" not in s["itemStyle"] and user_color:
                s["itemStyle"]["color"] = user_color
    return option


def _validate_echarts_option(option: Any) -> Dict[str, Any]:
    """Valida estrutura b√°sica de um option ECharts. Retorna dict v√°lido ou {}."""
    if not isinstance(option, dict):
        return {}

    allowed_series = {"bar", "line", "scatter", "pie", "funnel", "radar", "heatmap", "boxplot", "candlestick", "tree", "treemap", "sunburst"}
    series = option.get("series")
    if not isinstance(series, list) or not series:
        return {}
    clean_series = []
    for s in series:
        if not isinstance(s, dict):
            continue
        s_type = s.get("type")
        if s_type not in allowed_series:
            continue
        clean_series.append(s)
    if not clean_series:
        return {}
    option["series"] = clean_series
    return option


def _build_basic_echarts_option(
    df: pd.DataFrame,
    chart_type: str,
    x_col: Optional[str],
    y_cols: List[str],
    agg: str = "sum",
    color: str = "#5470c6",
) -> Dict[str, Any]:
    """Cria option ECharts simples sem usar IA, respeitando colunas do usu√°rio."""

    if df is None or df.empty or not x_col or not y_cols:
        return {}

    dff = df.copy()
    if agg in {"sum", "avg", "count"}:
        agg_map = {"sum": "sum", "avg": "mean", "count": "count"}
        dff = (
            dff.groupby(x_col)[y_cols]
            .agg(agg_map[agg])
            .reset_index()
        )

    x_values = _convert_series_to_native(dff[x_col])
    option: Dict[str, Any] = {
        "tooltip": {"trigger": "axis"},
        "legend": {"type": "scroll"},
        "xAxis": {"type": "category", "data": x_values},
        "yAxis": {"type": "value"},
        "series": [],
    }

    # Mapear tipo textual para ECharts
    chart_map = {
        "barras": "bar",
        "linhas": "line",
        "area": "line",
        "pizza": "pie",
        "dispersao": "scatter",
        "funil": "funnel",
        "combo": "combo",
    }
    kind = chart_map.get(chart_type.lower(), chart_type.lower())

    for idx, y in enumerate(y_cols):
        y_values = _convert_series_to_native(dff[y])
        series_color = color if idx == 0 else None

        if kind == "pie":
            option["tooltip"] = {"trigger": "item"}
            option.pop("xAxis", None)
            option.pop("yAxis", None)
            option["series"] = [
                {
                    "name": y,
                    "type": "pie",
                    "radius": ["40%", "70%"],
                    "avoidLabelOverlap": False,
                    "itemStyle": {"borderRadius": 6, "borderColor": "#fff", "borderWidth": 2},
                    "label": {"show": True, "formatter": "{b}: {d}%"},
                    "data": [
                        {"name": name, "value": val}
                        for name, val in zip(x_values, y_values)
                    ],
                }
            ]
            break

        if kind == "funnel":
            option["tooltip"] = {"trigger": "item"}
            option.pop("xAxis", None)
            option.pop("yAxis", None)
            data_pairs = [
                {"name": name, "value": val}
                for name, val in sorted(zip(x_values, y_values), key=lambda p: p[1], reverse=True)
            ]
            option["series"].append(
                {
                    "name": y,
                    "type": "funnel",
                    "label": {"show": True, "formatter": "{b}: {c}"},
                    "data": data_pairs,
                }
            )
            break

        if kind == "combo" and len(y_cols) >= 2:
            # Primeira m√©trica em barras, segunda em linha
            option["series"].append(
                {
                    "name": y_cols[0],
                    "type": "bar",
                    "data": _convert_series_to_native(dff[y_cols[0]]),
                    "itemStyle": {"color": color},
                    "label": {"show": True, "formatter": "{c}"},
                }
            )
            option["series"].append(
                {
                    "name": y_cols[1],
                    "type": "line",
                    "yAxisIndex": 1,
                    "smooth": True,
                    "data": _convert_series_to_native(dff[y_cols[1]]),
                    "label": {"show": True, "formatter": "{c}"},
                }
            )
            option["yAxis"] = [
                {"type": "value"},
                {"type": "value"},
            ]
            break

        series_entry: Dict[str, Any] = {
            "name": y,
            "type": "line" if kind == "area" else kind,
            "data": y_values,
            "label": {"show": True, "formatter": "{c}"},
        }
        if kind == "area":
            series_entry["areaStyle"] = {}
        if series_color:
            series_entry["itemStyle"] = {"color": series_color}
        option["series"].append(series_entry)

    option = _enforce_label_visibility(option, color)
    option = _add_save_as_image(option)
    return option


def try_generate_ai_echarts_option(df: pd.DataFrame, user_query: str, user_color: str, possible_x_cols: List[str], numeric_cols: List[str]) -> Dict[str, Any]:
    """Usa IA (Bedrock) para gerar um option ECharts totalmente personalizado."""
    bedrock = get_bedrock_client()
    if not bedrock or not st.session_state.feature_flags.get('ai_chart_builder', True):
        return {}

    sample_df = df.head(50).copy()
    sample_payload = sample_df.to_dict(orient="list")
    columns_info = []
    for col in sample_df.columns:
        dtype = str(sample_df[col].dtype)
        sample_vals = sample_payload.get(col, [])[:5]
        columns_info.append({"name": col, "dtype": dtype, "sample": sample_vals})

    prompt = f"""
Voc√™ √© um especialista em ECharts. Gere APENAS um JSON de op√ß√£o ECharts (sem Markdown, sem texto extra) para o pedido abaixo.

Regras obrigat√≥rias:
- Use apenas colunas existentes: {list(sample_df.columns)}
- Poss√≠veis eixos X preferenciais: {possible_x_cols}
- Colunas num√©ricas dispon√≠veis: {numeric_cols}
- Labels DEVEM estar vis√≠veis (show=true) em todas as s√©ries.
- Eixos DEVEM exibir r√≥tulos (axisLabel.show=true) e linhas de eixo/grade vis√≠veis.
- Se o usu√°rio pediu cor, aplique como cor principal das s√©ries: {user_color}
- Priorize gr√°fico combinado (barra + linha) quando pedido ‚Äúbarras e linhas‚Äù ou ‚Äúcombo‚Äù.
- Respeite o pedido do usu√°rio para tipo de gr√°fico, cores e m√©trica/eixo.
- N√£o invente colunas.
- Respeite escalas: se for percentual, use valores do dado; n√£o normalizar.
- Mantenha tooltips e legendas simples.

Dados (amostra at√© 50 linhas):
{json.dumps(sample_payload)[:4000]}

Schema resumido:
{json.dumps(columns_info)[:2000]}

Pedido do usu√°rio:
{user_query}

Responda SOMENTE com o JSON do option ECharts.
"""

    try:
        response = bedrock.invoke_model(
            modelId="us.anthropic.claude-sonnet-4-20250514-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 2048,
                "temperature": 0.3,
                "messages": [{"role": "user", "content": prompt}]
            })
        )
        response_body = json.loads(response['body'].read())
        text = response_body['content'][0]['text'].strip()
        json_match = re.search(r"\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}", text, re.DOTALL)
        if json_match:
            option = json.loads(json_match.group())
        else:
            option = json.loads(text)
    except Exception as e:
        print(f"[AI_CHART] Erro ao gerar option: {e}")
        return {}

    option = _sanitize_option_types(option)
    option = _validate_echarts_option(option)
    if not option:
        return {}

    option = _enforce_label_visibility(option, user_color)
    option = _add_save_as_image(option)
    return option


def try_generate_ai_web_chart_html(df: pd.DataFrame, user_query: str, user_color: str, possible_x_cols: List[str], numeric_cols: List[str]) -> str:
    """Pede √† IA um snippet HTML/JS (ECharts) com os dados embutidos. Retorna HTML string ou ''."""
    bedrock = get_bedrock_client()
    if not bedrock or not st.session_state.feature_flags.get('ai_chart_builder', True):
        return ""

    sample_df = df.head(200).copy()
    payload = {
        'columns': list(sample_df.columns),
        'data': sample_df.to_dict(orient='records')
    }

    prompt = f"""
Gere APENAS um snippet HTML completo (sem explica√ß√µes) que renderize um gr√°fico interativo usando ECharts.
- Incluir biblioteca ECharts via CDN.
- Incluir uma vari√°vel JavaScript `data` contendo os registros JSON (campo `data` abaixo).
- Usar colunas dispon√≠veis: {list(sample_df.columns)}.
- Preferir X/Y sugeridos pelo contexto quando apropriado. Se houver coluna com 'taxa'/'aprov', trate-a como percentual e formate labels com '%'.
- Aplicar cor principal solicitada: {user_color} (se vazio, escolha roxo #7B61FF).
- O HTML deve ser autossuficiente e funcionar quando renderizado em um iframe.

Dados para embutir (JSON):
{json.dumps(payload)[:6000]}

Pedido do usu√°rio:
{user_query}

Responda SOMENTE com o HTML final.
"""

    try:
        response = bedrock.invoke_model(
            modelId="us.anthropic.claude-sonnet-4-20250514-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 2048,
                "temperature": 0.2,
                "messages": [{"role": "user", "content": prompt}]
            })
        )
        response_body = json.loads(response['body'].read())
        text = response_body['content'][0]['text'].strip()
        # Extrair HTML completo (assumir come√ßa com < and ends with >)
        html_start = text.find('<')
        if html_start != -1:
            html = text[html_start:]
        else:
            html = text
    except Exception as e:
        print(f"[AI_WEB_CHART] Erro: {e}")
        return ""

    # Seguran√ßa m√≠nima: garantir que retorna uma string n√£o vazia
    if not html:
        return ""
    return html

# Translations (mantido igual ao original)
translations = {
    'pt': {
        'title': 'ü§ñ BRIAN - Potencialize suas an√°lises',
        'subtitle': 'Transforme linguagem natural em queries SQL e obtenha insights inteligentes dos seus dados Databricks.',
        'sidebar_config': '‚öôÔ∏è Configura√ß√£o',
        'connection': 'üîê Conex√£o Databricks',
        'ai_config': 'üß† Configura√ß√£o do Modelo IA',
        'advanced': 'üöÄ Recursos Avan√ßados',
        'context': 'üìù Configura√ß√£o de Contexto',
        'history': 'üìö Hist√≥rico de Queries',
        'clear_history': 'üóëÔ∏è Limpar Hist√≥rico',
        'main_question': 'üí¨ Fa√ßa sua pergunta',
        'placeholder': 'Exemplo: "Mostre-me o total de limite de cr√©dito por status do cart√£o" ou "Liste os 10 cart√µes com maior limite de cr√©dito"',
        'execute': 'üöÄ Executar Query',
        'refine': 'üîß Refinar √öltima Query',
        'clear': 'üóëÔ∏è Limpar Resultados',
        'suggestions': 'üí° Sugest√µes R√°pidas',
        'direct': '(A) Resposta direta √† pergunta',
        'additional': '(B) An√°lises adicionais + resumo executivo',
        'causes': '(C) Causas prov√°veis + novas an√°lises',
        'code': '(D) C√≥digo',
        'note': '(E) Nota do Prompt + Como melhorar',
        'current_results': 'üìã Resultados Atuais',
        'footer': 'Desenvolvido com ‚ù§Ô∏è usando Streamlit, Databricks e AWS Bedrock',
        'language': 'üåê Configura√ß√µes de Idioma',
        'select_lang': 'Selecionar Idioma',
        'langs': {'pt': 'Portugu√™s', 'en': 'English', 'es': 'Espa√±ol'},
        'generating_direct': 'Gerando resposta direta...',
        'generating_analysis': 'Gerando an√°lises adicionais...',
        'generating_causes': 'Gerando hip√≥teses e sugest√µes...',
        'generating_note': 'Gerando avalia√ß√£o...',
        'refine_instructions': 'Instru√ß√µes de refinamento:',
        'refine_query': 'Refinar Query',
        'refining_query': 'Refinando query...',
        'interactive_exploration': 'üß≠ Explora√ß√£o Interativa de Dados',
        'agent_type': 'Tipo de Agente',
        'chat_agent': 'Agente de Chat',
        'modular_prompt_agent': 'Prompt Modular',
        'agent_role': 'Papel do Agente',
        'context_step': 'Contexto - Etapa',
        'start_modular': 'Iniciar Processo Modular',
        'continue_step': 'Continuar para Pr√≥xima Etapa',
        'finish_process': 'Finalizar Processo',
        'query_assistant': 'ü§ù Assistente de Consulta',
        'assistant_chat': 'Conversa√ß√£o com Assistente',
        'assistant_message': 'Mensagem para o Assistente',
        'send_to_assistant': 'üì§ Enviar ao Assistente',
        'finalize_query': '‚úÖ Finalizar e Executar Query',
        'assistant_analyzing': 'Assistente analisando sua solicita√ß√£o...',
        'confidence_high': 'Confian√ßa ALTA - Pronto para executar!',
        'confidence_medium': 'Confian√ßa M√âDIA - Vamos refinar mais?',
        'confidence_low': 'Confian√ßa BAIXA - Preciso de mais detalhes'
    },
    'en': {
        'title': 'ü§ñ BRIAN - Boost your analyses',
        'subtitle': 'Transform natural language into SQL queries and get intelligent insights from your Databricks data warehouse.',
        'sidebar_config': '‚öôÔ∏è Configuration',
        'connection': 'üîê Databricks Connection',
        'ai_config': 'üß† AI Model Configuration',
        'advanced': 'üöÄ Advanced Features',
        'context': 'üìù Context Configuration',
        'history': 'üìö Query History',
        'clear_history': 'üóëÔ∏è Clear History',
        'main_question': 'üí¨ Ask Your Question',
        'placeholder': 'Example: "Show me the total credit limit by card status" or "List the top 10 cards by credit limit"',
        'execute': 'üöÄ Execute Query',
        'refine': 'üîß Refine Last Query',
        'clear': 'üóëÔ∏è Clear Results',
        'suggestions': 'üí° Quick Suggestions',
        'direct': '(A) Direct Response to the Question',
        'additional': '(B) Additional Analyses + Executive Summary',
        'causes': '(C) Probable Causes + New Analyses',
        'code': '(D) Code',
        'note': '(E) Prompt Note + How to Improve',
        'current_results': 'üìã Current Results',
        'footer': 'Built with ‚ù§Ô∏è using Streamlit, Databricks, and AWS Bedrock',
        'language': 'üåê Language Settings',
        'select_lang': 'Select Language',
        'langs': {'pt': 'Portugu√™s', 'en': 'English', 'es': 'Espa√±ol'},
        'generating_direct': 'Generating direct response...',
        'generating_analysis': 'Generating additional analyses...',
        'generating_causes': 'Generating hypotheses and suggestions...',
        'generating_note': 'Generating evaluation...',
        'refine_instructions': 'Refinement instructions:',
        'refine_query': 'Refine Query',
        'refining_query': 'Refining query...',
        'interactive_exploration': 'üß≠ Interactive Data Exploration',
        'agent_type': 'Agent Type',
        'chat_agent': 'Chat Agent',
        'modular_prompt_agent': 'Modular Prompt',
        'agent_role': 'Agent Role',
        'context_step': 'Context - Step',
        'start_modular': 'Start Modular Process',
        'continue_step': 'Continue to Next Step',
        'finish_process': 'Finish Process',
        'query_assistant': 'ü§ù Query Assistant',
        'assistant_chat': 'Conversation with Assistant',
        'assistant_message': 'Message to Assistant',
        'send_to_assistant': 'üì§ Send to Assistant',
        'finalize_query': '‚úÖ Finalize and Execute Query',
        'assistant_analyzing': 'Assistant analyzing your request...',
        'confidence_high': 'HIGH Confidence - Ready to execute!',
        'confidence_medium': 'MEDIUM Confidence - Should we refine more?',
        'confidence_low': 'LOW Confidence - Need more details'
    },
    'es': {
        'title': 'ü§ñ BRIAN - Potencia tus an√°lisis',
        'subtitle': 'Transforma lenguaje natural en consultas SQL y obt√©n insights inteligentes de tu almac√©n de datos Databricks.',
        'sidebar_config': '‚öôÔ∏è Configuraci√≥n',
        'connection': 'üîê Conexi√≥n Databricks',
        'ai_config': 'üß† Configuraci√≥n del Modelo IA',
        'advanced': 'üöÄ Caracter√≠sticas Avanzadas',
        'context': 'üìù Configuraci√≥n de Contexto',
        'history': 'üìö Historial de Consultas',
        'clear_history': 'üóëÔ∏è Limpiar Historial',
        'main_question': 'üí¨ Haz tu pregunta',
        'placeholder': 'Ejemplo: "Mu√©strame el l√≠mite total de cr√©dito por estado de tarjeta" o "Lista las 10 tarjetas con mayor l√≠mite de cr√©dito"',
        'execute': 'üöÄ Ejecutar Consulta',
        'refine': 'üîß Refinar √öltima Consulta',
        'clear': 'üóëÔ∏è Limpiar Resultados',
        'suggestions': 'üí° Sugerencias R√°pidas',
        'direct': '(A) Respuesta directa a la pregunta',
        'additional': '(B) An√°lisis adicionales + resumen ejecutivo',
        'causes': '(C) Causas probables + nuevos an√°lisis',
        'code': '(D) C√≥digo',
        'note': '(E) Nota del Prompt + C√≥mo mejorar',
        'current_results': 'üìã Resultados Actuales',
        'footer': 'Desarrollado con ‚ù§Ô∏è usando Streamlit, Databricks y AWS Bedrock',
        'language': 'üåê Configuraciones de Idioma',
        'select_lang': 'Seleccionar Idioma',
        'langs': {'pt': 'Portugu√™s', 'en': 'English', 'es': 'Espa√±ol'},
        'generating_direct': 'Generando respuesta directa...',
        'generating_analysis': 'Generando an√°lisis adicionales...',
        'generating_causes': 'Generando hip√≥tesis y sugerencias...',
        'generating_note': 'Generando evaluaci√≥n...',
        'refine_instructions': 'Instrucciones de refinamiento:',
        'refine_query': 'Refinar Consulta',
        'refining_query': 'Refinando consulta...',
        'interactive_exploration': 'üß≠ Exploraci√≥n Interactiva de Datos',
        'agent_type': 'Tipo de Agente',
        'chat_agent': 'Agente de Chat',
        'modular_prompt_agent': 'Prompt Modular',
        'agent_role': 'Papel del Agente',
        'context_step': 'Contexto - Etapa',
        'start_modular': 'Iniciar Proceso Modular',
        'continue_step': 'Continuar a Siguiente Etapa',
        'finish_process': 'Finalizar Proceso',
        'query_assistant': 'ü§ù Asistente de Consulta',
        'assistant_chat': 'Conversaci√≥n con Asistente',
        'assistant_message': 'Mensaje para el Asistente',
        'send_to_assistant': 'üì§ Enviar al Asistente',
        'finalize_query': '‚úÖ Finalizar y Ejecutar Consulta',
        'assistant_analyzing': 'Asistente analizando su solicitud...',
        'confidence_high': 'Confianza ALTA - ¬°Listo para ejecutar!',
        'confidence_medium': 'Confianza MEDIA - ¬øRefinamos m√°s?',
        'confidence_low': 'Confianza BAJA - Necesito m√°s detalles'
    }
}

# Configure page
st.set_page_config(
    page_title="AIQUERY2 - Databricks AI Assistant",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        background: linear-gradient(45deg, #1f77b4, #ff7f0e);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        text-align: center;
        margin-bottom: 2rem;
    }
    .query-input {
        border: 2px solid #1f77b4;
        border-radius: 10px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .success-message {
        background-color: #d4edda;
        color: #155724;
        padding: 1rem;
        border-radius: 5px;
        border: 1px solid #c3e6cb;
        margin: 1rem 0;
    }
    .error-message {
        background-color: #f8d7da;
        color: #721c24;
        padding: 1rem;
        border-radius: 5px;
        border: 1px solid #f5c6cb;
        margin: 1rem 0;
    }
    .info-message {
        background-color: #d1ecf1;
        color: #0c5460;
        padding: 1rem;
        border-radius: 5px;
        border: 1px solid #bee5eb;
        margin: 1rem 0;
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1.5rem;
        border-radius: 10px;
        text-align: center;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    .sidebar-section {
        background-color: #f8f9fa;
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
    }
    
    /* Garantir scroll no sidebar */
    [data-testid="stSidebar"] {
        overflow-y: auto !important;
        max-height: 100vh !important;
    }
    
    [data-testid="stSidebar"] > div:first-child {
        overflow-y: auto !important;
        max-height: 100vh !important;
    }
    
    /* Melhorar visualiza√ß√£o de expanders no sidebar */
    [data-testid="stSidebar"] .streamlit-expanderHeader {
        font-weight: 600;
        font-size: 0.95rem;
    }
    
    [data-testid="stSidebar"] .streamlit-expanderContent {
        max-height: 70vh;
        overflow-y: auto;
    }
</style>
""", unsafe_allow_html=True)

# Default context definition (mantido igual)
DEFAULT_CONTEXT = """tabela: prd.sand_snd_box_cartao.tb_pbi_autz_pampa_diar

descri√ß√£o:
Tabela contendo dados di√°rios de autoriza√ß√µes do sistema PAMPA. Esta √© a base principal do Dashboard de Autoriza√ß√µes Di√°rio, que oferece vis√£o consolidada e detalhada dos principais indicadores da √°rea de autoriza√ß√µes em diferentes n√≠veis (Executivo, Gerencial e T√°tico).

contexto do neg√≥cio:
Esta tabela armazena transa√ß√µes di√°rias processadas pela autorizadora PAMPA, incluindo informa√ß√µes sobre aprova√ß√µes, negativas, bandeiras, segmentos, produtos, utiliza√ß√£o (NFC, Compra Presente, Compra N√£o Presente), carteiras digitais (Apple Pay, Google Pay, Samsung Pay, etc.) e tipos de cart√£o.

principais an√°lises suportadas:
- Taxa de Aprova√ß√£o por dia, funcionalidade (Cr√©dito/D√©bito), bandeira, segmento, produto
- Volume de transa√ß√µes e valores financeiros
- An√°lise de negativas e seus motivos
- Share de bandeiras, produtos, segmentos e tipos de utiliza√ß√£o
- Performance de carteiras digitais
- An√°lise de transa√ß√µes dom√©sticas vs cross border
- Comparativos temporais (YOY, MOM, MTD, YTD)

schema da tabela:

dt_tran
date
Data da transa√ß√£o. Campo principal para an√°lises temporais e filtros de per√≠odo.

cd_resposta
string
C√≥digo da resposta da transa√ß√£o (ex: 0001 para aprovada). Usado para identificar o status da autoriza√ß√£o.

ds_resposta
string
Descri√ß√£o da resposta da transa√ß√£o (ex: "APROVADA"). Vers√£o leg√≠vel do c√≥digo de resposta.

ds_resposta_ajust
string
Descri√ß√£o ajustada/padronizada da resposta (ex: "Aprovada"). Usada para consolida√ß√£o e an√°lises.

considerar_negativa
string
Indica se a resposta deve ser considerada negativa nas an√°lises (valores: "SIM" ou "NAO"). Fundamental para c√°lculo de taxa de aprova√ß√£o.

moeda
string
Tipo de moeda da transa√ß√£o. Indica se √© transa√ß√£o Nacional ou Internacional.

cd_atvd_estb
string
C√≥digo de atividade do estabelecimento (MCC - Merchant Category Code). Identifica o tipo de neg√≥cio onde ocorreu a transa√ß√£o.

pais
string
Pa√≠s onde a transa√ß√£o foi realizada. Vazio para transa√ß√µes dom√©sticas, preenchido para cross border.

utilizacao
string
Tipo de utiliza√ß√£o do cart√£o. Valores poss√≠veis: "Compra Presente", "Compra N√£o Presente", "NFC", "N√£o Informado". Essencial para an√°lise de canais.

nm_wallet
string
Nome da carteira digital utilizada. Valores: "PAN" (cart√£o f√≠sico), "Apple Pay", "Google Pay", "Samsung Pay", "Garmin Pay", "Click to Pay". Fundamental para an√°lise de carteiras digitais.

pf_pj
string
Tipo de pessoa. Valores: "PF" (Pessoa F√≠sica) ou "PJ" (Pessoa Jur√≠dica).

tp_cartao
string
Tipo f√≠sico do cart√£o. Valores: "F√≠sico" ou "Virtual".

ds_bandeira
string
Descri√ß√£o da bandeira do cart√£o. Valores principais: "Visa", "MasterCard", "American Express" (Amex).

ds_funcionalidade
string
Funcionalidade do cart√£o. Valores: "Cr√©dito", "D√©bito", "TecBan". Fundamental para an√°lises por fun√ß√£o.

cd_subproduto
string
C√≥digo do subproduto do cart√£o. Identificador num√©rico do produto espec√≠fico.

ds_produto
string
Descri√ß√£o do produto do cart√£o (ex: "Santander Elite Platinum", "Free", "Santander Unique Infinite", "Santander SX", etc.).

ds_segmento
string
Descri√ß√£o do segmento do cliente (ex: "SELECT", "ESPECIAL", "EMPRESAS MEI", etc.).

tp_segmento
string
Tipo de segmento do cliente com prefixo de espa√ßo (ex: " SELECT", " ESPECIAL", " MEI").

vl_tran
decimal(17,2)
Valor financeiro da transa√ß√£o em reais. Usado para an√°lises de volume financeiro e ticket m√©dio.

qt_tran
bigint
Quantidade de transa√ß√µes. Utilizado para contagens e an√°lises volum√©tricas.

regras de neg√≥cio importantes:

1. TAXA DE APROVA√á√ÉO:
   - Calculada como: (Transa√ß√µes Aprovadas / Total de Transa√ß√µes) * 100
   - Considerar apenas transa√ß√µes onde considerar_negativa = "SIM" no denominador
   - Transa√ß√µes aprovadas s√£o aquelas com cd_resposta = "0001" ou ds_resposta_ajust = "Aprovada"

2. FILTROS COMUNS:
   - Por funcionalidade: WHERE ds_funcionalidade IN ('Cr√©dito', 'D√©bito')
   - Por bandeira: WHERE ds_bandeira IN ('Visa', 'MasterCard', 'American Express')
   - Por tipo de utiliza√ß√£o: WHERE utilizacao IN ('Compra Presente', 'Compra N√£o Presente', 'NFC')
   - Por carteira digital: WHERE nm_wallet != 'PAN' para transa√ß√µes com wallet
   - Por tipo de pessoa: WHERE pf_pj IN ('PF', 'PJ')

3. M√âTRICAS PRINCIPAIS:
   - Quantidade de transa√ß√µes: SUM(qt_tran)
   - Volume financeiro: SUM(vl_tran)
   - Ticket m√©dio: SUM(vl_tran) / SUM(qt_tran)
   - Taxa de aprova√ß√£o: SUM(CASE WHEN ds_resposta_ajust = 'Aprovada' THEN qt_tran ELSE 0 END) / SUM(qt_tran) * 100

4. NEGATIVAS:
   - Principal indicador de an√°lise para identificar oportunidades de melhoria
   - Agrupar por ds_resposta ou ds_resposta_ajust para analisar motivos
   - Filtrar considerar_negativa = "SIM" para an√°lises de taxa de aprova√ß√£o
   - TOP negativas: ordenar por quantidade/volume e pegar os principais motivos

5. AN√ÅLISES TEMPORAIS:
   - Campo de data: dt_tran
   - Compara√ß√µes t√≠picas: YOY (ano anterior), MOM (m√™s anterior), MTD (m√©dia do m√™s), YTD (m√©dia do ano)
   - Agrupar por dia, semana ou m√™s conforme necess√°rio

6. SEGMENTA√á√ïES IMPORTANTES:
   - Por bandeira: Visa, MasterCard, Amex
   - Por funcionalidade: Cr√©dito vs D√©bito
   - Por canal: Compra Presente vs Compra N√£o Presente vs NFC
   - Por carteira: PAN (f√≠sico) vs Apple Pay vs Google Pay vs Samsung Pay
   - Por segmento: SELECT, ESPECIAL, MEI, EMPRESAS
   - Por tipo de pessoa: PF vs PJ

7. TRANSA√á√ïES DOM√âSTICAS VS INTERNACIONAL:
   - Dom√©stica: pais vazio ou NULL
   - Internacional (Cross Border): pais preenchido

8. SHARE/PARTICIPA√á√ÉO:
   - Share de bandeira: (Transa√ß√µes bandeira X / Total transa√ß√µes) * 100
   - Share de funcionalidade: (Transa√ß√µes fun√ß√£o X / Total transa√ß√µes) * 100
   - Share de utiliza√ß√£o: (Transa√ß√µes utiliza√ß√£o X / Total transa√ß√µes) * 100

exemplos de dados na tabela:
- Transa√ß√£o aprovada Visa Cr√©dito NFC PF: dt_tran='2025-09-27', cd_resposta='0001', ds_resposta='APROVADA', ds_funcionalidade='Cr√©dito', ds_bandeira='Visa', utilizacao='NFC', pf_pj='PF'
- Transa√ß√£o aprovada MasterCard D√©bito com Apple Pay: dt_tran='2025-09-27', nm_wallet='Apple Pay', ds_funcionalidade='D√©bito', ds_bandeira='MasterCard'
- Volume financeiro m√©dio de transa√ß√µes aprovadas: vl_tran entre 10MM a 11MM por dia
- Quantidade de transa√ß√µes aprovadas: qt_tran entre 90k a 140k por dia

contexto dashboard:
O Dashboard de Autoriza√ß√µes Di√°rio possui 3 n√≠veis:
- EXECUTIVO: Taxa de aprova√ß√£o, total de transa√ß√µes, top negativas com comparativos dTOY, MOM, MTD, YTD
- GERENCIAL: An√°lises detalhadas por transa√ß√µes, utiliza√ß√£o, funcionalidade, bandeira, carteiras digitais, negativas e insights
- T√ÅTICO: Tabela anal√≠tica com m√∫ltiplas dimens√µes personaliz√°veis

observa√ß√µes importantes:
- A tabela cont√©m apenas dados do sistema PAMPA (n√£o inclui Plard ou Amex)
- Dados agregados por dia com m√©tricas j√° calculadas (qt_tran e vl_tran)
- considerar_negativa="SIM" indica que a resposta deve entrar no c√°lculo de taxa de aprova√ß√£o
- nm_wallet="PAN" indica uso do cart√£o f√≠sico sem carteira digital
- ds_resposta_ajust padroniza as respostas para facilitar an√°lises

"""

# Initialize session state - SEMPRE LIMPA AO INICIAR (SEM PERSIST√äNCIA)
st.session_state.query_history = []
st.session_state.current_df = None
st.session_state.last_query = ""
st.session_state.last_sql = ""
if 'conversation_history' not in st.session_state:
    # Armazena hist√≥rico de perguntas e SQL gerados
    st.session_state.conversation_history = []
if 'last_query_context' not in st.session_state:
    # Guarda contexto da √∫ltima execu√ß√£o para refinamentos
    st.session_state.last_query_context = {}

if 'connection_config' not in st.session_state:
    st.session_state.connection_config = {
        'server_hostname': 'adb-8528950209693760.0.azuredatabricks.net',
        'http_path': '/sql/1.0/warehouses/96c8ade94d8e503b',
        'access_token': 'dapi529a25f2362d74a4973d26b7cc5aee60',
        'catalog': 'prd',
        'schema': 'sand_snd_box_cartao'
    }
if 'ai_config' not in st.session_state:
    st.session_state.ai_config = {
        'model': 'bedrock_claude',
        'max_retries': 2,
        'context_table': 'tb_pbi_autz_pampa_diar'
    }
if 'aws_bedrock_token' not in st.session_state:
    # Inicializa vazio - usu√°rio deve configurar no frontend
    st.session_state.aws_bedrock_token = ''
if 'feature_flags' not in st.session_state:
    st.session_state.feature_flags = {
        'intelligent_analysis': True,
        'auto_comparison': True,
        'smart_visualization': True,
        'ai_chart_builder': True,
        'error_correction': True,
        'context_enrichment': True,
        'pre_validation': True,
        'smart_suggestions': True,
        'query_reasoning': True,
        'sampling_alerts': True,
        'query_judge': True
    }
else:
    # Garantir que novas flags existam em sess√µes antigas
    st.session_state.feature_flags.setdefault('ai_chart_builder', True)
    st.session_state.feature_flags.setdefault('context_enrichment', True)
    st.session_state.feature_flags.setdefault('intelligent_analysis', True)
    st.session_state.feature_flags.setdefault('auto_comparison', True)
    st.session_state.feature_flags.setdefault('smart_visualization', True)
    st.session_state.feature_flags.setdefault('error_correction', True)
    st.session_state.feature_flags.setdefault('pre_validation', True)
    st.session_state.feature_flags.setdefault('smart_suggestions', True)
    st.session_state.feature_flags.setdefault('query_reasoning', True)
    st.session_state.feature_flags.setdefault('sampling_alerts', True)
    st.session_state.feature_flags.setdefault('query_judge', True)

# Chart type default
if 'chart_type' not in st.session_state:
    st.session_state.chart_type = 'Autom√°tico'

# Session state para par√¢metros do gr√°fico manual (evita limpeza ao mudar par√¢metros)
if 'manual_chart_type' not in st.session_state:
    st.session_state.manual_chart_type = 'Autom√°tico'
if 'manual_color' not in st.session_state:
    st.session_state.manual_color = '#5470c6'
if 'manual_x' not in st.session_state:
    st.session_state.manual_x = None
if 'manual_y' not in st.session_state:
    st.session_state.manual_y = []
if 'manual_agg' not in st.session_state:
    st.session_state.manual_agg = 'sum'
if 'manual_chart_generated' not in st.session_state:
    st.session_state.manual_chart_generated = False
if 'manual_chart_option' not in st.session_state:
    st.session_state.manual_chart_option = None

# Configura√ß√µes de explora√ß√£o (limites e parti√ß√£o)
if 'exploration_limit_per_column' not in st.session_state:
    st.session_state.exploration_limit_per_column = 100  # Reduzido de 1000 para 100 para melhor performance
if 'partition_column_name' not in st.session_state:
    st.session_state.partition_column_name = 'dt_tran'
if 'exploration_sample_limit' not in st.session_state:
    st.session_state.exploration_sample_limit = 1000
if 'query_timeout_seconds' not in st.session_state:
    st.session_state.query_timeout_seconds = 60  # Timeout padr√£o de 60 segundos para queries

# Sempre inicializa com contexto vazio - usu√°rio deve configurar manualmente
st.session_state.user_context = DEFAULT_CONTEXT

if 'language' not in st.session_state:
    st.session_state.language = 'pt'
if 'first_execution_attempted' not in st.session_state:
    st.session_state.first_execution_attempted = False
if 'agent_type' not in st.session_state:
    st.session_state.agent_type = 'chat'  # 'chat' or 'modular_prompt'
if 'modular_config' not in st.session_state:
    st.session_state.modular_config = {
        'role': '',
        'contexts': [''] * 10,  # Up to 10 context fields
        'current_step': 0,
        'responses': []
    }

# Modo de consulta: direto, assistente, aut√¥nomo
if 'query_mode' not in st.session_state:
    st.session_state.query_mode = 'autonomous'  # Consulta Aut√¥noma como padr√£o
if 'autonomous_mode' not in st.session_state:
    st.session_state.autonomous_mode = True  # Modo aut√¥nomo como padr√£o

# Initialize Query Assistant state
if 'assistant_mode' not in st.session_state:
    st.session_state.assistant_mode = False
if 'assistant_chat_history' not in st.session_state:
    st.session_state.assistant_chat_history = []
if 'assistant_confidence' not in st.session_state:
    st.session_state.assistant_confidence = 0
if 'final_refined_query' not in st.session_state:
    st.session_state.final_refined_query = ""
if 'user_input_value' not in st.session_state:
    st.session_state.user_input_value = ""
if 'auto_execute_from_assistant' not in st.session_state:
    st.session_state.auto_execute_from_assistant = False
if 'last_assistant_chat' not in st.session_state:
    st.session_state.last_assistant_chat = []
if 'last_assistant_confidence' not in st.session_state:
    st.session_state.last_assistant_confidence = 0
if 'last_refined_query' not in st.session_state:
    st.session_state.last_refined_query = ""
if 'consolidated_assistant_prompt' not in st.session_state:
    st.session_state.consolidated_assistant_prompt = ""
if 'last_execution_time' not in st.session_state:
    st.session_state.last_execution_time = None
if 'analysis_scores' not in st.session_state:
    st.session_state.analysis_scores = {}
if 'user_feedback' not in st.session_state:
    st.session_state.user_feedback = {}
if 'show_feedback_form' not in st.session_state:
    st.session_state.show_feedback_form = {}
if 'assistant_input_cleared' not in st.session_state:
    st.session_state.assistant_input_cleared = False
if 'assistant_input_counter' not in st.session_state:
    st.session_state.assistant_input_counter = 0
if 'show_refine_section' not in st.session_state:
    st.session_state.show_refine_section = False
if 'query_executed_successfully' not in st.session_state:
    st.session_state.query_executed_successfully = False
if 'execution_errors' not in st.session_state:
    st.session_state.execution_errors = []
if 'sql_override' not in st.session_state:
    st.session_state.sql_override = ""
if 'last_error_analysis' not in st.session_state:
    st.session_state.last_error_analysis = {}
if 'table_exploration_cache' not in st.session_state:
    st.session_state.table_exploration_cache = {}
if 'auto_exploration_done' not in st.session_state:
    st.session_state.auto_exploration_done = False
if 'smart_questions' not in st.session_state:
    st.session_state.smart_questions = []
if 'questions_generated' not in st.session_state:
    st.session_state.questions_generated = False

def explore_table_metadata(table_name: str, force_refresh: bool = False) -> Dict[str, Any]:
    """Executa explora√ß√£o completa da tabela: colunas, tipos, valores distintos para flags e descri√ß√µes.
    
    Cacheia resultados para performance. Use force_refresh=True para atualizar cache.
    
    Args:
        table_name: Nome completo da tabela (catalog.schema.table)
        force_refresh: Se True, ignora cache e executa nova explora√ß√£o
    
    Returns:
        Dict com:
        - columns: List[Dict] - todas as colunas com tipo
        - flag_columns: Dict[str, List] - valores distintos de colunas flag_*
        - description_columns: Dict[str, List] - valores distintos de colunas dscr_* e desc_*
        - sample_data: pd.DataFrame - amostra dos dados
        - exploration_time: str - timestamp da explora√ß√£o
    """
    cache_key = table_name
    
    # Usar cache se dispon√≠vel e n√£o for√ßar refresh
    if not force_refresh and cache_key in st.session_state.table_exploration_cache:
        cached_data = st.session_state.table_exploration_cache[cache_key]
        print(f"[EXPLORE] ‚úÖ Usando cache para {table_name}")
        print(f"[EXPLORE] üìä Cache: {len(cached_data.get('columns', []))} colunas, "
              f"{len(cached_data.get('flag_columns', {}))} flags")
        return cached_data
    
    print(f"[EXPLORE] üîç Iniciando explora√ß√£o de {table_name}...")
    
    exploration = {
        'columns': [],
        'flag_columns': {},
        'description_columns': {},
        'code_columns': {},
        'sample_data': None,
        'exploration_time': datetime.now().isoformat()
    }
    
    try:
        # 1. Obter todas as colunas
        print(f"[EXPLORE] üìä Consultando colunas...")
        columns = get_table_columns(table_name)
        exploration['columns'] = columns
        
        if not columns:
            print(f"[EXPLORE] ‚ö†Ô∏è Nenhuma coluna encontrada")
            return exploration
        
        print(f"[EXPLORE] ‚úÖ {len(columns)} colunas encontradas")
        
        # 2. Identificar colunas de flag (flag_*), descri√ß√£o (dscr_*, desc_*, ds_*) e c√≥digo (cd_*, codigo_*, codi_*)
        flag_cols = [c['name'] for c in columns if c['name'].startswith('flag_')]
        desc_cols = [c['name'] for c in columns if c['name'].startswith('dscr_') or c['name'].startswith('desc_') or c['name'].startswith('ds_')]
        code_cols = [c['name'] for c in columns if c['name'].startswith('cd_') or c['name'].startswith('codigo_') or c['name'].startswith('codi_')]
        
        print(f"[EXPLORE] üö© {len(flag_cols)} colunas de flag encontradas (limite 1000 valores)")
        print(f"[EXPLORE] üìù {len(desc_cols)} colunas de descri√ß√£o encontradas (limite 1000 valores)")
        print(f"[EXPLORE] üî¢ {len(code_cols)} colunas de c√≥digo encontradas (limite 1000 valores)")
        
        # 3. Consultar valores distintos para flags (at√© 1000 valores por coluna)
        flags_to_explore = flag_cols
        print(f"[EXPLORE] üîç Explorando {len(flags_to_explore)} colunas de flag (limite de valores: 1000)...")
        
        # Feedback visual
        if flags_to_explore:
            progress_placeholder = st.empty()
            progress_placeholder.info(f"üîç Explorando {len(flags_to_explore)} colunas de flag...")
        
        for i, col in enumerate(flags_to_explore, 1):
            print(f"[EXPLORE] üîç [{i}/{len(flags_to_explore)}] Consultando valores: {col}")
            if flags_to_explore:
                progress_placeholder.info(f"üîç Explorando flags: {col} ({i}/{len(flags_to_explore)})")
            
            try:
                # Limitar valores distintos por coluna conforme configura√ß√£o
                distinct_values = get_distinct_values(
                    table_name,
                    col,
                    limit=st.session_state.exploration_limit_per_column,
                    use_partition_filter=False  # Desabilitar filtro de parti√ß√£o
                )
                if distinct_values:
                    exploration['flag_columns'][col] = distinct_values
                    print(f"[EXPLORE]   ‚úÖ {len(distinct_values)} valores: {distinct_values}")
            except Exception as e:
                print(f"[EXPLORE]   ‚ö†Ô∏è Erro: {str(e)[:100]}")
        
        if flags_to_explore:
            progress_placeholder.empty()
        
        # 4. Consultar valores distintos para descri√ß√µes (at√© 1000 valores por coluna)
        descs_to_explore = desc_cols
        print(f"[EXPLORE] üîç Explorando {len(descs_to_explore)} colunas de descri√ß√£o (limite de valores: 1000)...")
        
        # Feedback visual
        if descs_to_explore:
            progress_placeholder = st.empty()
            progress_placeholder.info(f"üîç Explorando {len(descs_to_explore)} colunas de descri√ß√£o...")
        
        for i, col in enumerate(descs_to_explore, 1):
            print(f"[EXPLORE] üîç [{i}/{len(descs_to_explore)}] Consultando valores: {col}")
            if descs_to_explore:
                progress_placeholder.info(f"üîç Explorando descri√ß√µes: {col} ({i}/{len(descs_to_explore)})")
            
            try:
                # Limitar valores distintos por coluna conforme configura√ß√£o
                distinct_values = get_distinct_values(
                    table_name,
                    col,
                    limit=st.session_state.exploration_limit_per_column,
                    use_partition_filter=False  # Desabilitar filtro de parti√ß√£o
                )
                if distinct_values:
                    exploration['description_columns'][col] = distinct_values
                    print(f"[EXPLORE]   ‚úÖ {len(distinct_values)} valores √∫nicos")
                    print(f"[EXPLORE]   üìã Amostra: {distinct_values[:5]}")
            except Exception as e:
                print(f"[EXPLORE]   ‚ö†Ô∏è Erro: {str(e)[:100]}")
        
        if descs_to_explore:
            progress_placeholder.empty()
        
        # 5. Consultar valores distintos para c√≥digos (at√© 1000 valores por coluna)
        codes_to_explore = code_cols[:10]  # Limitar a 10 colunas de c√≥digo
        print(f"[EXPLORE] üîç Explorando {len(codes_to_explore)} colunas de c√≥digo (limite de valores: 1000)...")
        
        # Feedback visual
        if codes_to_explore:
            progress_placeholder = st.empty()
            progress_placeholder.info(f"üîç Explorando {len(codes_to_explore)} colunas de c√≥digo...")
        
        for i, col in enumerate(codes_to_explore, 1):
            print(f"[EXPLORE] üîç [{i}/{len(codes_to_explore)}] Consultando valores: {col}")
            if codes_to_explore:
                progress_placeholder.info(f"üîç Explorando c√≥digos: {col} ({i}/{len(codes_to_explore)})")
            
            try:
                # Limitar valores distintos por coluna conforme configura√ß√£o
                distinct_values = get_distinct_values(
                    table_name,
                    col,
                    limit=st.session_state.exploration_limit_per_column,
                    use_partition_filter=False  # Desabilitar filtro de parti√ß√£o
                )
                if distinct_values:
                    exploration['code_columns'][col] = distinct_values
                    print(f"[EXPLORE]   ‚úÖ {len(distinct_values)} c√≥digos √∫nicos")
                    print(f"[EXPLORE]   üìã Amostra: {distinct_values[:5]}")
            except Exception as e:
                print(f"[EXPLORE]   ‚ö†Ô∏è Erro: {str(e)[:100]}")
        
        if codes_to_explore:
            progress_placeholder.empty()
        
        # 6. Obter amostra dos dados (limite configur√°vel) com filtro de parti√ß√£o mais recente
        print(f"[EXPLORE] üìä Consultando amostra dos dados (parti√ß√£o mais recente)...")
        try:
            # Detectar coluna de parti√ß√£o (usando configura√ß√£o)
            configured_partition = st.session_state.partition_column_name
            partition_col = configured_partition if any(c['name'] == configured_partition for c in columns) else None
            sample_limit = max(10, int(st.session_state.exploration_sample_limit))
            if partition_col:
                sample_query = (
                    f"SELECT * FROM {table_name} "
                    f"WHERE {partition_col} = (SELECT MAX({partition_col}) FROM {table_name}) "
                    f"LIMIT {sample_limit}"
                )
            else:
                sample_query = f"SELECT * FROM {table_name} LIMIT {sample_limit}"
            sample_df = execute_query(sample_query)
            if sample_df is not None and not sample_df.empty:
                exploration['sample_data'] = sample_df
                print(f"[EXPLORE] ‚úÖ Amostra obtida: {len(sample_df)} linhas (limite {sample_limit})")
        except Exception as e:
            print(f"[EXPLORE] ‚ö†Ô∏è Erro ao obter amostra: {str(e)}")
        
        # 6. Cachear resultados
        st.session_state.table_exploration_cache[cache_key] = exploration
        print(f"[EXPLORE] ‚úÖ Explora√ß√£o conclu√≠da e cacheada!")
        print(f"[EXPLORE] üìä Resumo:")
        print(f"[EXPLORE]   - {len(exploration['columns'])} colunas")
        print(f"[EXPLORE]   - {len(exploration['flag_columns'])} flags mapeadas")
        print(f"[EXPLORE]   - {len(exploration['description_columns'])} descri√ß√µes mapeadas")
        
        return exploration
        
    except Exception as e:
        print(f"[EXPLORE] ‚ùå Erro na explora√ß√£o: {str(e)}")
        return exploration

def get_business_temporal_context():
    """Gera contexto temporal e de neg√≥cio para ajudar IA entender datas relativas."""
    today = datetime.now()
    
    temporal_context = f"""
üìÖ CONTEXTO TEMPORAL E DE NEG√ìCIO (Data de Refer√™ncia: {today.strftime('%Y-%m-%d')})

1. INTERPRETA√á√ÉO DE PER√çODOS:
   - "hoje" = {today.strftime('%Y-%m-%d')}
   - "ontem" = {(today - timedelta(days=1)).strftime('%Y-%m-%d')}
   - "esta semana" = desde {(today - timedelta(days=today.weekday())).strftime('%Y-%m-%d')}
   - "semana passada" = de {(today - timedelta(days=today.weekday()+7)).strftime('%Y-%m-%d')} at√© {(today - timedelta(days=today.weekday()+1)).strftime('%Y-%m-%d')}
   - "este m√™s" = desde {today.strftime('%Y-%m-01')}
   - "m√™s passado" = de {(today.replace(day=1) - timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')} at√© {(today.replace(day=1) - timedelta(days=1)).strftime('%Y-%m-%d')}
   - "√∫ltimos 7 dias" = desde {(today - timedelta(days=7)).strftime('%Y-%m-%d')}
   - "√∫ltimos 30 dias" = desde {(today - timedelta(days=30)).strftime('%Y-%m-%d')}
   - "√∫ltimos 3 meses" = desde {(today - timedelta(days=90)).strftime('%Y-%m-%d')}
   - "este ano" = desde {today.strftime('%Y-01-01')}
   - "ano passado" = de {(today.year-1)}-01-01 at√© {(today.year-1)}-12-31

2. CAMPOS DE DATA DISPON√çVEIS:
   - Identifique automaticamente colunas de data (tipos DATE, TIMESTAMP, ou nomes como data_*, dt_*, *_date)
   - Use essas colunas para filtros temporais

3. CONTEXTO DE NEG√ìCIO:
   - Per√≠odo de fechamento t√≠pico: final do m√™s
   - Hor√°rio comercial: 08:00-18:00 (considere para an√°lises hor√°rias)
   - Feriados: podem impactar volumes (considere mencionar se relevante)

4. BOAS PR√ÅTICAS TEMPORAIS:
   - Sempre use >= e < para intervalos de datas (n√£o <= no final)
   - Formato padr√£o: 'YYYY-MM-DD' ou 'YYYY-MM-DD HH:MM:SS'
   - Para "m√™s atual" ou "ano atual", considere se deve incluir dados parciais
"""
    return temporal_context

def get_context():
    """Get context with user-provided context if available"""
    # Use user-provided context if available, otherwise use default
    base_context = st.session_state.user_context.strip() if st.session_state.user_context.strip() else DEFAULT_CONTEXT
    
    # Adicionar contexto temporal e de neg√≥cio
    temporal_context = get_business_temporal_context()
    
    return f"""{base_context}

{temporal_context}
"""

# Resto das fun√ß√µes mantidas igual, exceto as que usam Bedrock
def get_databricks_connection():
    """Create Databricks SQL connection."""
    print(f"[BRIAN_CONNECTION] üîó Iniciando conex√£o Databricks...")
    
    if not DATABRICKS_AVAILABLE:
        print(f"[BRIAN_CONNECTION] ‚ùå Databricks SQL connector n√£o dispon√≠vel")
        st.error("‚ùå Databricks SQL connector not available. Install with: python -m poetry add databricks-sql-connector")
        return None
    
    print(f"[BRIAN_CONNECTION] ‚úÖ Databricks connector dispon√≠vel")
    
    try:
        config = st.session_state.connection_config
        print(f"[BRIAN_CONNECTION] üìã Configura√ß√£o:")
        print(f"[BRIAN_CONNECTION]   - server_hostname: {config['server_hostname'][:20]}...")
        print(f"[BRIAN_CONNECTION]   - http_path: {config['http_path']}")
        print(f"[BRIAN_CONNECTION]   - access_token: {'*' * 10}...")
        
        print(f"[BRIAN_CONNECTION] üîå Estabelecendo conex√£o...")
        connection = dbsql.connect(
            server_hostname=config['server_hostname'],
            http_path=config['http_path'],
            access_token=config['access_token'],
            verify=False
        )
        print(f"[BRIAN_CONNECTION] ‚úÖ Conex√£o estabelecida com sucesso!")
        return connection
        
    except Exception as e:
        print(f"[BRIAN_CONNECTION] ‚ùå Erro na conex√£o: {str(e)}")
        st.error(f"‚ùå Databricks Connection Error: {str(e)}")
        return None

def execute_query(query: str) -> Optional[pd.DataFrame]:
    """Execute SQL query against Databricks com valida√ß√µes de seguran√ßa.
    
    Args:
        query: SQL query string a ser executada
        
    Returns:
        DataFrame com os resultados ou None em caso de erro
        
    Raises:
        ValueError: Se a query for inv√°lida ou muito grande
    """
    # Valida√ß√µes de seguran√ßa
    if not query or not isinstance(query, str):
        print(f"[BRIAN_QUERY] ‚ùå Query inv√°lida")
        return None
    
    query = query.strip()
    if not query:
        print(f"[BRIAN_QUERY] ‚ùå Query vazia")
        return None
    
    # Validar tamanho da query
    if len(query) > MAX_SQL_LENGTH:
        print(f"[BRIAN_QUERY] ‚ùå Query muito grande: {len(query)} caracteres")
        st.error(f"‚ùå Query muito grande ({len(query)} caracteres). M√°ximo permitido: {MAX_SQL_LENGTH}")
        return None
    
    # Valida√ß√£o b√°sica contra SQL injection (comandos perigosos)
    dangerous_keywords = ['DROP', 'DELETE', 'TRUNCATE', 'ALTER', 'CREATE', 'INSERT', 'UPDATE']
    query_upper = query.upper()
    for keyword in dangerous_keywords:
        if keyword in query_upper:
            print(f"[BRIAN_QUERY] ‚ö†Ô∏è Palavra-chave perigosa detectada: {keyword}")
            st.warning(f"‚ö†Ô∏è Query cont√©m opera√ß√£o: {keyword}. Apenas SELECT √© permitido.")
            # N√£o bloquear completamente, apenas alertar
    
    print(f"[BRIAN_QUERY] üöÄ Iniciando execu√ß√£o da query...")
    print(f"[BRIAN_QUERY] Query: {query[:100]}...")
    
    connection = get_databricks_connection()
    if not connection:
        print(f"[BRIAN_QUERY] ‚ùå Falha na conex√£o Databricks")
        return None

    try:
        print(f"[BRIAN_QUERY] ‚úÖ Conex√£o estabelecida, executando query...")
        
        start_time = time.time()
        
        with connection.cursor() as cursor:
            print(f"[BRIAN_QUERY] üìù Cursor criado, executando SQL...")
            
            # Definir timeout da query (padr√£o 60 segundos)
            query_timeout = st.session_state.get('query_timeout_seconds', 60)
            
            try:
                # Adicionar timeout na query via SET STATEMENT
                # Nota: timeout pode n√£o funcionar em todas as vers√µes do Databricks
                # Usaremos limite de tempo no Python tamb√©m
                cursor.execute(query)
            except Exception as exec_error:
                error_msg = str(exec_error)
                print(f"[BRIAN_QUERY] ‚ùå Erro na execu√ß√£o SQL: {error_msg}")
                
                # Identificar tipo de erro para mensagem mais clara
                if "ParseException" in error_msg or "Syntax" in error_msg:
                    st.error("‚ùå Erro de sintaxe SQL. A query gerada pode estar incorreta.")
                elif "SemanticException" in error_msg or "cannot resolve" in error_msg:
                    st.error("‚ùå Coluna ou tabela n√£o encontrada. Verifique o contexto.")
                else:
                    st.error(f"‚ùå Erro na execu√ß√£o: {error_msg[:200]}")
                
                # Armazenar erros para rean√°lise
                if 'execution_errors' in st.session_state:
                    st.session_state.execution_errors.append(error_msg)
                
                return None
            
            print(f"[BRIAN_QUERY] ‚ö° Query executada, obtendo resultados...")
            
            if not cursor.description:
                print(f"[BRIAN_QUERY] ‚ö†Ô∏è Query n√£o retornou metadados (poss√≠vel query vazia)")
                return pd.DataFrame()
            
            columns = [desc[0] for desc in cursor.description]
            print(f"[BRIAN_QUERY] üìä Colunas obtidas: {len(columns)} colunas")
            
            rows = cursor.fetchall()
            execution_time = time.time() - start_time
            print(f"[BRIAN_QUERY] üìã Dados obtidos: {len(rows)} linhas em {execution_time:.2f}s")
            
            # Alertar sobre queries muito lentas
            # Armazenar tempo de execu√ß√£o no session_state para exibir no expander
            st.session_state.last_execution_time = execution_time
            
            if len(rows) > MAX_ROWS_FOR_ANALYSIS:
                print(f"[BRIAN_QUERY] ‚ö†Ô∏è Muitas linhas retornadas: {len(rows)}. Limitando an√°lise.")
            
            df = pd.DataFrame(rows, columns=columns)
            print(f"[BRIAN_QUERY] ‚úÖ DataFrame criado: {df.shape}")
            return df
            
    except Exception as e:
        error_message = str(e)
        print(f"[BRIAN_QUERY] ‚ùå Erro inesperado na execu√ß√£o: {error_message}")
        
        # Armazenar erros para fluxo de rean√°lise p√≥s-falha
        if 'execution_errors' in st.session_state:
            st.session_state.execution_errors.append(error_message)
        
        st.error(f"‚ùå Erro na execu√ß√£o: {error_message[:500]}")
        return None
        
    finally:
        if connection:
            try:
                print(f"[BRIAN_QUERY] üîí Fechando conex√£o...")
                connection.close()
                print(f"[BRIAN_QUERY] ‚úÖ Conex√£o fechada")
            except Exception as close_error:
                print(f"[BRIAN_QUERY] ‚ö†Ô∏è Erro ao fechar conex√£o: {str(close_error)}")

def generate_sql_with_databricks_llama(user_query: str) -> str:
    """Generate SQL using Databricks Llama model."""
    prompt = f"""
{get_context()}

User Query: {user_query}

Generate a SQL query that answers this question. Return only the SQL query, no explanations.
"""

    try:
        # This would integrate with Databricks Model Serving
        # For now, return a placeholder
        return f"SELECT * FROM {st.session_state.connection_config['catalog']}.{st.session_state.connection_config['schema']}.{st.session_state.ai_config['context_table']} LIMIT 100"
    except Exception as e:
        st.error(f"‚ùå Databricks Llama Error: {str(e)}")
        return ""

def get_table_columns(table_name: str) -> List[Dict[str, str]]:
    """
    Consulta colunas dispon√≠veis em uma tabela.
    Retorna lista de dicion√°rios com informa√ß√µes sobre cada coluna.
    USA CACHE para evitar m√∫ltiplas consultas.
    """
    # Verificar cache primeiro
    cache_key = f"table_columns_{table_name}"
    if cache_key in st.session_state:
        print(f"[BRIAN_COLUMNS] ‚úÖ Usando cache para {table_name}")
        return st.session_state[cache_key]
    
    print(f"[BRIAN_COLUMNS] üîç Consultando colunas da tabela: {table_name}")
    
    query = f"DESCRIBE TABLE {table_name}"
    print(f"[BRIAN_COLUMNS] üìù Query: {query}")
    
    try:
        df = execute_query(query)
        print(f"[BRIAN_COLUMNS] üìä Query executada, processando resultado...")
        
        if df is not None and not df.empty:
            print(f"[BRIAN_COLUMNS] üìã DataFrame recebido com {len(df)} linhas")
            print(f"[BRIAN_COLUMNS] üîë Colunas do DataFrame: {df.columns.tolist()}")
            
            columns_info = []
            for idx, row in df.iterrows():
                # Tentar diferentes nomes de colunas poss√≠veis
                col_name = row.get('col_name', row.get('name', row.get('column_name', '')))
                col_type = row.get('data_type', row.get('type', row.get('column_type', '')))
                col_comment = row.get('comment', '')
                
                print(f"[BRIAN_COLUMNS] üìÑ Linha {idx}: name={col_name}, type={col_type}")
                
                col_info = {
                    'name': str(col_name).strip(),
                    'type': str(col_type).strip(),
                    'comment': str(col_comment).strip() if col_comment else ''
                }
                
                # Ignorar linhas de metadados (como # Partitioning, etc)
                if col_info['name'] and not col_info['name'].startswith('#'):
                    columns_info.append(col_info)
                    print(f"[BRIAN_COLUMNS] ‚úÖ Coluna adicionada: {col_info['name']}")
            
            print(f"[BRIAN_COLUMNS] ‚úÖ Total de {len(columns_info)} colunas v√°lidas encontradas")
            if columns_info:
                print(f"[BRIAN_COLUMNS] üìã Primeiras colunas: {[c['name'] for c in columns_info[:5]]}")
            
            # Cachear resultado
            cache_key = f"table_columns_{table_name}"
            st.session_state[cache_key] = columns_info
            print(f"[BRIAN_COLUMNS] üíæ Colunas cacheadas")
            
            return columns_info
        else:
            print(f"[BRIAN_COLUMNS] ‚ö†Ô∏è DataFrame vazio ou None")
            return []
    except Exception as e:
        print(f"[BRIAN_COLUMNS] ‚ùå Erro ao consultar colunas: {str(e)}")
        import traceback
        print(f"[BRIAN_COLUMNS] üìú Traceback: {traceback.format_exc()}")
        return []

def get_distinct_values(table_name: str, column_name: str, limit: int = 100, 
                       contains: Optional[str] = None,
                       use_partition_filter: bool = False) -> List[str]:
    """Consulta valores distintos de uma coluna espec√≠fica com filtros e limites.
    
    Args:
        table_name: Nome completo da tabela
        column_name: Nome da coluna
        limit: N√∫mero m√°ximo de valores a retornar (padr√£o: 100)
        contains: String para filtrar valores (case-insensitive LIKE)
        use_partition_filter: Se True, usa filtro de parti√ß√£o (DESATIVADO por padr√£o para performance)
        
    Returns:
        Lista de valores distintos encontrados (strings)
    """
    # Valida√ß√µes
    if not table_name or not column_name:
        print(f"[BRIAN_DISTINCT] ‚ùå table_name ou column_name inv√°lidos")
        return []
    
    # Impor limite m√°ximo para evitar consultas muito pesadas
    if limit is None or limit > 1000:
        limit = 1000
    
    print(f"[BRIAN_DISTINCT] üîç Consultando valores distintos: {table_name}.{column_name} "
          f"(limit={limit}, contains={contains})")

    filter_clause = ""
    if contains:
        # Sanitizar entrada para prevenir SQL injection
        safe_contains = str(contains).replace("'", "''").replace("\\", "\\\\").lower()
        filter_clause = f" AND LOWER({column_name}) LIKE '%{safe_contains}%'"

    # Filtro de parti√ß√£o DESATIVADO por padr√£o (causa lentid√£o)
    # O filtro de parti√ß√£o adiciona uma subquery SELECT MAX que pode travar em tabelas grandes
    partition_filter = ""
    if use_partition_filter:
        try:
            partition_column = st.session_state.partition_column_name
            cols = [c['name'] for c in get_table_columns(table_name)]
            if partition_column in cols:
                partition_filter = f" AND {partition_column} = (SELECT MAX({partition_column}) FROM {table_name})"
        except Exception:
            partition_filter = ""

    query = f"""
    SELECT DISTINCT {column_name}
    FROM {table_name}
    WHERE {column_name} IS NOT NULL{partition_filter}{filter_clause}
    LIMIT {limit}
    """

    try:
        df = execute_query(query)
        if df is not None and not df.empty:
            values = [str(v) for v in df[column_name].tolist() if v is not None]
            print(f"[BRIAN_DISTINCT] ‚úÖ Encontrados {len(values)} valores distintos")
            return values
        else:
            print(f"[BRIAN_DISTINCT] ‚ö†Ô∏è Nenhum valor encontrado")
            return []
    except Exception as e:
        print(f"[BRIAN_DISTINCT] ‚ùå Erro ao consultar valores: {str(e)}")
        return []

def find_matching_values(user_values: List[str], column_values: List[str]) -> Dict[str, List[str]]:
    """
    Faz matching entre valores mencionados e valores reais, mas preserva TODAS as variantes compat√≠veis.
    √ötil para casos gen√©ricos (ex: "elite") onde precisamos incluir todas as variantes da coluna.
    """
    print(f"[BRIAN_MATCHING] üéØ Iniciando matching: {len(user_values)} valores do usu√°rio vs {len(column_values)} valores do banco")

    matches: Dict[str, List[str]] = {}
    normalized_column_values = [(col_val, str(col_val).lower().strip()) for col_val in column_values if col_val is not None]

    for user_val in user_values:
        user_val_lower = str(user_val).lower().strip()
        if not user_val_lower:
            continue

        candidates: List[str] = []
        exact: List[str] = []

        for original_val, col_val_lower in normalized_column_values:
            # Matching exato
            if user_val_lower == col_val_lower:
                exact.append(original_val)
                continue

            # Matching parcial (substring em qualquer dire√ß√£o)
            if user_val_lower in col_val_lower or col_val_lower in user_val_lower:
                candidates.append(original_val)
                continue

            # Matching por palavras em comum
            user_words = set(user_val_lower.split())
            col_words = set(col_val_lower.split())
            if user_words & col_words:
                candidates.append(original_val)

        # Deduplicar preservando ordem de apari√ß√£o
        def dedup(seq: List[str]) -> List[str]:
            seen = set()
            result: List[str] = []
            for item in seq:
                if item not in seen:
                    seen.add(item)
                    result.append(item)
            return result

        combined = dedup(exact + candidates)

        if combined:
            matches[user_val] = combined
            print(f"[BRIAN_MATCHING] ‚úÖ '{user_val}' -> {combined} (total: {len(combined)})")
        else:
            print(f"[BRIAN_MATCHING] ‚ö†Ô∏è Nenhum match encontrado para '{user_val}'")

    return matches

def get_table_metadata(table_name: str) -> Dict[str, Any]:
    """
    Coleta metadados da tabela para enriquecer contexto e gerar sugest√µes.
    Retorna contagem total, intervalos de data, amostras categ√≥ricas e stats num√©ricos simples.
    """
    print(f"[BRIAN_METADATA] üß† Coletando metadados da tabela: {table_name}")

    metadata: Dict[str, Any] = {
        'columns': [],
        'total_rows': None,
        'date_ranges': {},
        'categorical_samples': {},
        'numeric_stats': {}
    }

    columns_info = get_table_columns(table_name)
    metadata['columns'] = columns_info

    if not columns_info:
        print(f"[BRIAN_METADATA] ‚ö†Ô∏è Sem colunas, abortando metadados")
        return metadata

    # Total de linhas
    try:
        count_df = execute_query(f"SELECT COUNT(*) AS total_count FROM {table_name}")
        if count_df is not None and not count_df.empty:
            metadata['total_rows'] = int(count_df.iloc[0]['total_count'])
            print(f"[BRIAN_METADATA] ‚úÖ Total de linhas: {metadata['total_rows']}")
    except Exception as e:
        print(f"[BRIAN_METADATA] ‚ùå Erro ao contar linhas: {e}")

    # Identifica colunas por tipo
    date_cols = [c['name'] for c in columns_info if 'date' in c['type'].lower() or 'timestamp' in c['type'].lower()]
    numeric_cols = [c['name'] for c in columns_info if any(t in c['type'].lower() for t in ['int', 'double', 'float', 'decimal', 'bigint'])]
    categorical_cols = [c['name'] for c in columns_info if any(t in c['type'].lower() for t in ['string', 'char', 'varchar'])]

    # Intervalos de datas (limita a 3 colunas para evitar queries pesadas)
    if date_cols:
        cols_expr = ", ".join([f"MIN({col}) AS {col}_min, MAX({col}) AS {col}_max" for col in date_cols[:3]])
        try:
            date_df = execute_query(f"SELECT {cols_expr} FROM {table_name}")
            if date_df is not None and not date_df.empty:
                for col in date_cols[:3]:
                    metadata['date_ranges'][col] = {
                        'min': str(date_df.iloc[0].get(f"{col}_min", '')),
                        'max': str(date_df.iloc[0].get(f"{col}_max", ''))
                    }
                print(f"[BRIAN_METADATA] ‚úÖ Intervalos de data calculados")
        except Exception as e:
            print(f"[BRIAN_METADATA] ‚ùå Erro em intervalos de data: {e}")

    # Amostras categ√≥ricas (top 5 valores) - limita a 3 colunas
    for col in categorical_cols[:3]:
        try:
            sample_df = execute_query(
                f"SELECT {col} AS value, COUNT(*) AS cnt FROM {table_name} WHERE {col} IS NOT NULL GROUP BY {col} ORDER BY cnt DESC LIMIT 5"
            )
            if sample_df is not None and not sample_df.empty:
                metadata['categorical_samples'][col] = [str(v) for v in sample_df['value'].tolist()]
                print(f"[BRIAN_METADATA] ‚úÖ Amostra categ√≥rica para {col}: {metadata['categorical_samples'][col]}")
        except Exception as e:
            print(f"[BRIAN_METADATA] ‚ùå Erro ao coletar amostra de {col}: {e}")

    # Estat√≠sticas num√©ricas simples - limita a 3 colunas
    if numeric_cols:
        exprs = ", ".join([f"AVG({col}) AS {col}_avg, MIN({col}) AS {col}_min, MAX({col}) AS {col}_max" for col in numeric_cols[:3]])
        try:
            num_df = execute_query(f"SELECT {exprs} FROM {table_name}")
            if num_df is not None and not num_df.empty:
                for col in numeric_cols[:3]:
                    metadata['numeric_stats'][col] = {
                        'avg': str(num_df.iloc[0].get(f"{col}_avg", '')),
                        'min': str(num_df.iloc[0].get(f"{col}_min", '')),
                        'max': str(num_df.iloc[0].get(f"{col}_max", '')),
                    }
                print(f"[BRIAN_METADATA] ‚úÖ Estat√≠sticas num√©ricas coletadas")
        except Exception as e:
            print(f"[BRIAN_METADATA] ‚ùå Erro nas estat√≠sticas num√©ricas: {e}")

    return metadata

def generate_smart_questions(context: str, bedrock_client, exploration_cache: Dict = None) -> List[str]:
    """
    Gera perguntas inteligentes e relevantes baseadas no contexto da tabela.
    
    Args:
        context: Contexto da tabela (DEFAULT_CONTEXT ou user_context)
        bedrock_client: Cliente Bedrock para gera√ß√£o
        exploration_cache: Cache de explora√ß√£o com valores reais das colunas
        
    Returns:
        Lista de 5-8 perguntas sugeridas
    """
    print("[BRIAN_SUGGESTIONS] üéØ Gerando perguntas inteligentes baseadas no contexto...")
    
    # Preparar contexto de explora√ß√£o se dispon√≠vel
    exploration_context = ""
    if exploration_cache:
        if exploration_cache.get('description_columns'):
            exploration_context += "\nüìù VALORES DISPON√çVEIS NAS DESCRI√á√ïES:\n"
            for col, values in list(exploration_cache['description_columns'].items())[:5]:
                exploration_context += f"  - {col}: {values[:5]}\n"
        
        if exploration_cache.get('code_columns'):
            exploration_context += "\nüî¢ VALORES DISPON√çVEIS NOS C√ìDIGOS:\n"
            for col, values in list(exploration_cache['code_columns'].items())[:5]:
                exploration_context += f"  - {col}: {values[:5]}\n"
    
    prompt = f"""
Voc√™ √© um especialista em an√°lise de dados e gera√ß√£o de insights.

üìä CONTEXTO DA TABELA:
{context}
{exploration_context}

üéØ TAREFA:
Gere uma lista de 6-8 perguntas RELEVANTES e √öTEIS que um analista de neg√≥cios poderia fazer sobre estes dados.

üìã REGRAS IMPORTANTES:
1. As perguntas devem ser ESPEC√çFICAS ao contexto da tabela
2. Use os valores REAIS que existem nas colunas (n√£o invente valores)
3. Varie entre perguntas simples e complexas
4. Inclua diferentes tipos de an√°lise: contagem, soma, m√©dia, agrupamento, tend√™ncia temporal
5. As perguntas devem ser pr√°ticas e aplic√°veis ao neg√≥cio
6. Use linguagem natural e clara
7. N√ÉO repita o mesmo tipo de pergunta
8. Foque em m√©tricas e KPIs relevantes

üí° EXEMPLOS DO TIPO DE PERGUNTA (adapte ao contexto):
- "Qual a taxa de aprova√ß√£o nos √∫ltimos 30 dias?"
- "Qual bandeira tem maior volume de transa√ß√µes?"
- "Quantas transa√ß√µes foram feitas com Apple Pay este m√™s?"
- "Qual o ticket m√©dio por tipo de segmento?"
- "Compare o volume de Cr√©dito vs D√©bito na √∫ltima semana"

üéØ FORMATO DE SA√çDA:
Retorne APENAS as perguntas, uma por linha, sem numera√ß√£o, sem explica√ß√µes adicionais.
Cada pergunta deve ser completa e autoexplicativa.
"""

    try:
        response = bedrock_client.invoke_model(
            modelId=bedrock_client.model_id,
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 1000,
                "temperature": 0.7,
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            })
        )
        
        response_body = json.loads(response['body'].read())
        generated_text = response_body['content'][0]['text']
        
        # Processar resposta - dividir por linhas e limpar
        questions = []
        for line in generated_text.strip().split('\n'):
            line = line.strip()
            # Remover numera√ß√£o se houver (1., 2., etc)
            line = re.sub(r'^\d+[\.\)]\s*', '', line)
            # Remover marcadores (-, *, etc)
            line = re.sub(r'^[\-\*\‚Ä¢]\s*', '', line)
            if line and len(line) > 10:  # Ignorar linhas muito curtas
                questions.append(line)
        
        print(f"[BRIAN_SUGGESTIONS] ‚úÖ {len(questions)} perguntas geradas com sucesso")
        return questions[:8]  # Retornar no m√°ximo 8 perguntas
        
    except Exception as e:
        print(f"[BRIAN_SUGGESTIONS] ‚ùå Erro ao gerar perguntas: {str(e)}")
        # Retornar perguntas gen√©ricas como fallback
        return [
            "Qual o total de transa√ß√µes no √∫ltimo m√™s?",
            "Mostre a distribui√ß√£o por tipo de funcionalidade",
            "Qual a taxa de aprova√ß√£o geral?",
            "Compare o volume financeiro por bandeira",
            "Mostre as principais negativas e suas quantidades"
        ]

def check_sampling_alert(sql_query: str, df: Optional[pd.DataFrame]) -> Optional[str]:
    """Detecta LIMIT e alerta se resultados podem estar amostrados."""
    if not sql_query or df is None:
        return None
    limit_match = re.search(r"limit\s+(\d+)", sql_query, re.IGNORECASE)
    if limit_match:
        try:
            limit_value = int(limit_match.group(1))
        except ValueError:
            limit_value = None
        sample_size = len(df)
        if limit_value is not None:
            return (
                f"‚ö†Ô∏è ATEN√á√ÉO: DADOS LIMITADOS\n\n"
                f"üìä Mostrando apenas {sample_size:,} de potencialmente {limit_value:,}+ registros\n"
                "üí° Para an√°lises mais precisas, considere restringir o per√≠odo, usar agrega√ß√µes ou remover o LIMIT."
            )
    return None

def generate_smart_suggestions_for_empty_result(user_query: str, table_name: str, metadata: Dict[str, Any]) -> str:
    """Gera sugest√µes inteligentes quando a query n√£o retorna dados."""
    suggestions = ["‚ùå N√£o encontrei dados para a consulta."]

    if metadata.get('date_ranges'):
        ranges = [f"{col}: {vals.get('min')} at√© {vals.get('max')}" for col, vals in metadata['date_ranges'].items()]
        suggestions.append("üìÖ Per√≠odos dispon√≠veis: " + "; ".join(ranges))

    if metadata.get('categorical_samples'):
        cat_text = []
        for col, values in metadata['categorical_samples'].items():
            cat_text.append(f"{col}: {', '.join(values)}")
        suggestions.append("üéØ Valores dispon√≠veis: " + " | ".join(cat_text))

    if metadata.get('numeric_stats'):
        num_text = []
        for col, stats in metadata['numeric_stats'].items():
            num_text.append(f"{col} (min {stats.get('min')}, max {stats.get('max')})")
        suggestions.append("üìà Estat√≠sticas: " + ", ".join(num_text))

    suggestions.append(
        "‚ùì Perguntas sugeridas:\n"
        "1. Refinar per√≠odo ou valor usando os dispon√≠veis acima.\n"
        "2. Perguntar por totais agregados (COUNT, SUM).\n"
        f"3. Confirmar nomes de colunas da tabela {table_name}."
    )

    return "\n\n".join(suggestions)

def pre_validate_query_with_count(sql_query: str, table_name: Optional[str] = None) -> Dict[str, Any]:
    """Executa COUNT(*) antes da query principal para evitar resultados vazios."""
    if not sql_query:
        return {'is_empty': False}

    # Detecta tabela do FROM se n√£o informada
    if not table_name:
        table_match = re.search(r"from\s+([\w\.]+)", sql_query, re.IGNORECASE)
        if table_match:
            table_name = table_match.group(1)

    if not table_name:
        print("[BRIAN_PREVALIDATE] ‚ö†Ô∏è N√£o foi poss√≠vel identificar a tabela")
        return {'is_empty': False}

    where_clause = ""
    where_match = re.search(r"where\s+(.*)", sql_query, re.IGNORECASE | re.DOTALL)
    if where_match:
        where_clause = where_match.group(1)
        # Remover poss√≠veis ORDER/GROUP/LIMIT
        where_clause = re.split(r"\border\s+by\b|\bgroup\s+by\b|\blimit\b", where_clause, flags=re.IGNORECASE)[0].strip()

    count_sql = f"SELECT COUNT(*) AS total_count FROM {table_name}"
    if where_clause:
        count_sql += f" WHERE {where_clause}"

    print(f"[BRIAN_PREVALIDATE] üîç Executando pr√©-valida√ß√£o: {count_sql}")
    try:
        df = execute_query(count_sql)
        if df is not None and not df.empty:
            total = int(df.iloc[0]['total_count'])
            print(f"[BRIAN_PREVALIDATE] ‚úÖ COUNT = {total}")
            return {'is_empty': total == 0, 'total': total, 'table_name': table_name}
    except Exception as e:
        print(f"[BRIAN_PREVALIDATE] ‚ùå Erro na pr√©-valida√ß√£o: {e}")

    return {'is_empty': False}

def interpret_user_query(user_query: str, context: str) -> Dict[str, Any]:
    """Interpreta a query do usu√°rio e retorna racioc√≠nio estruturado sobre as decis√µes tomadas.
    
    Retorna:
        Dict com:
        - interpretation: Interpreta√ß√£o da query em linguagem clara
        - decisions: Decis√µes tomadas (per√≠odo, m√©tricas, filtros, agrupamentos)
        - reasoning: Racioc√≠nio completo passo-a-passo
        - final_prompt: Prompt final que ser√° usado para gerar SQL
    """
    print(f"[BRIAN_INTERPRET] üß† Interpretando query do usu√°rio: {user_query}")
    
    prompt = f"""
Voc√™ √© um assistente anal√≠tico que interpreta solicita√ß√µes de usu√°rios e explica seu racioc√≠nio.

Contexto dos dados:
{context}

Pergunta do usu√°rio: {user_query}

Por favor, analise a pergunta e retorne um JSON estruturado com:

1. interpretation: Como voc√™ entendeu a pergunta em linguagem clara e objetiva
2. decisions: Um objeto com as decis√µes tomadas:
   - period: Per√≠odo temporal (ex: "todo per√≠odo dispon√≠vel", "√∫ltimos 30 dias", "m√™s a m√™s em 2024")
   - metrics: Lista de m√©tricas a calcular (ex: ["taxa de aprova√ß√£o", "volume financeiro"])
   - filters: Lista de filtros aplicados (ex: ["bandeira = MASTERCARD", "produto ELITE"])
   - grouping: Agrupamento dos dados (ex: "por dia", "por m√™s", "por produto")
   - formulas: F√≥rmulas usadas se aplic√°vel (ex: "Taxa de aprova√ß√£o = (transa√ß√µes aprovadas / total transa√ß√µes) * 100")
3. reasoning: Racioc√≠nio passo-a-passo detalhado de como voc√™ chegou nessas decis√µes
4. clarifications: Lista de suposi√ß√µes ou decis√µes que voc√™ tomou por conta pr√≥pria

Exemplo para "taxa de aprova√ß√£o":
{{
  "interpretation": "Calcular a taxa de aprova√ß√£o de transa√ß√µes em todo o per√≠odo dispon√≠vel com abertura m√™s a m√™s",
  "decisions": {{
    "period": "Todo per√≠odo dispon√≠vel com abertura m√™s a m√™s (formato YYYYMM)",
    "metrics": ["Taxa de aprova√ß√£o (%)", "Quantidade total de transa√ß√µes", "Quantidade aprovadas"],
    "filters": ["considerar_negativa = 'SIM' (apenas transa√ß√µes que entram no c√°lculo)"],
    "grouping": "Agrupado por m√™s (SUBSTR(dt_tran, 1, 6))",
    "formulas": ["Taxa de aprova√ß√£o = (SUM(qt_tran WHERE cd_retorno = '00') / SUM(qt_tran)) * 100"]
  }},
  "reasoning": "1. Identifiquei que 'taxa de aprova√ß√£o' requer calcular a propor√ß√£o de transa√ß√µes aprovadas. 2. C√≥digo de retorno '00' indica aprova√ß√£o. 3. Devo usar apenas transa√ß√µes com considerar_negativa='SIM'. 4. Per√≠odo n√£o especificado, ent√£o considero todo per√≠odo. 5. Agrupamento por m√™s para mostrar evolu√ß√£o temporal.",
  "clarifications": ["Assumindo que deseja ver evolu√ß√£o mensal", "Considerando cd_retorno='00' como aprovado", "Incluindo apenas transa√ß√µes com flag considerar_negativa='SIM'"]
}}

Retorne APENAS o JSON, sem explica√ß√µes adicionais.
"""
    
    try:
        bedrock = get_bedrock_client()
        if not bedrock:
            return {
                'interpretation': user_query,
                'decisions': {},
                'reasoning': 'Interpreta√ß√£o n√£o dispon√≠vel',
                'clarifications': []
            }
        
        response = bedrock.invoke_model(
            modelId="us.anthropic.claude-sonnet-4-20250514-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 3000,
                "messages": [{"role": "user", "content": prompt}]
            })
        )
        
        response_body = json.loads(response['body'].read())
        text = response_body['content'][0]['text'].strip()
        
        # Extrair JSON da resposta
        json_match = re.search(r"\{.*\}", text, re.DOTALL)
        if json_match:
            interpretation = json.loads(json_match.group(0))
            print(f"[BRIAN_INTERPRET] ‚úÖ Interpreta√ß√£o gerada com sucesso")
            return interpretation
        else:
            print(f"[BRIAN_INTERPRET] ‚ö†Ô∏è N√£o foi poss√≠vel extrair JSON da resposta")
            return {
                'interpretation': user_query,
                'decisions': {},
                'reasoning': text,
                'clarifications': []
            }
            
    except Exception as e:
        print(f"[BRIAN_INTERPRET] ‚ùå Erro na interpreta√ß√£o: {e}")
        return {
            'interpretation': user_query,
            'decisions': {},
            'reasoning': f'Erro na interpreta√ß√£o: {str(e)}',
            'clarifications': []
        }

def validate_query_with_reasoning(sql_query: str, context: str) -> Dict[str, Any]:
    """Pede ao modelo uma auto-avalia√ß√£o da query antes da execu√ß√£o."""
    if not sql_query:
        return {'is_valid': True}

    prompt = f"""
Contexto de dados:
{context}

Analise a SQL abaixo e responda em JSON:
SQL: {sql_query}

Valide:
- Colunas existem?
- Filtros fazem sentido?
- Sintaxe Databricks correta?
- Probabilidade de retornar dados relevantes?

Retorne JSON no formato:
{{
  "is_valid": true|false,
  "confidence": 0-1,
  "reasoning": "...",
  "suggestions": ["...", "..."]
}}
"""

    try:
        bedrock = get_bedrock_client()
        if not bedrock:
            return {'is_valid': True}
        response = bedrock.invoke_model(
            modelId="us.anthropic.claude-sonnet-4-20250514-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 2000,
                "messages": [{"role": "user", "content": prompt}]
            })
        )
        response_body = json.loads(response['body'].read())
        text = response_body['content'][0]['text'].strip()
        json_match = re.search(r"\{.*\}", text, re.DOTALL)
        if json_match:
            return json.loads(json_match.group(0))
    except Exception as e:
        print(f"[BRIAN_REASONING] ‚ùå Erro na valida√ß√£o: {e}")

    return {'is_valid': True, 'confidence': 0.5, 'reasoning': 'Valida√ß√£o indispon√≠vel', 'suggestions': []}

def judge_user_query_intent(user_query: str, table_name: str) -> Dict[str, Any]:
    """Analisa a pergunta e sugere clarifica√ß√µes antes de gerar SQL."""
    print(f"[BRIAN_JUDGE] üß≠ Julgando inten√ß√£o da query: {user_query}")
    columns_info = get_table_columns(table_name)

    if not columns_info:
        print("[BRIAN_JUDGE] ‚ö†Ô∏è Sem colunas para julgar inten√ß√£o")
        return {'needs_clarification': False}

    date_cols = [c['name'] for c in columns_info if any(k in c['name'].lower() for k in ['data', 'dt_', 'date']) or 'date' in c['type'].lower()]
    sales_cols = [c['name'] for c in columns_info if any(k in c['name'].lower() for k in ['vend', 'sale', 'venda'])]
    numeric_cols = [c['name'] for c in columns_info if any(t in c['type'].lower() for t in ['int', 'double', 'float', 'decimal', 'bigint'])]

    prompt = f"""
Voc√™ √© um juiz de inten√ß√£o de consultas. Avalie se a pergunta est√° amb√≠gua e proponha uma formula√ß√£o mais espec√≠fica.

Pergunta do usu√°rio: {user_query}

Colunas da tabela {table_name}:
- Datas: {date_cols}
- Campos relacionados a venda (heur√≠stico): {sales_cols}
- M√©tricas num√©ricas: {numeric_cols[:10]}

Responda apenas em JSON no formato:
{{
  "needs_clarification": true|false,
  "message": "texto curto explicando a ambiguidade ou confirmando",
  "suggested_query": "pergunta ou SQL em linguagem natural sugerida"
}}

Regras:
- Marque needs_clarification=true se faltar especificar qual m√©trica ou qual campo de data.
- A sugest√£o deve ser concreta, sem ambiguidades, citando o campo de data e a m√©trica sugerida.
"""

    try:
        bedrock = get_bedrock_client()
        if not bedrock:
            return {'needs_clarification': False}
        response = bedrock.invoke_model(
            modelId="us.anthropic.claude-sonnet-4-20250514-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 1500,
                "messages": [{"role": "user", "content": prompt}]
            })
        )
        response_body = json.loads(response['body'].read())
        text = response_body['content'][0]['text'].strip()
        json_match = re.search(r"\{.*\}", text, re.DOTALL)
        if json_match:
            return json.loads(json_match.group(0))
    except Exception as e:
        print(f"[BRIAN_JUDGE] ‚ùå Erro ao julgar inten√ß√£o: {e}")

    return {'needs_clarification': False}

def update_last_query_context(user_query: str, sql_query: str, df: Optional[pd.DataFrame]):
    """Atualiza mem√≥ria conversacional com √∫ltima execu√ß√£o."""
    st.session_state.last_query = user_query
    result_count = len(df) if df is not None else 0
    column_count = len(df.columns) if df is not None else 0
    st.session_state.last_query_context = {
        'user_query': user_query,
        'sql_query': sql_query,
        'result_count': result_count,
        'column_count': column_count,
        'timestamp': datetime.now().isoformat()
    }
    st.session_state.conversation_history.append(st.session_state.last_query_context)

def extract_column_filters_from_query(user_query: str, available_columns: List[Dict[str, str]]) -> Dict[str, List[str]]:
    """
    Extrai poss√≠veis filtros de colunas mencionados na pergunta do usu√°rio.
    Usa IA para identificar quais colunas e valores o usu√°rio est√° mencionando.
    ADAPTATIVO: Usa apenas as colunas REALMENTE dispon√≠veis na tabela.
    
    Args:
        user_query: Pergunta do usu√°rio
        available_columns: Lista de colunas dispon√≠veis na tabela
    
    Returns:
        Dicion√°rio com coluna -> lista de valores mencionados
    """
    print(f"[BRIAN_EXTRACT] üìù Extraindo filtros da query: {user_query}")
    print(f"[BRIAN_EXTRACT] üìä Colunas dispon√≠veis: {[col['name'] for col in available_columns]}")
    
    # Criar lista de colunas para o prompt
    columns_list = "\n".join([f"  - {col['name']} ({col['type']})" + (f": {col['comment']}" if col['comment'] else "") 
                              for col in available_columns])
    
    prompt = f"""
Analise a seguinte pergunta do usu√°rio e identifique quais colunas e valores est√£o sendo mencionados para filtros.

Pergunta: {user_query}

COLUNAS DISPON√çVEIS NA TABELA (USE APENAS ESTAS):
{columns_list}

IMPORTANTE: 
- Use APENAS colunas que existem na lista acima
- Identifique qual coluna o usu√°rio provavelmente quer filtrar baseado no contexto
- Se o usu√°rio mencionar "produtos", identifique qual coluna tem informa√ß√£o de produto
- Se mencionar "status", identifique qual coluna tem status
- Seja inteligente para mapear inten√ß√£o do usu√°rio -> coluna real

Retorne APENAS um JSON no formato:
{{
    "nome_coluna_real": ["valor1", "valor2", ...],
    ...
}}

Se n√£o houver filtros mencionados, retorne: {{}}

Exemplos:
- Query: "Analise produtos FREE, ELITE e UNIQUE"
  Colunas: [..., "produto_nome", ...]
  Output: {{"produto_nome": ["FREE", "ELITE", "UNIQUE"]}}

- Query: "Status ativo"
  Colunas: [..., "ds_status", ...]
  Output: {{"ds_status": ["ativo"]}}
"""

    try:
        bedrock = get_bedrock_client()
        response = bedrock.invoke_model(
            modelId="us.anthropic.claude-sonnet-4-20250514-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 2000,
                "messages": [{"role": "user", "content": prompt}]
            })
        )
        response_body = json.loads(response['body'].read())
        result_text = response_body['content'][0]['text'].strip()
        
        # Extrair JSON da resposta
        json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
        if json_match:
            filters = json.loads(json_match.group(0))
            print(f"[BRIAN_EXTRACT] ‚úÖ Filtros extra√≠dos: {filters}")
            return filters
        else:
            print(f"[BRIAN_EXTRACT] ‚ö†Ô∏è Nenhum filtro identificado")
            return {}
            
    except Exception as e:
        print(f"[BRIAN_EXTRACT] ‚ùå Erro ao extrair filtros: {str(e)}")
        return {}

def enrich_context_with_column_values(user_query: str, base_context: str) -> str:
    """
    Enriquece o contexto consultando valores reais de colunas quando o usu√°rio menciona filtros.
    SISTEMA ADAPTATIVO: Consulta colunas dispon√≠veis primeiro.
    """
    print(f"\n{'='*80}")
    print(f"[BRIAN_ENRICH] üîß Iniciando enriquecimento de contexto ADAPTATIVO...")
    print(f"[BRIAN_ENRICH] üìù Query do usu√°rio: {user_query}")
    print(f"{'='*80}")
    
    # Verificar estado do session_state
    print(f"\n[BRIAN_ENRICH] üîç VERIFICANDO SESSION_STATE:")
    print(f"[BRIAN_ENRICH]   connection_config existe? {hasattr(st.session_state, 'connection_config')}")
    print(f"[BRIAN_ENRICH]   ai_config existe? {hasattr(st.session_state, 'ai_config')}")
    
    if hasattr(st.session_state, 'connection_config'):
        print(f"[BRIAN_ENRICH]   connection_config keys: {list(st.session_state.connection_config.keys())}")
    if hasattr(st.session_state, 'ai_config'):
        print(f"[BRIAN_ENRICH]   ai_config keys: {list(st.session_state.ai_config.keys())}")
    
    # Obter informa√ß√µes da tabela
    catalog = st.session_state.connection_config.get('catalog', 'prd')
    schema = st.session_state.connection_config.get('schema', 'sand_snd_box_cartao')
    context_table = st.session_state.ai_config.get('context_table', 'tb_pbi_autz_pampa_diar')
    
    print(f"\n[BRIAN_ENRICH] üìä CONFIGURA√á√ÉO OBTIDA:")
    print(f"[BRIAN_ENRICH]   catalog = '{catalog}'")
    print(f"[BRIAN_ENRICH]   schema = '{schema}'")
    print(f"[BRIAN_ENRICH]   context_table = '{context_table}'")
    
    if not (catalog and schema and context_table):
        print(f"[BRIAN_ENRICH] ‚ö†Ô∏è Configura√ß√£o de tabela incompleta - usando contexto base")
        print(f"[BRIAN_ENRICH] ‚ùå SISTEMA ADAPTATIVO N√ÉO PODE EXECUTAR")
        return base_context
    
    full_table_name = f"{catalog}.{schema}.{context_table}"
    print(f"[BRIAN_ENRICH] üéØ Tabela completa: {full_table_name}")
    print(f"[BRIAN_ENRICH] ‚úÖ Configura√ß√£o v√°lida - prosseguindo com sistema adaptativo\n")
    
    # PASSO 1: Consultar colunas dispon√≠veis na tabela
    print(f"[BRIAN_ENRICH] üìä PASSO 1: Consultando colunas dispon√≠veis...")
    available_columns = get_table_columns(full_table_name)
    
    if not available_columns:
        print(f"[BRIAN_ENRICH] ‚ùå N√£o foi poss√≠vel obter colunas, usando contexto base")
        return base_context
    
    print(f"[BRIAN_ENRICH] ‚úÖ Encontradas {len(available_columns)} colunas")
    
    # PASSO 2: Extrair filtros baseado nas colunas REAIS
    print(f"[BRIAN_ENRICH] üîç PASSO 2: Extraindo filtros da query...")
    filters = extract_column_filters_from_query(user_query, available_columns)
    
    # PASSO 3: Enriquecer contexto com informa√ß√µes das colunas (SEMPRE)
    enriched_context = base_context + "\n\n=== SISTEMA ADAPTATIVO ATIVADO ===\n"
    enriched_context += f"\n--- COLUNAS DISPON√çVEIS NA TABELA {full_table_name} ---\n"
    for col in available_columns:
        enriched_context += f"  - {col['name']} ({col['type']})"
        if col['comment']:
            enriched_context += f": {col['comment']}"
        enriched_context += "\n"
    
    if not filters:
        print(f"[BRIAN_ENRICH] ‚ÑπÔ∏è Nenhum filtro espec√≠fico detectado")
        enriched_context += "\n‚ö†Ô∏è IMPORTANTE: Use APENAS as colunas listadas acima.\n"
        print(f"[BRIAN_ENRICH] ‚úÖ Contexto enriquecido (sem filtros espec√≠ficos)")
        return enriched_context
    
    print(f"[BRIAN_ENRICH] ‚úÖ Filtros detectados: {filters}")
    enriched_context += "\n--- VALORES REAIS DAS COLUNAS (para matching preciso) ---\n"
    
    # PASSO 4: Para cada coluna mencionada, buscar valores reais
    for column, user_values in filters.items():
        print(f"[BRIAN_ENRICH] üîç PASSO 4: Processando coluna '{column}' com valores: {user_values}")

        # Buscar valores distintos com filtro por termo para pegar TODAS as variantes relevantes
        all_distinct: List[str] = []
        for uval in user_values:
            distinct_filtered = get_distinct_values(full_table_name, column, limit=2000, contains=str(uval))
            all_distinct.extend(distinct_filtered)

        # Se nada veio filtrado, fazer fallback amplo (sem filtro) com limite menor
        if not all_distinct:
            all_distinct = get_distinct_values(full_table_name, column, limit=500)

        # Deduplicar preservando ordem
        seen_vals = set()
        distinct_values: List[str] = []
        for v in all_distinct:
            if v not in seen_vals:
                seen_vals.add(v)
                distinct_values.append(v)

        if distinct_values:
            print(f"[BRIAN_ENRICH] ‚úÖ Encontrados {len(distinct_values)} valores distintos")
            # Fazer matching inteligente preservando todas as variantes compat√≠veis
            matches = find_matching_values(user_values, distinct_values)

            if matches:
                print(f"[BRIAN_ENRICH] ‚úÖ Matches encontrados: {matches}")
                enriched_context += f"\nColuna '{column}':\n"
                enriched_context += f"  Valores DISTINTOS encontrados na tabela (sample): {distinct_values[:20]}\n"
                enriched_context += f"  \n"
                enriched_context += f"  Valores mencionados pelo usu√°rio -> Valores EXATOS encontrados no banco:\n"
                for user_val, real_vals in matches.items():
                    enriched_context += f"  - '{user_val}' = {real_vals}\n"

                # Flatten de todos os valores aprovados para montar exemplo de filtro amplo
                matched_set = []
                for vals in matches.values():
                    for v in vals:
                        if v not in matched_set:
                            matched_set.append(v)

                enriched_context += f"\n  ‚ö†Ô∏è CR√çTICO: Use TODOS os valores listados acima (IN).\n"
                enriched_context += f"  ‚ö†Ô∏è N√ÉO use LIKE '%{user_values[0]}%' - prefira IN com valores exatos.\n"
                matched_values = "', '".join(str(v) for v in matched_set)
                enriched_context += f"  ‚ö†Ô∏è Exemplo correto: WHERE {column} IN ('{matched_values}')\n"
            else:
                # Nenhum match - mostrar valores dispon√≠veis
                print(f"[BRIAN_ENRICH] ‚ö†Ô∏è Nenhum match encontrado")
                enriched_context += f"\nColuna '{column}':\n"
                enriched_context += f"  ‚ö†Ô∏è ATEN√á√ÉO: Nenhum match exato encontrado para {user_values}\n"
                enriched_context += f"  Valores DISPON√çVEIS na coluna (sample de {len(distinct_values)}): {distinct_values[:30]}\n"
                enriched_context += f"  Verifique se os valores solicitados realmente existem.\n"
        else:
            print(f"[BRIAN_ENRICH] ‚ö†Ô∏è N√£o foi poss√≠vel consultar valores distintos")
            enriched_context += f"\nColuna '{column}': ‚ö†Ô∏è N√£o foi poss√≠vel consultar valores distintos\n"
    
    print(f"[BRIAN_ENRICH] ‚úÖ Contexto enriquecido com matching de valores")
    return enriched_context

def generate_sql_with_bedrock_claude(user_query: str) -> tuple:
    """Generate SQL using AWS Bedrock Claude with intelligent column value matching.
    
    Retorna:
        tuple: (sql_query, interpretation_data) onde interpretation_data cont√©m o racioc√≠nio da IA
    """
    print(f"\n{'='*80}")
    print(f"[BRIAN_AI] ü§ñ Iniciando gera√ß√£o de SQL com Bedrock Claude...")
    print(f"[BRIAN_AI] üìù Query do usu√°rio: {user_query}")
    print(f"{'='*80}\n")
    
    # PASSO 0: Interpretar a query do usu√°rio e gerar racioc√≠nio
    print(f"[BRIAN_AI] üß† PASSO 0: Interpretando query do usu√°rio...")
    base_context = get_context()
    interpretation_data = None
    
    # Sempre interpretar, mas especialmente importante no modo aut√¥nomo
    if st.session_state.feature_flags.get('query_reasoning', True):
        interpretation_data = interpret_user_query(user_query, base_context)
        print(f"[BRIAN_AI] ‚úÖ Interpreta√ß√£o conclu√≠da")
        # Armazenar interpreta√ß√£o no session state para exibi√ß√£o
        st.session_state['last_query_interpretation'] = interpretation_data
    
    # Verificar se veio do assistente
    assistant_context = ""
    if st.session_state.get('last_assistant_chat'):
        print(f"[BRIAN_AI] ü§ù Detectado contexto do Assistente de Consulta!")
        assistant_context = "\n\nü§ù CONTEXTO DO ASSISTENTE DE CONSULTA:\n"
        assistant_context += "Esta query foi refinada atrav√©s de conversa com o usu√°rio. Hist√≥rico:\n\n"
        for msg in st.session_state.last_assistant_chat:
            if msg['role'] == 'user':
                assistant_context += f"üë§ Usu√°rio: {msg['content']}\n"
            else:
                assistant_context += f"ü§ñ Assistente: {msg['content']}\n"
        assistant_context += f"\n‚úÖ QUERY FINAL CONSOLIDADA: {user_query}\n"
        assistant_context += "‚ö†Ô∏è Esta query J√Å FOI REFINADA - use exatamente o que foi acordado na conversa!\n\n"
    
    # Enriquecer contexto com valores reais de colunas
    print(f"[BRIAN_AI] üîß PASSO 1: Obtendo contexto base...")
    base_context = get_context()
    print(f"[BRIAN_AI] ‚úÖ Contexto base obtido ({len(base_context)} caracteres)")
    
    # Adicionar explora√ß√£o autom√°tica ao contexto
    full_table = f"{st.session_state.connection_config.get('catalog', 'prd')}.{st.session_state.connection_config.get('schema', 'sand_snd_box_cartao')}.{st.session_state.ai_config.get('context_table', 'tb_pbi_autz_pampa_diar')}"
    exploration = st.session_state.table_exploration_cache.get(full_table, {})
    
    exploration_context = "\n\n=== EXPLORA√á√ÉO AUTOM√ÅTICA DA TABELA ===\n"
    if exploration.get('flag_columns'):
        exploration_context += "\nüö© FLAGS E SEUS VALORES REAIS:\n"
        for col, values in list(exploration['flag_columns'].items())[:20]:
            exploration_context += f"  - {col}: {values}\n"
    
    if exploration.get('description_columns'):
        exploration_context += "\nüìù DESCRI√á√ïES E VALORES DISPON√çVEIS:\n"
        for col, values in list(exploration['description_columns'].items())[:20]:
            exploration_context += f"  - {col}: {len(values)} valores √∫nicos\n"
            exploration_context += f"    Exemplos: {values[:5]}\n"
    
    if exploration:
        exploration_context += "\n‚ö†Ô∏è CR√çTICO: Use APENAS valores que aparecem acima. S√£o dados REAIS da tabela.\n"
    else:
        exploration_context += "\n‚ö†Ô∏è Explora√ß√£o n√£o dispon√≠vel. Recomenda-se executar explora√ß√£o autom√°tica.\n"
    
    if st.session_state.feature_flags.get('context_enrichment', True):
        print(f"\n[BRIAN_AI] üîß PASSO 2: Enriquecendo contexto com valores de colunas...")
        enriched_context = enrich_context_with_column_values(user_query, base_context)
        enriched_context += exploration_context
        print(f"[BRIAN_AI] ‚úÖ Contexto enriquecido ({len(enriched_context)} caracteres)")
    else:
        print(f"\n[BRIAN_AI] üîß PASSO 2: Enriquecimento desabilitado - usando contexto base")
        enriched_context = base_context + exploration_context

    print(f"\n[BRIAN_AI] üîß PASSO 3: Enviando para Claude Sonnet 4...")
    
    prompt = f"""
{enriched_context}

{assistant_context}

User Query: {user_query}

Generate a SQL query that answers this question. Return only the SQL query, no explanations.

üî¥ REGRAS CR√çTICAS - LEIA COM ATEN√á√ÉO:

1. COLUNAS DISPON√çVEIS:
   - Use APENAS colunas listadas na se√ß√£o "COLUNAS DISPON√çVEIS NA TABELA"
   - Se uma coluna n√£o est√° na lista, ela N√ÉO EXISTE

2. FILTROS POR VALORES:
   - Se h√° se√ß√£o "VALORES REAIS DAS COLUNAS", voc√™ DEVE usar esses valores EXATOS
   - NUNCA use LIKE com % quando valores exatos s√£o fornecidos
   - Use WHERE coluna IN ('valor1', 'valor2', ...) ou WHERE coluna = 'valor'
   - Os valores fornecidos J√Å FORAM VERIFICADOS - eles EXISTEM no banco
   
3. EXEMPLO DO QUE FAZER:
   ‚úÖ CORRETO: WHERE dscr_prod_come IN ('ELITE MASTERCARD', 'UNIQUE VISA', 'FREE CARD')
   ‚ùå ERRADO: WHERE dscr_prod_come LIKE '%ELITE%'
   
4. SE N√ÉO H√Å VALORES EXATOS:
   - Apenas se n√£o houver se√ß√£o "VALORES REAIS DAS COLUNAS", a√≠ pode usar LIKE
   - Mas prefira sempre valores exatos quando dispon√≠veis

5. SE VEIO DO ASSISTENTE DE CONSULTA:
   - A query foi REFINADA atrav√©s de m√∫ltiplas intera√ß√µes
   - Use EXATAMENTE as especifica√ß√µes acordadas (per√≠odo, m√©tricas, filtros, agrupamentos)
   - A confian√ßa na query √© ALTA - implemente fielmente o que foi solicitado

Use sintaxe SQL do Databricks.
"""

    # Armazenar prompt final no session state para exibi√ß√£o opcional
    st.session_state['last_generation_prompt'] = prompt
    
    if not AWS_AVAILABLE:
        print(f"[BRIAN_AI] ‚ùå AWS Bedrock n√£o dispon√≠vel")
        st.error("‚ùå AWS Bedrock not available. Check AWS configuration.")
        return ("", None)
    
    print(f"[BRIAN_AI] ‚úÖ AWS Bedrock dispon√≠vel")
        
    try:
        print(f"[BRIAN_AI] üîó Obtendo cliente Bedrock...")
        bedrock = get_bedrock_client()
        if not bedrock:
            print(f"[BRIAN_AI] ‚ùå Falha ao obter cliente Bedrock")
            return ("", None)

        print(f"[BRIAN_AI] üìù Enviando prompt para Claude...")
        response = bedrock.invoke_model(
            modelId="us.anthropic.claude-sonnet-4-20250514-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 65536,
                "messages": [{"role": "user", "content": prompt}]
            })
        )

        print(f"[BRIAN_AI] üì• Resposta recebida do Claude, processando...")
        response_body = json.loads(response['body'].read())
        sql_query = response_body['content'][0]['text'].strip()

        # Extract SQL from response (remove markdown formatting if present)
        sql_match = re.search(r'```sql\s*(.*?)\s*```', sql_query, re.DOTALL)
        if sql_match:
            sql_query = sql_match.group(1)
            print(f"[BRIAN_AI] üîç SQL extra√≠do do markdown")
        
        print(f"[BRIAN_AI] ‚úÖ SQL gerado com sucesso: {len(sql_query)} caracteres")
        print(f"[BRIAN_AI] SQL preview: {sql_query[:100]}...")
        return (sql_query, interpretation_data)

    except Exception as e:
        print(f"[BRIAN_AI] ‚ùå Erro no Bedrock: {str(e)}")
        st.error(f"‚ùå AWS Bedrock Error: {str(e)}")
        return ("", None)

def generate_column_summary_for_ai(df: pd.DataFrame) -> str:
    """Generate a concise summary of columns for the AI prompt."""
    summary_lines = []
    for col in df.columns:
        dtype = str(df[col].dtype)
        summary_lines.append(f"- {col}: {dtype}")
    return "\n".join(summary_lines)


def infer_chart_spec_with_ai(df: pd.DataFrame, user_query: str, selected_type: str) -> Dict[str, Any]:
    """Pede √† IA um JSON de especifica√ß√£o de gr√°fico (seguro) com base no pedido do usu√°rio e nos dados."""
    bedrock = get_bedrock_client()
    if not bedrock or not st.session_state.feature_flags.get('ai_chart_builder', True):
        return {}

    # Usar amostra maior para melhor an√°lise, mas informar tamanho total
    total_rows = len(df)
    sample_df = df.head(100)  # Aumentado de 30 para 100
    numeric_cols = sample_df.select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = sample_df.select_dtypes(include=['object']).columns.tolist()
    date_cols = [c for c in sample_df.columns if 'date' in c.lower() or 'dt_' in c.lower() or 'period' in c.lower()]
    
    print(f"[AI_CHART] Dataset completo: {total_rows} linhas, amostra para IA: {len(sample_df)} linhas")

    manual_map = {
        'Barras': 'bar',
        'Linhas': 'line',
        'Barras + Linhas': 'combo',
        'Distribui√ß√£o': 'histogram',
        'Correla√ß√£o': 'scatter',
        'Dispers√£o': 'scatter',
        'Boxplot': 'box',
        'Matriz de Dispers√£o': 'scatter_matrix',
        'Funil': 'funnel'
    }
    forced_type = manual_map.get(selected_type) if selected_type != 'Autom√°tico' else None

    prompt = f"""
Voc√™ √© um especialista em Plotly. Gere APENAS um JSON v√°lido com a especifica√ß√£o do gr√°fico.
Use SOMENTE colunas dispon√≠veis e um dos tipos: bar, line, area, scatter, histogram, box, funnel, combo, scatter_matrix.

REGRAS IMPORTANTES:
- Respeite o tipo solicitado pelo usu√°rio no texto ou, se fornecido, o tipo for√ßado: {forced_type}
- Inclua chaves: chart_type, x, y, group_by (opcional), orientation ('v' ou 'h'), barmode (group/stack), agg (sum/avg/count/none), top_n (int), title
- Para combo, defina y como lista com at√© 2 m√©tricas
- S√≥ use colunas existentes: {list(sample_df.columns)}
- Colunas num√©ricas: {numeric_cols}
- Colunas categ√≥ricas: {categorical_cols}
- Colunas de data: {date_cols}

IMPORTANTE SOBRE LIMITA√á√ÉO DE DADOS:
- O dataset tem {total_rows} linhas TOTAIS
- Por padr√£o, use top_n: 0 (significa TODOS os dados, sem limite)
- S√ì use top_n > 0 se o usu√°rio EXPLICITAMENTE pedir "top 10", "top 20", "principais", etc.
- Exemplos:
  * "total de vendas" ‚Üí top_n: 0 (mostra tudo)
  * "top 10 produtos" ‚Üí top_n: 10 (limita a 10)
  * "principais 20 clientes" ‚Üí top_n: 20 (limita a 20)

DIFEREN√áA IMPORTANTE:
1. **group_by**: Use quando o usu√°rio quer AGRUPAR/SEGMENTAR dados por uma coluna (ex: "por segmento", "por categoria", "por regi√£o")
   - Exemplos: "vendas por segmento", "transa√ß√µes por bandeira", "receita por produto"
   - Deve ser uma coluna categ√≥rica do dataframe
   
2. **Cores visuais**: NUNCA coloque cores visuais (vermelho, azul, verde, etc) no JSON
   - Cores visuais ser√£o aplicadas automaticamente pelo sistema
   - N√ÉO inclua "color", "visual_color" ou similar no JSON

Exemplo CORRETO (sem limita√ß√£o):
{{
  "chart_type": "bar",
  "x": "ds_bandeira",
  "y": ["vl_tran"],
  "group_by": "ds_segmento",
  "orientation": "v",
  "barmode": "group",
  "agg": "sum",
  "top_n": 0,
  "title": "Volume por bandeira"
}}

Exemplo com limite (apenas se pedido):
{{
  "chart_type": "bar",
  "x": "ds_produto",
  "y": ["vl_vendas"],
  "orientation": "v",
  "barmode": "group",
  "agg": "sum",
  "top_n": 10,
  "title": "Top 10 Produtos"
}}

Pedido do usu√°rio: {user_query}
Responda somente com JSON.
"""

    try:
        response = bedrock.invoke_model(
            modelId="us.anthropic.claude-sonnet-4-20250514-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 1024,
                "temperature": 0.4,
                "messages": [{"role": "user", "content": prompt}]
            })
        )
        response_body = json.loads(response['body'].read())
        text = response_body['content'][0]['text'].strip()
        json_match = re.search(r"\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}", text, re.DOTALL)
        if json_match:
            spec = json.loads(json_match.group())
        else:
            spec = json.loads(text)
    except Exception as e:
        print(f"[AI_CHART] Erro ao gerar spec: {e}")
        return {}

    def col_ok(col):
        return isinstance(col, str) and col in sample_df.columns

    if 'x' in spec and not col_ok(spec['x']):
        spec.pop('x')
    if 'y' in spec:
        if isinstance(spec['y'], list):
            spec['y'] = [c for c in spec['y'] if col_ok(c)]
        elif not col_ok(spec['y']):
            spec.pop('y')
    # Validar group_by (novo nome para agrupamento)
    if 'group_by' in spec and spec.get('group_by') and not col_ok(spec['group_by']):
        spec.pop('group_by')
    # Remover 'color' se existir (deve usar 'group_by')
    if 'color' in spec:
        # Se color for uma coluna v√°lida, converter para group_by
        if col_ok(spec['color']):
            spec['group_by'] = spec['color']
        spec.pop('color')

    if forced_type:
        spec['chart_type'] = forced_type

    return spec

def interpret_chart_personalization(user_query: str) -> Dict[str, Any]:
    """Interpreta o pedido de personaliza√ß√£o E tipo de gr√°fico usando IA.
    
    Analisa a query do usu√°rio para extrair:
    - Tipo de gr√°fico mais adequado
    - Cor exata desejada
    - Se r√≥tulos s√£o obrigat√≥rios
    - Estilo visual preferido
    - Outras personaliza√ß√µes (transpar√™ncia, bordas, etc)
    
    Retorna:
        Dict com recomenda√ß√£o de tipo de gr√°fico e personaliza√ß√µes
    """
    print(f"[BRIAN_CHART] üé® Interpretando gr√°fico solicitado: {user_query}")
    
    prompt = f"""Voc√™ √© um especialista em visualiza√ß√£o de dados. Analise o pedido do usu√°rio e recomende o MELHOR tipo de gr√°fico e personaliza√ß√µes.

Pedido do usu√°rio: {user_query}

Tipos de gr√°ficos dispon√≠veis:
- bar: Gr√°fico de barras (compara√ß√µes, rankings, distribui√ß√µes)
- line: Gr√°fico de linhas (tend√™ncias, s√©ries temporais)
- area: Gr√°fico de √°rea (composi√ß√£o ao longo do tempo)
- scatter: Gr√°fico de dispers√£o (correla√ß√µes, rela√ß√µes entre vari√°veis)
- histogram: Histograma (distribui√ß√£o de frequ√™ncias)
- box: Boxplot (distribui√ß√£o com quartis)
- funnel: Funil (progress√£o, convers√£o)
- combo: Combinado barras + linhas (m√∫ltiplas m√©tricas)
- scatter_matrix: Matriz de dispers√£o (m√∫ltiplas correla√ß√µes)
- pie: Pizza (composi√ß√£o percentual)
- sunburst: Sunburst (hierarquia circular)
- treemap: Treemap (hierarquia com √°reas)
- waterfall: Waterfall (mudan√ßas incrementais)

Retorne um JSON estruturado com:
{{
  "recommended_chart_type": "tipo mais adequado entre os listados acima",
  "chart_type_reason": "porque recomenda este tipo",
  "alternative_chart_types": ["tipo alternativo 1", "tipo alternativo 2"],
  "has_color_request": true/false,
  "color_name": "nome da cor (se solicitado)",
  "color_hex": "c√≥digo hexadecimal",
  "requires_labels": true/false,
  "chart_style": "bold|minimal|detailed|gradient|outlined",
  "transparency_level": 1.0,
  "font_size": "small|normal|large",
  "font_weight": "normal|bold|light",
  "show_grid": true/false,
  "text_color": "#FFFFFF",
  "other_personalizations": []
}}

Regras:
1. Seja assertivo: Se pediu "gr√°fico roxo de pizza", retorne chart_type: pie com color_hex: #800080
2. R√≥tulos: Default true (obrigat√≥rio em dashboards), false apenas se pedir "limpo" ou "sem r√≥tulos"
3. Cores mapeadas:
   - vermelho/red ‚Üí #FF0000
   - azul/blue ‚Üí #0000FF
   - verde/green ‚Üí #00FF00
   - amarelo/yellow ‚Üí #FFFF00
   - laranja/orange ‚Üí #FF8C00
   - roxo/purple ‚Üí #800080
   - rosa/pink ‚Üí #FFC0CB
   - marrom/brown ‚Üí #8B4513
   - preto/black ‚Üí #000000
   - cinza/gray ‚Üí #808080
   - branco/white ‚Üí #FFFFFF

4. Tipo de gr√°fico: Escolha o MELHOR tipo:
   - "top 10 produtos" ‚Üí bar
   - "evolu√ß√£o mensal" ‚Üí line
   - "rela√ß√£o entre" ‚Üí scatter
   - "distribui√ß√£o" ‚Üí histogram
   - "funil de vendas" ‚Üí funnel
   - "composi√ß√£o" ‚Üí pie
   - Etc.

Retorne APENAS o JSON, sem explica√ß√µes.
"""
    
    try:
        bedrock = get_bedrock_client()
        if not bedrock:
            print(f"[BRIAN_CHART] ‚ö†Ô∏è Bedrock n√£o dispon√≠vel, retornando valores padr√£o")
            return _get_default_chart_personalization()
        
        response = bedrock.invoke_model(
            modelId="us.anthropic.claude-sonnet-4-20250514-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 2500,
                "messages": [{"role": "user", "content": prompt}]
            })
        )
        
        response_body = json.loads(response['body'].read())
        text = response_body['content'][0]['text'].strip()
        
        # Extrair JSON da resposta
        json_match = re.search(r"\{.*\}", text, re.DOTALL)
        if json_match:
            personalization = json.loads(json_match.group(0))
            print(f"[BRIAN_CHART] ‚úÖ Gr√°fico interpretado")
            print(f"[BRIAN_CHART]   - Tipo: {personalization.get('recommended_chart_type')}")
            print(f"[BRIAN_CHART]   - Motivo: {personalization.get('chart_type_reason')}")
            print(f"[BRIAN_CHART]   - Cor: {personalization.get('color_name')} ({personalization.get('color_hex')})")
            print(f"[BRIAN_CHART]   - R√≥tulos: {personalization.get('requires_labels')}")
            return personalization
        else:
            print(f"[BRIAN_CHART] ‚ö†Ô∏è N√£o foi poss√≠vel extrair JSON da resposta")
            return _get_default_chart_personalization()
            
    except Exception as e:
        print(f"[BRIAN_CHART] ‚ùå Erro na interpreta√ß√£o: {e}")
        return _get_default_chart_personalization()

def _get_default_chart_personalization() -> Dict[str, Any]:
    """Retorna personaliza√ß√£o padr√£o quando IA n√£o est√° dispon√≠vel."""
    return {
        "recommended_chart_type": "bar",  # Tipo padr√£o
        "chart_type_reason": "Tipo vers√°til para a maioria das an√°lises",
        "alternative_chart_types": ["line", "scatter", "histogram"],
        "has_color_request": False,
        "color_hex": "#1f77b4",  # Blue padr√£o do plotly
        "requires_labels": True,  # R√≥tulos sempre obrigat√≥rios
        "labels_description": "Valores num√©ricos em cada barra/ponto",
        "chart_style": "bold",
        "transparency_level": 1.0,
        "font_size": "normal",
        "font_weight": "normal",
        "show_grid": True,
        "text_color": "#FFFFFF",
        "other_personalizations": []
    }


def build_plotly_from_spec(df: pd.DataFrame, spec: Dict[str, Any], chart_counter: int, query_context: str = ""):
    """Constr√≥i um gr√°fico Plotly seguro a partir de um spec validado.
    
    Args:
        df: DataFrame com os dados
        spec: Especifica√ß√£o do gr√°fico
        chart_counter: Contador para keys √∫nicas
        query_context: Contexto da query para extra√ß√£o de cores
    """
    
    # Interpretar personaliza√ß√£o do gr√°fico com IA
    personalization = interpret_chart_personalization(query_context) if query_context else _get_default_chart_personalization()
    
    def format_value_label(col_name):
        """Retorna template de formata√ß√£o para valores com fonte."""
        col_lower = str(col_name).lower()
        # Detectar moeda
        if 'valor' in col_lower or 'vlr' in col_lower or 'receita' in col_lower or 'custo' in col_lower or 'price' in col_lower or '$' in col_lower:
            template = 'R$ %{y:,.1f}'
        # Detectar porcentagem
        elif 'perc' in col_lower or 'pct' in col_lower or '%' in col_lower or 'taxa' in col_lower or 'rate' in col_lower:
            template = '%{y:.1f}%'
        # Formato padr√£o com separador de milhares
        else:
            template = '%{y:,.0f}'
        
        return {
            'template': template,
            'font': dict(size=12, color='white', family='Arial, sans-serif')
        }
    
    chart_type = spec.get('chart_type', 'bar')
    
    # Verificar se a IA recomendou um tipo diferente baseado na interpreta√ß√£o
    if personalization.get('recommended_chart_type'):
        ai_recommended = personalization.get('recommended_chart_type')
        # Usar tipo recomendado pela IA se for compat√≠vel com os dados
        if ai_recommended in ['bar', 'line', 'area', 'scatter', 'histogram', 'box', 'funnel', 'combo', 'scatter_matrix', 'pie', 'sunburst', 'treemap', 'waterfall']:
            print(f"[BRIAN_CHART] üìä Usando tipo recomendado pela IA: {ai_recommended} (motivo: {personalization.get('chart_type_reason')})")
            chart_type = ai_recommended
    
    x = spec.get('x')
    y = spec.get('y')
    group_by = spec.get('group_by')  # Agrupamento por coluna (ex: segmento, categoria)
    barmode = spec.get('barmode', 'group')
    orientation = spec.get('orientation', 'v')
    agg = spec.get('agg', 'none')
    top_n = int(spec.get('top_n', 0) or 0)
    title = spec.get('title', 'Gr√°fico')
    
    print(f"[BRIAN_CHART] Constru√ß√£o do gr√°fico:")
    print(f"[BRIAN_CHART]   - Dataset original: {len(df)} linhas")
    print(f"[BRIAN_CHART]   - Tipo: {chart_type}")
    print(f"[BRIAN_CHART]   - X: {x}, Y: {y}")
    print(f"[BRIAN_CHART]   - Group by: {group_by}")
    print(f"[BRIAN_CHART]   - Agrega√ß√£o: {agg}")
    print(f"[BRIAN_CHART]   - Top N: {top_n} (0 = sem limite)")

    dff = df.copy()
    if agg in ['sum', 'avg', 'count'] and x and y:
        if isinstance(y, list) and y:
            y_cols = y
        elif isinstance(y, str):
            y_cols = [y]
        else:
            y_cols = []
        if y_cols:
            agg_map = {'sum': 'sum', 'avg': 'mean', 'count': 'count'}
            # Se houver group_by, agrupar por x E group_by
            if group_by:
                dff = dff.groupby([x, group_by])[y_cols].agg(agg_map[agg]).reset_index()
            else:
                dff = dff.groupby(x)[y_cols].agg(agg_map[agg]).reset_index()
            print(f"[BRIAN_CHART]   - Ap√≥s agrega√ß√£o: {len(dff)} linhas")

    # Aplicar top_n APENAS se for > 0 (ou seja, se o usu√°rio pediu explicitamente)
    if top_n > 0 and x:
        print(f"[BRIAN_CHART] ‚ö†Ô∏è Aplicando limite top_n={top_n}")
        original_count = len(dff)
        sort_col = None
        if isinstance(y, list) and y:
            sort_col = y[0]
        elif isinstance(y, str):
            sort_col = y
        else:
            sort_col = x
        dff = dff.sort_values(by=sort_col, ascending=False).head(top_n)
        print(f"[BRIAN_CHART]   - Ap√≥s top_n: {len(dff)} linhas")
        
        # Avisar usu√°rio se dados foram limitados
        if original_count > top_n:
            st.info(f"‚ÑπÔ∏è Gr√°fico mostrando top {top_n} de {original_count} registros. Para ver todos, pe√ßa 'sem limite' ou 'todos os dados'.")
    else:
        print(f"[BRIAN_CHART] ‚úÖ Usando TODOS os dados: {len(dff)} linhas")
        if len(dff) != len(df):
            st.info(f"üìä Gr√°fico constru√≠do com {len(dff)} grupos agregados a partir de {len(df)} linhas originais.")

    if chart_type == 'bar':
        if isinstance(y, list) and len(y) > 1:
            # Se usu√°rio pediu cor espec√≠fica, aplicar via color_discrete_sequence
            if personalization['has_color_request'] and not group_by:
                fig = px.bar(dff, x=x, y=y, barmode=barmode, orientation=orientation, title=title, 
                            color_discrete_sequence=[personalization['color_hex']])
            elif personalization['has_color_request'] and group_by:
                # Com group_by, usar color_discrete_map
                unique_vals = dff[group_by].unique()
                color_map = {val: personalization['color_hex'] for val in unique_vals}
                fig = px.bar(dff, x=x, y=y, barmode=barmode, color=group_by, orientation=orientation, title=title,
                            color_discrete_map=color_map)
            else:
                # Sem cor personalizada, usar group_by normalmente
                fig = px.bar(dff, x=x, y=y, barmode=barmode, color=group_by, orientation=orientation, title=title)
            
            # Adicionar r√≥tulos SEMPRE em todas as traces
            fmt = format_value_label(y[0])
            fig.update_traces(
                texttemplate=fmt['template'], 
                textposition='outside', 
                textfont=fmt['font'],
                selector=dict(type='bar')
            )
        else:
            y_col = y[0] if isinstance(y, list) else y
            # Se usu√°rio pediu cor espec√≠fica, aplicar via color_discrete_sequence
            if personalization['has_color_request'] and not group_by:
                fig = px.bar(dff, x=x, y=y_col, barmode=barmode, orientation=orientation, title=title,
                            color_discrete_sequence=[personalization['color_hex']])
            elif personalization['has_color_request'] and group_by:
                # Com group_by, usar color_discrete_map
                unique_vals = dff[group_by].unique()
                color_map = {val: personalization['color_hex'] for val in unique_vals}
                fig = px.bar(dff, x=x, y=y_col, barmode=barmode, color=group_by, orientation=orientation, title=title,
                            color_discrete_map=color_map)
            else:
                # Sem cor personalizada, usar group_by normalmente
                fig = px.bar(dff, x=x, y=y_col, color=group_by, barmode=barmode, orientation=orientation, title=title)
            
            # Adicionar r√≥tulos SEMPRE em todas as traces
            fmt = format_value_label(y_col)
            fig.update_traces(
                texttemplate=fmt['template'], 
                textposition='outside', 
                textfont=fmt['font'],
                selector=dict(type='bar')
            )
        return fig, f"plotly_bar_ai_{chart_counter}"

    if chart_type == 'line':
        y_col = y[0] if isinstance(y, list) else y
        # Aplicar cor personalizada se solicitada
        if personalization['has_color_request'] and not group_by:
            fig = px.line(dff, x=x, y=y, title=title, color_discrete_sequence=[personalization['color_hex']], markers=True)
        elif personalization['has_color_request'] and group_by:
            unique_vals = dff[group_by].unique()
            color_map = {val: personalization['color_hex'] for val in unique_vals}
            fig = px.line(dff, x=x, y=y, color=group_by, title=title, color_discrete_map=color_map, markers=True)
        else:
            fig = px.line(dff, x=x, y=y, color=group_by, title=title, markers=True)
        
        # Adicionar r√≥tulos SEMPRE em todas as traces
        fmt = format_value_label(y_col)
        fmt['font']['color'] = 'black'
        fig.update_traces(
            mode='lines+markers+text',
            texttemplate=fmt['template'], 
            textposition='top center', 
            textfont=fmt['font'],
            selector=dict(type='scatter')
        )
        return fig, f"plotly_line_ai_{chart_counter}"

    if chart_type == 'area':
        y_col = y[0] if isinstance(y, list) else y
        # Aplicar cor personalizada se solicitada
        if personalization['has_color_request'] and not group_by:
            fig = px.area(dff, x=x, y=y, title=title, color_discrete_sequence=[personalization['color_hex']])
        elif personalization['has_color_request'] and group_by:
            unique_vals = dff[group_by].unique()
            color_map = {val: personalization['color_hex'] for val in unique_vals}
            fig = px.area(dff, x=x, y=y, color=group_by, title=title, color_discrete_map=color_map)
        else:
            fig = px.area(dff, x=x, y=y, color=group_by, title=title)
        
        # Adicionar r√≥tulos SEMPRE em todas as traces
        fmt = format_value_label(y_col)
        fmt['font']['color'] = 'black'
        fig.update_traces(
            mode='lines+markers+text',
            texttemplate=fmt['template'], 
            textposition='top center', 
            textfont=fmt['font'],
            selector=dict(type='scatter')
        )
        return fig, f"plotly_area_ai_{chart_counter}"

    if chart_type == 'scatter':
        y_col = y[0] if isinstance(y, list) else y
        # Aplicar cor personalizada se solicitada
        if personalization['has_color_request'] and not group_by:
            fig = px.scatter(dff, x=x, y=y, title=title, color_discrete_sequence=[personalization['color_hex']])
        elif personalization['has_color_request'] and group_by:
            unique_vals = dff[group_by].unique()
            color_map = {val: personalization['color_hex'] for val in unique_vals}
            fig = px.scatter(dff, x=x, y=y, color=group_by, title=title, color_discrete_map=color_map)
        else:
            fig = px.scatter(dff, x=x, y=y, color=group_by, title=title)
        
        # Adicionar r√≥tulos SEMPRE em todas as traces
        fmt = format_value_label(y_col)
        fmt['font']['color'] = 'black'
        fig.update_traces(
            mode='markers+text',
            texttemplate=fmt['template'], 
            textposition='top center', 
            textfont=fmt['font'],
            selector=dict(type='scatter')
        )
        return fig, f"plotly_scatter_ai_{chart_counter}"

    if chart_type == 'histogram':
        # Aplicar cor personalizada se solicitada
        if personalization['has_color_request'] and not group_by:
            fig = px.histogram(dff, x=y if not x else x, title=title, color_discrete_sequence=[personalization['color_hex']])
        elif personalization['has_color_request'] and group_by:
            unique_vals = dff[group_by].unique()
            color_map = {val: personalization['color_hex'] for val in unique_vals}
            fig = px.histogram(dff, x=y if not x else x, color=group_by, title=title, color_discrete_map=color_map)
        else:
            fig = px.histogram(dff, x=y if not x else x, color=group_by, title=title)
        
        # Adicionar r√≥tulos SEMPRE em todas as traces
        fig.update_traces(
            texttemplate='%{y:,.0f}', 
            textposition='outside', 
            textfont=dict(size=12, color='white'),
            selector=dict(type='bar')
        )
        return fig, f"plotly_hist_ai_{chart_counter}"

    if chart_type == 'box':
        # Aplicar cor personalizada se solicitada
        if personalization['has_color_request'] and not group_by:
            fig = px.box(dff, x=x, y=y, title=title, color_discrete_sequence=[personalization['color_hex']])
        elif personalization['has_color_request'] and group_by:
            unique_vals = dff[group_by].unique()
            color_map = {val: personalization['color_hex'] for val in unique_vals}
            fig = px.box(dff, x=x, y=y, color=group_by, title=title, color_discrete_map=color_map)
        else:
            fig = px.box(dff, x=x, y=y, color=group_by, title=title)
        return fig, f"plotly_box_ai_{chart_counter}"

    if chart_type == 'funnel':
        y_col = y if y else x
        # Aplicar cor personalizada se solicitada
        if personalization['has_color_request'] and not group_by:
            fig = px.funnel(dff, x=y if y else None, y=x, title=title, color_discrete_sequence=[personalization['color_hex']])
        elif personalization['has_color_request'] and group_by:
            unique_vals = dff[group_by].unique()
            color_map = {val: personalization['color_hex'] for val in unique_vals}
            fig = px.funnel(dff, x=y if y else None, y=x, color=group_by, title=title, color_discrete_map=color_map)
        else:
            fig = px.funnel(dff, x=y if y else None, y=x, color=group_by, title=title)
        
        # Adicionar r√≥tulos SEMPRE em todas as traces
        fmt = format_value_label(y_col)
        fig.update_traces(
            texttemplate=fmt['template'], 
            textposition='inside', 
            textfont=fmt['font'],
            selector=dict(type='funnel')
        )
        return fig, f"plotly_funnel_ai_{chart_counter}"

    if chart_type == 'combo':
        if isinstance(y, list) and len(y) >= 2 and x:
            # Ordenar por x se for data/temporal
            try:
                dff = dff.sort_values(by=x)
            except:
                pass
            
            fig = make_subplots(specs=[[{"secondary_y": True}]])
            # Barras com r√≥tulos e cor personalizada
            fmt_bar = format_value_label(y[0])
            bar_color = personalization['color_hex'] if personalization['has_color_request'] else '#1f77b4'
            fig.add_trace(
                go.Bar(x=dff[x], y=dff[y[0]], name=y[0],
                      text=dff[y[0]], texttemplate=fmt_bar['template'], textposition='outside',
                      textfont=fmt_bar['font'], marker_color=bar_color)
            )
            # Linha com r√≥tulos e cor diferente
            fmt_line = format_value_label(y[1])
            fmt_line['font']['color'] = 'black'
            line_color = '#FF7F0E'  # Cor diferente para linha
            fig.add_trace(
                go.Scatter(x=dff[x], y=dff[y[1]], mode='lines+markers+text', name=y[1],
                          text=dff[y[1]], texttemplate=fmt_line['template'], textposition='top center',
                          textfont=fmt_line['font'], line_color=line_color),
                secondary_y=True
            )
            fig.update_layout(title_text=title)
            return fig, f"plotly_combo_ai_{chart_counter}"

    if chart_type == 'scatter_matrix':
        if isinstance(y, list) and len(y) >= 2:
            cols = y[:4]
        else:
            num_cols = dff.select_dtypes(include=[np.number]).columns.tolist()[:4]
            cols = num_cols
        fig = px.scatter_matrix(dff, dimensions=cols, color=group_by, title=title)
        return fig, f"plotly_scatter_mtx_ai_{chart_counter}"
    
    # Gr√°ficos de composi√ß√£o
    if chart_type == 'pie':
        y_col = y[0] if isinstance(y, list) else y
        x_col = x if x else categorical_cols[0] if 'categorical_cols' in locals() else None
        if x_col and y_col:
            # Aplicar cor personalizada se solicitada
            if personalization['has_color_request']:
                fig = px.pie(dff, names=x_col, values=y_col, title=title, 
                           color_discrete_sequence=[personalization['color_hex']])
            else:
                fig = px.pie(dff, names=x_col, values=y_col, title=title)
            # Adicionar percentuais nos r√≥tulos
            fig.update_traces(textposition='inside', textinfo='percent+label')
            return fig, f"plotly_pie_ai_{chart_counter}"
    
    if chart_type == 'sunburst':
        # Para sunburst, precisa de hierarquia
        y_col = y[0] if isinstance(y, list) else y
        if x and y_col:
            fig = px.sunburst(dff, names=x, values=y_col, title=title)
            return fig, f"plotly_sunburst_ai_{chart_counter}"
    
    if chart_type == 'treemap':
        # Para treemap, tamb√©m precisa de valores e labels
        y_col = y[0] if isinstance(y, list) else y
        if x and y_col:
            fig = px.treemap(dff, names=x, values=y_col, title=title)
            return fig, f"plotly_treemap_ai_{chart_counter}"
    
    if chart_type == 'waterfall':
        # Para waterfall, precisa de x (categoria) e y (valor)
        y_col = y[0] if isinstance(y, list) else y
        if x and y_col:
            # Aplicar cor personalizada se solicitada
            if personalization['has_color_request']:
                fig = px.bar(dff, x=x, y=y_col, title=title, color_discrete_sequence=[personalization['color_hex']])
            else:
                fig = px.bar(dff, x=x, y=y_col, title=title)
            
            # Adicionar r√≥tulos SEMPRE
            fmt = format_value_label(y_col)
            fig.update_traces(texttemplate=fmt['template'], textposition='outside', textfont=fmt['font'])
            return fig, f"plotly_waterfall_ai_{chart_counter}"

    return None, None


def generate_smart_visualization(df: pd.DataFrame, query_context: str = ""):
    """
    Gera visualiza√ß√µes inteligentes usando ECharts (moderno e interativo)
    
    Features:
    - Detec√ß√£o autom√°tica de tipo de gr√°fico via keywords
    - Gr√°ficos modernos e interativos (ECharts)
    - Suporte para m√∫ltiplos tipos: barras, linhas, pizza, √°rea, dispers√£o, funil, combo
    - Cores personaliz√°veis
    - Responsivo e profissional
    """
    
    # Mapa de cores (inclui formas femininas/variantes)
    color_map = {
        'vermelho': '#FF0000', 'vermelha': '#FF0000', 'red': '#FF0000',
        'azul': '#0000FF', 'azuis': '#0000FF', 'blue': '#0000FF',
        'verde': '#00AA00', 'green': '#00AA00',
        'amarelo': '#FFFF00', 'amarela': '#FFFF00', 'yellow': '#FFFF00',
        'laranja': '#FFA500', 'orange': '#FFA500',
        'roxo': '#800080', 'roxa': '#800080', 'purple': '#800080',
        'rosa': '#FFC0CB', 'pink': '#FFC0CB',
        'marrom': '#8B4513', 'brown': '#8B4513',
        'cinza': '#808080', 'cizento': '#808080', 'gray': '#808080', 'grey': '#808080',
        'preto': '#000000', 'preta': '#000000', 'black': '#000000',
    }

    st.subheader("üìä Visualiza√ß√£o Inteligente")
    
    if not ECHARTS_AVAILABLE:
        st.warning("‚ö†Ô∏è ECharts n√£o dispon√≠vel. Instale com: pip install streamlit-echarts")
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        if numeric_cols:
            st.bar_chart(df[numeric_cols[0]])
        return

    # DEBUG: Mostrar informa√ß√µes do DataFrame original
    with st.expander("üîç Debug: Dados Recebidos", expanded=False):
        st.write(f"**Shape:** {df.shape}")
        st.write(f"**Colunas:** {list(df.columns)}")
        st.write(f"**Tipos originais:**")
        st.dataframe(pd.DataFrame({'Coluna': df.columns, 'Tipo': df.dtypes.astype(str), 'Sample': [str(df[col].iloc[0]) if len(df) > 0 else 'N/A' for col in df.columns]}))
        st.write("**Primeiras linhas:**")
        st.dataframe(df.head())

    # Converter Decimal para float para compatibilidade com JSON/ECharts
    from decimal import Decimal
    df = df.copy()
    
    # Convers√£o robusta de Decimal
    for col in df.columns:
        # Verificar tipo da coluna
        if df[col].dtype == 'object':
            # Se √© object, verificar se cont√©m Decimal
            if len(df) > 0:
                first_val = df[col].iloc[0]
                if isinstance(first_val, Decimal):
                    df[col] = pd.to_numeric(df[col], errors='coerce')
        else:
            # Para colunas num√©ricas, verificar se h√° Decimal
            if df[col].apply(lambda x: isinstance(x, Decimal)).any():
                df[col] = df[col].apply(lambda x: float(x) if isinstance(x, Decimal) else x)
    
    # Garantir que todos os valores num√©ricos sejam float
    for col in df.select_dtypes(include=[np.number]).columns:
        df[col] = df[col].astype(float)

    # Analisar tipos de dados
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
    date_cols = []
    
    # Detectar colunas discretas que podem servir como categorias (ex: m√™s, ano)
    discrete_cols = []
    for col in numeric_cols:
        unique_count = df[col].nunique()
        if unique_count <= 50:  # At√© 50 valores √∫nicos = considerado discreto
            discrete_cols.append(col)
    
    # Detectar colunas de data
    for col in df.columns:
        if 'date' in col.lower() or 'dt_' in col.lower() or 'period' in col.lower() or 'data' in col.lower():
            try:
                pd.to_datetime(df[col], errors='coerce')
                date_cols.append(col)
            except:
                pass
    
    # Criar lista de poss√≠veis eixos X (categ√≥ricas, discretas ou datas)
    possible_x_cols = categorical_cols + discrete_cols + date_cols
    # Remover duplicatas mantendo ordem
    possible_x_cols = list(dict.fromkeys(possible_x_cols))
    
    # Colunas num√©ricas para eixo Y (excluindo as usadas como X)
    numeric_y_cols = [col for col in numeric_cols if col not in discrete_cols]
    if not numeric_y_cols:  # Se todas s√£o discretas, usar todas mesmo
        numeric_y_cols = numeric_cols

    query_lower = query_context.lower()
    selected_type = st.session_state.get('chart_type', 'Autom√°tico')

    user_color = _extract_colors_from_query(query_context)

    # Verificar se usu√°rio quer gr√°fico - expandir keywords
    chart_keywords = ['gr√°fico', 'grafico', 'chart', 'barras', 'barra', 'linhas', 'linha', 'pizza', 
                      'funnel', 'funil', 'scatter', 'dispers√£o', 'distribui√ß√£o', 
                      'evolu√ß√£o', 'trend', 'combo', '√°rea', 'area', 'plot', 'visualiza√ß√£o', 'visualizacao']
    wants_chart = any(k in query_lower for k in chart_keywords) or selected_type != 'Autom√°tico'
    
    if not wants_chart:
        # Mesmo sem palavra-chave, tentar gr√°fico autom√°tico se tiver dados adequados
        if not (possible_x_cols and numeric_y_cols) and not (len(numeric_cols) >= 2):
            return
    
    # Tentar gera√ß√£o completa via IA (option ECharts customizado)
    if wants_chart and st.session_state.feature_flags.get('ai_chart_builder', True):
        # Preparar dicas/contexto para a IA (sem hardcode de gr√°ficos)
        try:
            hint_x = chosen_x if 'chosen_x' in locals() else (possible_x_cols[0] if possible_x_cols else None)
            hint_y = chosen_y if 'chosen_y' in locals() else (numeric_y_cols[0] if numeric_y_cols else None)
        except Exception:
            hint_x = possible_x_cols[0] if possible_x_cols else None
            hint_y = numeric_y_cols[0] if numeric_y_cols else None

        # Construir um query/contexto enriquecido para a IA
        ai_query = query_context + "\n\nContexto para o gerador de gr√°ficos:\n"
        ai_query += f"Colunas dispon√≠veis: {list(df.columns)}\n"
        if hint_x and hint_y:
            ai_query += f"Preferir X={hint_x} e Y={hint_y} quando fizer sentido.\n"
        # Prioridade sem hardcode: indicar que colunas contendo 'taxa'/'aprov' devem ser tratadas como percentuais
        ai_query += "Se a m√©trica for uma taxa (coluna com 'taxa'/'aprov'/'percent' no nome), trate os valores como percentuais e formate eixo Y com '%' e labels com '{c}%'.\n"
        if user_color:
            ai_query += f"Prefer√™ncia de cor: {user_color} (usar como cor principal das s√©ries).\n"

        option_ai = try_generate_ai_echarts_option(df, ai_query, user_color, possible_x_cols, numeric_cols)
        if option_ai:
            st_echarts(option_ai, height="520px")
            return

        # Fallback: pedir √† IA um snippet HTML/JS (ECharts) com os dados incorporados
        web_html = try_generate_ai_web_chart_html(df, ai_query, user_color, possible_x_cols, numeric_cols)
        if web_html:
            import streamlit.components.v1 as components
            components.html(web_html, height=540, scrolling=True)
            return
    
    # Se chegou aqui, nenhuma gera√ß√£o AI bem-sucedida
    st.warning("‚ùå N√£o consegui gerar um gr√°fico baseado no seu pedido. Tente ser mais espec√≠fico (ex: 'gr√°fico de barras de transa√ß√µes por per√≠odo em vermelho').")


def render_manual_chart_panel(df: pd.DataFrame, query_context: str = ""):
    """Exibe painel colaps√°vel para gerar gr√°fico manual sem limpar a tela."""
    if df is None or df.empty:
        return

    with st.expander("üìä Gerar gr√°fico manual", expanded=False):
        # Preparar listas de colunas
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
        discrete_cols = []
        for col in numeric_cols:
            if df[col].nunique() <= 50:
                discrete_cols.append(col)
        date_cols = []
        for col in df.columns:
            if any(k in col.lower() for k in ['date', 'dt_', 'period', 'data']):
                try:
                    pd.to_datetime(df[col], errors='coerce')
                    date_cols.append(col)
                except Exception:
                    pass

        possible_x_cols = list(dict.fromkeys(categorical_cols + discrete_cols + date_cols))
        numeric_y_cols = [c for c in numeric_cols if c not in discrete_cols] or numeric_cols

        # Cor padr√£o a partir do texto
        user_color = _extract_colors_from_query(query_context)

        with st.form("manual_chart_form"):
            col1, col2 = st.columns(2)
            with col1:
                chart_type = st.selectbox(
                    "Tipo de gr√°fico",
                    ["Autom√°tico", "Barras", "Linhas", "√Årea", "Pizza", "Dispers√£o", "Funil", "Combo"],
                    index=["Autom√°tico", "Barras", "Linhas", "√Årea", "Pizza", "Dispers√£o", "Funil", "Combo"].index(st.session_state.get('manual_chart_type', 'Autom√°tico'))
                )
            with col2:
                color_pick = st.color_picker(
                    "Cor principal",
                    value=st.session_state.get('manual_color', user_color)
                )

            default_x = 0
            if st.session_state.get('manual_x') in possible_x_cols:
                default_x = possible_x_cols.index(st.session_state.manual_x)
            x_sel = st.selectbox("Eixo X", possible_x_cols, index=default_x if possible_x_cols else 0)

            y_default = st.session_state.get('manual_y', numeric_y_cols[:2] if numeric_y_cols else [])
            y_sel = st.multiselect("Colunas Y", numeric_y_cols, default=y_default)

            agg_options = ["sum", "avg", "count", "none"]
            agg_sel = st.selectbox(
                "Agrega√ß√£o",
                agg_options,
                index=agg_options.index(st.session_state.get('manual_agg', 'sum'))
            )

            submit = st.form_submit_button("Gerar gr√°fico", use_container_width=True)

        # Renderizar gr√°fico armazenado
        if st.session_state.get('manual_chart_generated') and st.session_state.get('manual_chart_option'):
            st_echarts(st.session_state.manual_chart_option, height="520px", key="echarts_manual_persisted")

        if submit:
            st.session_state.manual_chart_type = chart_type
            st.session_state.manual_color = color_pick
            st.session_state.manual_x = x_sel
            st.session_state.manual_y = y_sel
            st.session_state.manual_agg = agg_sel

            chosen_kind = chart_type
            if chart_type == "Autom√°tico":
                if date_cols:
                    chosen_kind = "Linhas"
                elif len(y_sel) >= 2:
                    chosen_kind = "Combo"
                else:
                    chosen_kind = "Barras"

            option_manual = _build_basic_echarts_option(
                df,
                chosen_kind,
                x_sel,
                y_sel,
                agg=agg_sel,
                color=color_pick,
            )

            if option_manual:
                st.session_state.manual_chart_option = option_manual
                st.session_state.manual_chart_generated = True
                st.success("‚úÖ Gr√°fico gerado com sucesso!")
                st_echarts(option_manual, height="520px", key="echarts_manual_new")
            else:
                st.warning("‚ö†Ô∏è N√£o foi poss√≠vel montar o gr√°fico. Verifique se X e Y est√£o selecionados.")

def calculate_kpis(df: pd.DataFrame):
    """Calculate intelligent KPIs based on data."""
    if df.empty:
        return

    st.subheader("üéØ Key Performance Indicators")

    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

    if not numeric_cols:
        return

    kpi_cols = st.columns(len(numeric_cols))

    for i, col in enumerate(numeric_cols):
        with kpi_cols[i]:
            total = df[col].sum()
            avg = df[col].mean()
            count = len(df)

            st.markdown(f"""
            <div class="metric-card">
                <h4>{col.replace('_', ' ').title()}</h4>
                <h2>{total:,.2f}</h2>
                <p>Avg: {avg:,.2f} | Count: {count:,}</p>
            </div>
            """, unsafe_allow_html=True)

def generate_natural_explanation(df: pd.DataFrame, query: str):
    """Generate natural language explanation of results."""
    if df.empty:
        return

    st.subheader("üí¨ Natural Language Explanation")

    try:
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

        explanation = f"Based on your query '{query}', I found {len(df)} records. "

        if numeric_cols:
            for col in numeric_cols[:2]:  # Focus on first 2 numeric columns
                total = df[col].sum()
                avg = df[col].mean()
                explanation += f"The total {col.replace('_', ' ')} is {total:,.2f} with an average of {avg:,.2f}. "

        if len(df) > 1:
            explanation += f"This represents data from {len(df)} different entries in your database."

        st.markdown(f'<div class="info-message">{explanation}</div>', unsafe_allow_html=True)

    except Exception as e:
        st.warning(f"Could not generate explanation: {str(e)}")

def auto_evaluate_analysis(analysis_text: str, df: pd.DataFrame, analysis_type: str) -> Dict[str, Any]:
    """
    Auto-avalia a qualidade de uma an√°lise gerada.
    
    Retorna:
    - score: int (0-100) - qualidade da an√°lise
    - issues: List[str] - problemas identificados
    - suggestions: List[str] - sugest√µes de melhoria
    - confidence: str - LOW/MEDIUM/HIGH
    """
    bedrock = get_bedrock_client()
    if not bedrock:
        return {
            "score": 50,
            "issues": ["N√£o foi poss√≠vel avaliar"],
            "suggestions": [],
            "confidence": "MEDIUM"
        }
    
    data_summary = f"Linhas: {len(df)}, Colunas: {len(df.columns)}, Tipos: {df.dtypes.to_dict()}"
    
    prompt = f"""
Voc√™ √© um AVALIADOR DE AN√ÅLISES DE DADOS. Sua fun√ß√£o √© avaliar criticamente an√°lises geradas por IA.

TIPO DE AN√ÅLISE: {analysis_type}

AN√ÅLISE GERADA:
{analysis_text}

DADOS DE CONTEXTO:
{data_summary}

Avalie a an√°lise considerando:
1. Precis√£o: Os n√∫meros fazem sentido?
2. Completude: Falta alguma informa√ß√£o importante?
3. Clareza: √â f√°cil de entender?
4. Insights: Fornece insights √∫teis?
5. Problemas: H√° informa√ß√µes incorretas ou enganosas?

Responda em JSON com este formato EXATO:
{{
  "score": <n√∫mero de 0 a 100>,
  "confidence": "<LOW|MEDIUM|HIGH>",
  "issues": ["<problema 1>", "<problema 2>"],
  "suggestions": ["<sugest√£o 1>", "<sugest√£o 2>"],
  "summary": "<resumo da avalia√ß√£o em uma frase>"
}}

Seja cr√≠tico mas justo. Se n√£o h√° problemas, deixe issues vazio.
"""
    
    try:
        response = bedrock.invoke_model(
            modelId="us.anthropic.claude-sonnet-4-20250514-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 2048,
                "temperature": 0.3,
                "messages": [{"role": "user", "content": prompt}]
            })
        )
        
        response_body = json.loads(response['body'].read())
        eval_text = response_body['content'][0]['text'].strip()
        
        # Extrair JSON
        json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', eval_text, re.DOTALL)
        if json_match:
            return json.loads(json_match.group())
        else:
            return json.loads(eval_text)
    except Exception as e:
        print(f"[AUTO_EVAL] Erro: {str(e)}")
        return {
            "score": 70,
            "confidence": "MEDIUM",
            "issues": [],
            "suggestions": ["Avalia√ß√£o autom√°tica n√£o dispon√≠vel"],
            "summary": "An√°lise gerada com sucesso"
        }

def process_user_feedback_and_regenerate(feedback_text: str, original_query: str, df: pd.DataFrame, analysis_type: str) -> Dict[str, Any]:
    """
    Processa feedback do usu√°rio e gera prompt corrigido.
    
    Retorna:
    - corrected_prompt: str - prompt corrigido
    - issues_identified: List[str] - problemas identificados
    - should_rerun: bool - se deve re-executar
    - suggested_sql: str - SQL sugerida (se aplic√°vel)
    """
    bedrock = get_bedrock_client()
    if not bedrock:
        return {
            "corrected_prompt": original_query,
            "issues_identified": ["N√£o foi poss√≠vel processar feedback"],
            "should_rerun": False,
            "suggested_sql": ""
        }
    
    prompt = f"""
Voc√™ √© um CORRETOR DE AN√ÅLISES. O usu√°rio identificou problemas na an√°lise.

QUERY ORIGINAL: {original_query}
TIPO DE AN√ÅLISE: {analysis_type}

FEEDBACK DO USU√ÅRIO (PROBLEMA IDENTIFICADO):
{feedback_text}

Sua tarefa:
1. Analisar o problema reportado
2. Identificar a causa raiz
3. Gerar um prompt CORRIGIDO que resolva o problema
4. Determinar se precisa re-executar a query SQL

Responda em JSON:
{{
  "corrected_prompt": "<prompt corrigido para gerar an√°lise melhor>",
  "issues_identified": ["<causa 1>", "<causa 2>"],
  "should_rerun": <true|false>,
  "suggested_sql": "<SQL corrigida se should_rerun=true, vazio caso contr√°rio>",
  "explanation": "<explica√ß√£o do que foi corrigido>"
}}
"""
    
    try:
        response = bedrock.invoke_model(
            modelId="us.anthropic.claude-sonnet-4-20250514-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 4096,
                "temperature": 0.5,
                "messages": [{"role": "user", "content": prompt}]
            })
        )
        
        response_body = json.loads(response['body'].read())
        correction_text = response_body['content'][0]['text'].strip()
        
        # Extrair JSON
        json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', correction_text, re.DOTALL)
        if json_match:
            return json.loads(json_match.group())
        else:
            return json.loads(correction_text)
    except Exception as e:
        print(f"[FEEDBACK] Erro: {str(e)}")
        return {
            "corrected_prompt": original_query,
            "issues_identified": [f"Erro ao processar: {str(e)}"],
            "should_rerun": False,
            "suggested_sql": "",
            "explanation": "N√£o foi poss√≠vel processar o feedback"
        }

def generate_direct_response(df: pd.DataFrame, user_query: str) -> str:
    """Gera resposta direta com n√∫meros-chave da an√°lise.
    
    Args:
        df: DataFrame com os dados
        user_query: Pergunta original do usu√°rio
        
    Returns:
        Resposta em texto formatado
    """
    if df is None or df.empty:
        return "‚ùå Nenhum dado dispon√≠vel para an√°lise."

    # Validar entrada
    if not isinstance(df, pd.DataFrame):
        return "‚ùå Dados inv√°lidos para an√°lise."
    
    # Configurar limite m√°ximo de linhas para an√°lise
    max_rows = MAX_ROWS_FOR_ANALYSIS
    
    # Determinar quantas linhas ser√£o inclu√≠das na an√°lise
    total_rows = len(df)
    rows_to_analyze = min(total_rows, max_rows)
    df_sample = df.head(rows_to_analyze)
    
    # Criar resumo dos dados otimizado
    try:
        # Limitar tamanho do resumo para evitar payloads muito grandes
        data_summary = df_sample.head(50).to_string(max_rows=50, max_cols=20)
        data_summary += "\n\n" + df.describe(include='all').to_string()
    except Exception as e:
        print(f"[BRIAN_RESPONSE] ‚ö†Ô∏è Erro ao gerar resumo: {str(e)}")
        data_summary = f"DataFrame com {len(df)} linhas e {len(df.columns)} colunas"
    
    # Data atual para contexto temporal
    current_date = datetime.now().strftime('%Y-%m-%d')
    current_year = datetime.now().year

    prompt = f"""
DATA ATUAL DE REFER√äNCIA: {current_date}
ANO ATUAL: {current_year}

‚ö†Ô∏è ATEN√á√ÉO CR√çTICA: Voc√™ est√° analisando dados em {current_date}. NUNCA mencione datas futuras ou anos que ainda n√£o aconteceram. Se os dados mostram datas futuras, isso √© um ERRO nos dados.

Baseado nos seguintes dados:

{data_summary}

Responda √† pergunta: {user_query}

Forne√ßa 1-3 senten√ßas com os n√∫meros-chave dos dados.

IMPORTANTE: 
- Comece sua resposta mencionando quantas linhas foram analisadas do total.
- Formato: "‚úÖ An√°lise de {rows_to_analyze:,} de {total_rows:,} linhas totais."
- N√ÉO gere c√≥digo Python, matplotlib, pandas ou qualquer outro c√≥digo.
- Responda apenas com TEXTO EXPLICATIVO em linguagem natural.
- Foque em insights e n√∫meros, n√£o em c√≥digo.

Responda em {st.session_state.language}.
"""

    try:
        bedrock = get_bedrock_client()
        if not bedrock:
            return "‚ùå Erro: Cliente Bedrock n√£o dispon√≠vel para an√°lise."
        
        # Validar tamanho do prompt
        prompt_size = len(prompt)
        if prompt_size > 50000:
            print(f"[BRIAN_RESPONSE] ‚ö†Ô∏è Prompt muito grande: {prompt_size} caracteres")
            # Reduzir data_summary
            data_summary = df_sample.head(20).to_string(max_rows=20)
            prompt = f"""
DATA ATUAL: {current_date}

Dados (amostra reduzida):
{data_summary}

Pergunta: {user_query}
Forne√ßa resposta concisa com n√∫meros-chave.
An√°lise de {rows_to_analyze:,} de {total_rows:,} linhas.
"""
        
        response = bedrock.invoke_model(
            modelId="us.anthropic.claude-sonnet-4-20250514-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 8192,  # Reduzir tokens para respostas mais concisas
                "messages": [{"role": "user", "content": prompt}]
            })
        )
        response_body = json.loads(response['body'].read())
        ai_response = response_body['content'][0]['text'].strip()
        
        # Adicionar informa√ß√£o de linhas analisadas no in√≠cio se n√£o estiver presente
        analysis_info = f"‚úÖ **An√°lise de {rows_to_analyze:,} de {total_rows:,} linhas totais** (limite configurado: {MAX_ROWS_FOR_ANALYSIS:,} linhas)\n\n"
        if not ai_response.startswith("‚úÖ"):
            ai_response = analysis_info + ai_response
            
        return ai_response
    except Exception as e:
        return f"Could not generate direct response: {str(e)}"

def generate_additional_analyses(df: pd.DataFrame, user_query: str) -> str:
    """Generate additional analyses with bullets."""
    if df.empty:
        return "No data for analysis."

    data_summary = df.head(10).to_string() + "\n\n" + df.describe(include='all').to_string()
    
    # Data atual para contexto temporal
    current_date = datetime.now().strftime('%Y-%m-%d')
    current_year = datetime.now().year

    prompt = f"""
DATA ATUAL DE REFER√äNCIA: {current_date}
ANO ATUAL: {current_year}

‚ö†Ô∏è ATEN√á√ÉO CR√çTICA: Estamos em {current_date}. N√ÉO mencione datas futuras ou anos que ainda n√£o aconteceram.

Based on the data:

{data_summary}

Provide 3-6 bullet points explaining patterns, variations, and anomalies in the data, always with numbers.

IMPORTANT: 
- DO NOT generate Python code, matplotlib, pandas or any other code.
- Respond ONLY with EXPLANATORY TEXT in natural language.
- Focus on insights and numbers, not code.

Respond in {st.session_state.language}.
"""

    try:
        bedrock = get_bedrock_client()
        response = bedrock.invoke_model(
            modelId="us.anthropic.claude-sonnet-4-20250514-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 65536,
                "messages": [{"role": "user", "content": prompt}]
            })
        )
        response_body = json.loads(response['body'].read())
        return response_body['content'][0]['text'].strip()
    except Exception as e:
        return f"Could not generate analyses: {str(e)}"

def generate_causes_and_new_analyses(df: pd.DataFrame, user_query: str) -> str:
    """Generate probable causes and new analyses."""
    if df.empty:
        return "No data for hypotheses."

    data_summary = df.head(10).to_string() + "\n\n" + df.describe(include='all').to_string()
    
    # Data atual para contexto temporal
    current_date = datetime.now().strftime('%Y-%m-%d')
    current_year = datetime.now().year

    prompt = f"""
DATA ATUAL DE REFER√äNCIA: {current_date}
ANO ATUAL: {current_year}

‚ö†Ô∏è ATEN√á√ÉO: Estamos em {current_date}. Analise tend√™ncias e causas baseado em dados PASSADOS e PRESENTES. N√£o mencione dados futuros.

Based on the data:

{data_summary}

Provide 2-4 hypotheses based on the data (mark as H1, H2, etc.).

Then, suggest 1-3 new analyses with clear instructions (what to group/filter/test).

IMPORTANT:
- DO NOT generate Python code, matplotlib, pandas or any other code.
- Respond ONLY with EXPLANATORY TEXT in natural language.
- Focus on business insights and actionable recommendations.

Respond in {st.session_state.language}.
"""

    try:
        bedrock = get_bedrock_client()
        response = bedrock.invoke_model(
            modelId="us.anthropic.claude-sonnet-4-20250514-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 65536,
                "messages": [{"role": "user", "content": prompt}]
            })
        )
        response_body = json.loads(response['body'].read())
        return response_body['content'][0]['text'].strip()
    except Exception as e:
        return f"Could not generate causes: {str(e)}"

def generate_note(df: pd.DataFrame, user_query: str, sql_query: str) -> str:
    """Generate note with rating and improvements."""
    data_summary = df.describe(include='all').to_string() if not df.empty else "No data"

    prompt = f"""
Question: {user_query}

Data summary: {data_summary}

SQL query used: {sql_query}

Rate the question and result from 1-10.

Suggest exactly how to improve precision (adjustments to prompt/data, e.g., define safra, add filters).

Respond in {st.session_state.language}.
"""

    try:
        bedrock = get_bedrock_client()
        response = bedrock.invoke_model(
            modelId="us.anthropic.claude-sonnet-4-20250514-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 65536,
                "messages": [{"role": "user", "content": prompt}]
            })
        )
        response_body = json.loads(response['body'].read())
        return response_body['content'][0]['text'].strip()
    except Exception as e:
        return f"Could not generate note: {str(e)}"

def process_modular_step(user_response: str) -> str:
    """Process a step in the modular prompt workflow."""
    current_step = st.session_state.modular_config['current_step']
    role = st.session_state.modular_config['role']
    context = st.session_state.modular_config['contexts'][current_step]
    previous_responses = st.session_state.modular_config['responses']

    prompt = f"""
{role}

Current Step Context: {context}

Previous Responses:
{chr(10).join(f"Step {i+1}: {resp}" for i, resp in enumerate(previous_responses))}

User Response for Step {current_step + 1}: {user_response}

Based on the role, current step context, and previous responses, provide guidance and instructions for the next step or complete the current step analysis.

Respond in {st.session_state.language}.
"""

    try:
        bedrock = get_bedrock_client()
        response = bedrock.invoke_model(
            modelId="us.anthropic.claude-sonnet-4-20250514-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 65536,
                "messages": [{"role": "user", "content": prompt}]
            })
        )
        response_body = json.loads(response['body'].read())
        ai_response = response_body['content'][0]['text'].strip()

        # Store user response
        st.session_state.modular_config['responses'].append(user_response)

        # Move to next step
        st.session_state.modular_config['current_step'] += 1

        return ai_response

    except Exception as e:
        return f"Error processing modular step: {str(e)}"

def query_assistant_analyze(user_message: str, chat_history: List[Dict]) -> Dict[str, Any]:
    """
    Assistente de Consulta - Analisa a mensagem do usu√°rio e fornece feedback interativo
    para refinar a consulta antes de executar.
    
    Retorna:
    - confidence: int (0-100) - n√≠vel de confian√ßa na consulta
    - message: str - resposta do assistente
    - suggestions: List[str] - sugest√µes de refinamento
    - is_ready: bool - se est√° pronto para executar
    - refined_query: str - query refinada
    """
    bedrock = get_bedrock_client()
    if not bedrock:
        return {
            "confidence": 0,
            "message": "Erro ao conectar com o assistente.",
            "suggestions": [],
            "is_ready": False,
            "refined_query": user_message
        }
    
    # Preparar hist√≥rico de conversa
    conversation_context = "\n".join([
        f"{msg['role'].upper()}: {msg['content']}" 
        for msg in chat_history[-5:]  # √∫ltimas 5 mensagens
    ])
    
    # Obter contexto da tabela - USAR APENAS CACHE, SEM QUERIES ADICIONAIS
    full_table = f"{st.session_state.connection_config.get('catalog', 'prd')}.{st.session_state.connection_config.get('schema', 'sand_snd_box_cartao')}.{st.session_state.ai_config.get('context_table', 'tb_pbi_autz_pampa_diar')}"
    
    # OTIMIZA√á√ÉO: Usar APENAS o cache de explora√ß√£o - n√£o executar queries adicionais
    exploration = st.session_state.table_exploration_cache.get(full_table, {})
    
    # Preparar metadados enriquecidos usando APENAS cache pr√©-carregado
    enriched_metadata = f"""
üìä METADADOS DA TABELA (Cache de Explora√ß√£o Autom√°tica):

üö© FLAGS DISPON√çVEIS ({len(exploration.get('flag_columns', {}))} mapeadas):
{chr(10).join([f"  - {col}: valores = {values}" for col, values in list(exploration.get('flag_columns', {}).items())[:10]])}

üìù DESCRI√á√ïES DISPON√çVEIS ({len(exploration.get('description_columns', {}))} mapeadas):
{chr(10).join([f"  - {col}: {len(values)} valores √∫nicos (amostra: {values[:3]})" for col, values in list(exploration.get('description_columns', {}).items())[:10]])}

üî¢ C√ìDIGOS DISPON√çVEIS ({len(exploration.get('code_columns', {}))} mapeadas):
{chr(10).join([f"  - {col}: {len(values)} valores √∫nicos" for col, values in list(exploration.get('code_columns', {}).items())[:5]])}

üìà COLUNAS NUM√âRICAS: {', '.join(exploration.get('numeric_columns', [])[:10])}
üìÖ COLUNAS DE DATA: {', '.join(exploration.get('date_columns', [])[:5])}

üí° IMPORTANTE: Esses s√£o os valores REAIS que existem na tabela (pr√©-carregados).
"""
    
    if not exploration:
        enriched_metadata = "‚ö†Ô∏è Explora√ß√£o da tabela n√£o dispon√≠vel. Execute explora√ß√£o autom√°tica primeiro."
    
    prompt = f"""
Voc√™ √© um ASSISTENTE DE CONSULTA especializado em ajudar usu√°rios a formular perguntas claras e espec√≠ficas para an√°lise de dados.

Seu objetivo √©:
1. Analisar a inten√ß√£o do usu√°rio
2. Identificar ambiguidades ou falta de informa√ß√µes
3. Fazer perguntas provocativas e direcionadas
4. Guiar o usu√°rio at√© uma consulta de ALTA CONFIAN√áA
5. Quando tiver confian√ßa >= 80%, declarar que est√° pronto para executar

CONTEXTO DA TABELA (do cache):
{get_context()}

{enriched_metadata}

HIST√ìRICO DA CONVERSA:
{conversation_context}

NOVA MENSAGEM DO USU√ÅRIO:
{user_message}

Analise a mensagem e responda em JSON com este formato EXATO:
{{
  "confidence": <n√∫mero de 0 a 100>,
  "message": "<sua resposta ao usu√°rio, fazendo perguntas ou confirmando>",
  "suggestions": ["<sugest√£o 1>", "<sugest√£o 2>", "<sugest√£o 3>"],
  "is_ready": <true se confidence >= 80, false caso contr√°rio>,
  "refined_query": "<query refinada e clara baseada na conversa>",
  "missing_info": ["<informa√ß√£o que ainda falta 1>", "<informa√ß√£o que ainda falta 2>"]
}}

DIRETRIZES:
- Se faltam especifica√ß√µes de PER√çODO/DATA, pergunte (ex: "√∫ltimos 30 dias? este m√™s?")
- Se faltam especifica√ß√µes de M√âTRICA, pergunte (ex: "quantidade? valor total? m√©dia?")
- Se faltam especifica√ß√µes de AGRUPAMENTO, sugira (ex: "por status? por produto?")
- Se faltam FILTROS espec√≠ficos, indague (ex: "somente ativos? incluir inativos?")
- Seja provocativo mas amig√°vel
- Use emojis para deixar a conversa mais leve
- Quando confian√ßa atingir 80%+, parabenize e confirme que est√° pronto

Responda APENAS com JSON v√°lido.
"""
    
    try:
        response = bedrock.invoke_model(
            modelId="us.anthropic.claude-sonnet-4-20250514-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 4096,
                "temperature": 0.7,
                "messages": [{"role": "user", "content": prompt}]
            })
        )
        
        response_body = json.loads(response['body'].read())
        assistant_text = response_body['content'][0]['text'].strip()
        
        # Extrair JSON da resposta
        json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', assistant_text, re.DOTALL)
        if json_match:
            result = json.loads(json_match.group())
            return result
        else:
            return json.loads(assistant_text)
            
    except Exception as e:
        print(f"[ASSISTANT] Erro: {str(e)}")
        return {
            "confidence": 30,
            "message": f"Entendi sua solicita√ß√£o. Poderia me dar mais detalhes sobre o per√≠odo e m√©tricas desejadas?",
            "suggestions": ["Especificar per√≠odo de an√°lise", "Definir m√©tricas principais", "Indicar filtros necess√°rios"],
            "is_ready": False,
            "refined_query": user_message,
            "missing_info": ["per√≠odo", "m√©trica"]
        }

def reanalyze_failed_query(user_query: str, sql_query: str, errors: List[str]) -> Dict[str, Any]:
    """Reanalisa a solicita√ß√£o original e os erros de execu√ß√£o para gerar uma SQL corrigida."""
    bedrock = get_bedrock_client()
    if not bedrock:
        return {
            "fixed_sql": "",
            "reason": "Bedrock n√£o configurado para rean√°lise.",
            "changes": [],
            "confidence": 0
        }

    context = get_context()
    errors_text = "\n".join([f"- {err}" for err in errors[-5:]]) if errors else "- Nenhum erro registrado"
    sql_preview = sql_query or "<SQL n√£o dispon√≠vel>"

    prompt = f"""
Voc√™ √© um ENGENHEIRO DE DADOS especialista em Databricks SQL.
Tarefa: Corrigir a SQL que falhou, usando a solicita√ß√£o original do usu√°rio e a lista de erros de execu√ß√£o.

CONTEXTO:
{context}

SOLICITA√á√ÉO ORIGINAL DO USU√ÅRIO:
{user_query}

SQL QUE FALHOU:
{sql_preview}

ERROS OBSERVADOS:
{errors_text}

Responda APENAS em JSON com o formato:
{{
  "fixed_sql": "<SQL corrigida pronta para executar em Databricks>",
  "reason": "<explica√ß√£o curta do que foi corrigido>",
  "changes": ["<mudan√ßa 1>", "<mudan√ßa 2>"],
  "confidence": <n√∫mero de 0 a 100>
}}

Regras:
- Use somente colunas e tabelas do contexto.
- Corrija problemas de sintaxe, GROUP BY, alias e filtros.
- Se o problema for filtro de produto com espa√ßos, use TRIM quando necess√°rio.
- N√£o devolva texto fora do JSON.
"""

    try:
        response = bedrock.invoke_model(
            modelId="us.anthropic.claude-sonnet-4-20250514-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 4096,
                "temperature": 0.3,
                "messages": [{"role": "user", "content": prompt}]
            })
        )

        response_body = json.loads(response['body'].read())
        assistant_text = response_body['content'][0]['text'].strip()

        json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', assistant_text, re.DOTALL)
        if json_match:
            return json.loads(json_match.group())
        return json.loads(assistant_text)
    except Exception as e:
        print(f"[REANALYZE] Erro: {str(e)}")
        return {
            "fixed_sql": "",
            "reason": "N√£o foi poss√≠vel gerar SQL corrigida.",
            "changes": ["Verifique sintaxe, GROUP BY e filtros."],
            "confidence": 0
        }

def generate_refined_sql(original_query: str, df: pd.DataFrame, refinement_instructions: str) -> str:
    """Legacy wrapper - calls new refine function."""
    # Get exploration cache
    full_table = f"{st.session_state.connection_config.get('catalog', 'prd')}.{st.session_state.connection_config.get('schema', 'sand_snd_box_cartao')}.{st.session_state.ai_config.get('context_table', 'tb_pbi_autz_pampa_diar')}"
    exploration = st.session_state.table_exploration_cache.get(full_table, {})
    
    bedrock = get_bedrock_client()
    result = refine_query_with_additional_context(
        original_query,
        st.session_state.last_sql,
        df,
        refinement_instructions,
        bedrock,
        get_context,
        exploration
    )
    return result.get('refined_sql', '')

def main():
    lang = st.session_state.language
    # Header
    st.markdown('<h1 class="main-header">' + translations[lang]['title'] + '</h1>', unsafe_allow_html=True)
    st.markdown(translations[lang]['subtitle'])

    # Sidebar Configuration
    with st.sidebar:
        st.header(translations[lang]['sidebar_config'])
        
        # Instru√ß√µes de Uso - Primeiros Passos
        with st.expander("üìñ Instru√ß√µes de Uso - Como Come√ßar", expanded=False):
            st.info("**üöÄ Siga estes passos para come√ßar a usar o BRIAN:**")
            
            st.markdown("""
            ### 1Ô∏è‚É£ Configurar Token AWS Bedrock (OBRIGAT√ìRIO)
            
            **üìç Acesse o menu lateral** ‚Üí **‚öôÔ∏è Configura√ß√£o AWS Bedrock**
            
            - **üîë Obtenha seu token**: Entre em contato com o desenvolvedor (**Nelson Stropa**) para receber seu token de acesso
            - **üìã Cole o token** no campo "Bearer Token"
            - **üíæ Clique em "Salvar Token"** para armazenar a configura√ß√£o
            - **‚úÖ Clique em "Testar Conex√£o"** para validar o acesso
            - **Aguarde a confirma√ß√£o** de "‚úÖ Conex√£o estabelecida com sucesso!"
            
            ‚ö†Ô∏è **Sem o token configurado, o sistema N√ÉO funcionar√°!**
            
            ---
            
            ### 2Ô∏è‚É£ Configurar Conex√µes (Opcional)
            
            **üîó Conex√£o Databricks**  
            As conex√µes Databricks j√° est√£o **pr√©-configuradas para testes**.  
            ‚û°Ô∏è Caso deseje usar sua conex√£o pessoal, acesse: **Menu Lateral** ‚Üí **üîå Conex√£o Databricks**
            
            **üìö Base de Conhecimento**  
            A base de conhecimento padr√£o est√° configurada como **"Autoriza√ß√µes PAMPA"** para testes.  
            ‚û°Ô∏è Para trocar por outra base dispon√≠vel, acesse: **Menu Lateral** ‚Üí **üìö Base de Conhecimento**
            
            ---
            
            ### 3Ô∏è‚É£ Come√ßar a Usar! üéØ
            
            Com o token configurado e conex√£o estabelecida, voc√™ j√° pode usar o BRIAN:
            
            ‚ú® **Digite sua pergunta** no campo "üí¨ Fa√ßa sua pergunta..."  
            ‚ú® **Obtenha respostas** com consultas SQL geradas automaticamente  
            ‚ú® **Refine consultas** para ajustar os resultados  
            ‚ú® **Explore sugest√µes** de novas an√°lises relacionadas  
            ‚ú® **Visualize dados** com gr√°ficos e tabelas interativas  
            
            üí° **Dica**: Quanto mais espec√≠fica for sua pergunta, melhores ser√£o os resultados!
            
            ---
            
            **‚ùì Precisa de ajuda?** Entre em contato com **Nelson Stropa** - Desenvolvedor Principal
            """)
        
        # Avisos e Instru√ß√µes Importantes
        with st.expander("‚ö†Ô∏è IMPORTANTE - Vers√£o Piloto para Testes", expanded=False):
            st.warning("**üß™ Esta √© uma vers√£o PILOTO do BRIAN para testes e valida√ß√£o.**")
            
            st.markdown("""
            ### üìã Limita√ß√µes e Considera√ß√µes Importantes:
            
            1. **üîë Token Tempor√°rio**  
               O token de acesso ao AWS Bedrock √© tempor√°rio e possui uma limita√ß√£o de poucas horas de uso.  
               ‚ö†Ô∏è Quando o token expirar, √© necess√°rio acionar **Nelson Stropa** para gerar um novo token.
            
            2. **üìä Contexto da Base de Dados**  
               O contexto das bases pode ser alterado no menu lateral (‚öôÔ∏è Configura√ß√£o).  
               Esta vers√£o traz a **base de Autoriza√ß√µes PAMPA** como exemplo para testes.
            
            3. **üí¨ Perguntas em Linguagem Natural**  
               As perguntas podem ser realizadas em linguagem natural, mas est√£o **limitadas ao contexto de dados** fornecido pela tabela.  
               ‚ÑπÔ∏è O usu√°rio precisa ter um **conhecimento m√≠nimo pr√©vio** sobre o assunto da tabela para melhores resultados.
            
            4. **‚ö° Instabilidades**  
               A solu√ß√£o pode apresentar instabilidades ocasionais.  
               üêõ Caso ocorram problemas, por favor reporte ao desenvolvedor.
            
            5. **üí° Feedbacks e Sugest√µes**  
               Feedbacks e sugest√µes s√£o **bem-vindos** para melhoria cont√≠nua da ferramenta!
            
            ---
            
            ### üöÄ Desenvolvimento Cont√≠nuo
            
            O time de desenvolvimento est√° trabalhando ativamente para melhorar a solu√ß√£o, com foco em:
            - ‚úÖ Aprimorar a capacidade anal√≠tica da ferramenta
            - ‚úÖ Resolver problemas cr√≠ticos de acesso e autentica√ß√£o
            - ‚úÖ Facilitar o compartilhamento e acesso para uma base maior de usu√°rios
            - ‚úÖ Aumentar a estabilidade e performance geral
            
            **Contato:** Nelson Stropa - Desenvolvedor Principal
            """)

        # Connection Settings
        with st.expander(translations[lang]['connection'], expanded=False):
            st.session_state.connection_config['server_hostname'] = st.text_input(
                "Server Hostname",
                value=st.session_state.connection_config['server_hostname'],
                placeholder="Ex: adb-xxx.azuredatabricks.net",
                help="Your Databricks workspace URL (e.g., adb-xxx.azuredatabricks.net)"
            )
            st.session_state.connection_config['http_path'] = st.text_input(
                "HTTP Path",
                value=st.session_state.connection_config['http_path'],
                placeholder="Ex: /sql/1.0/warehouses/xxx",
                help="SQL warehouse path (e.g., /sql/1.0/warehouses/xxx)"
            )
            st.session_state.connection_config['access_token'] = st.text_input(
                "Access Token",
                value=st.session_state.connection_config['access_token'],
                type="password",
                placeholder="Databricks Personal Access Token",
                help="Databricks personal access token"
            )

        # AWS Bedrock Configuration
        with st.expander("üîë Configura√ß√£o AWS Bedrock", expanded=False):
            st.markdown("### üîê Token de Acesso AWS Bedrock")
            st.caption("Configure o token Bearer para autentica√ß√£o na API do AWS Bedrock")
            
            # Obter o token atual do session_state
            current_token = st.session_state.get('aws_bedrock_token', '')
            
            new_token = st.text_input(
                "Bearer Token",
                value=current_token,
                type="password",
                placeholder="bedrock-api-key-...",
                help="Token Bearer completo para autentica√ß√£o no AWS Bedrock"
            )
            
            # Bot√£o para salvar/atualizar token
            col1, col2 = st.columns([1, 1])
            with col1:
                if st.button("üíæ Salvar Token", type="primary", use_container_width=True):
                    if new_token and new_token.strip():
                        # Atualizar vari√°vel de ambiente
                        os.environ['AWS_BEARER_TOKEN_BEDROCK'] = new_token.strip()
                        # Salvar no session_state
                        st.session_state.aws_bedrock_token = new_token.strip()
                        st.success("‚úÖ Token AWS Bedrock atualizado com sucesso!")
                    else:
                        st.error("‚ùå Por favor, insira um token v√°lido")
            
            with col2:
                if st.button("üîÑ Testar Conex√£o", use_container_width=True):
                    try:
                        token_to_test = st.session_state.get('aws_bedrock_token', '').strip()
                        if not token_to_test:
                            st.error("‚ùå Por favor, configure um token primeiro")
                        else:
                            test_client = BedrockClient(bearer_token=token_to_test)
                            st.success("‚úÖ Conex√£o com AWS Bedrock estabelecida!")
                            st.info(f"üåç Regi√£o: {test_client.region}")
                            st.info(f"ü§ñ Modelo: {test_client.model_id}")
                    except Exception as e:
                        st.error(f"‚ùå Erro ao conectar: {str(e)}")
            
            # Status atual
            st.markdown("---")
            st.markdown("**Status da Configura√ß√£o:**")
            if st.session_state.get('aws_bedrock_token', '').strip():
                token_preview = st.session_state.aws_bedrock_token[:50] + "..."
                st.success(f"‚úÖ Token configurado: `{token_preview}`")
            else:
                st.warning("‚ö†Ô∏è Nenhum token configurado")

        # AI Model Settings
        with st.expander(translations[lang]['ai_config'], expanded=False):
            st.session_state.ai_config['model'] = st.selectbox(
                "AI Model",
                ["databricks", "bedrock_claude"],
                index=["databricks", "bedrock_claude"].index(st.session_state.ai_config['model']),
                help="Choose AI model for SQL generation (Bedrock Claude is recommended)"
            )

            # Agent Type Selection
            st.markdown("---")
            st.markdown(f"ü§ñ **{translations[lang]['agent_type']}**")
            agent_options = {
                'chat': translations[lang]['chat_agent'],
                'modular_prompt': translations[lang]['modular_prompt_agent']
            }
            st.session_state.agent_type = st.selectbox(
                translations[lang]['agent_type'],
                list(agent_options.keys()),
                index=list(agent_options.keys()).index(st.session_state.agent_type),
                format_func=lambda x: agent_options[x],
                help="Choose agent interaction mode"
            )

            # Modular Prompt Configuration
            if st.session_state.agent_type == 'modular_prompt':
                st.markdown("---")
                st.markdown(f"üìã **{translations[lang]['modular_prompt_agent']}**")

                st.session_state.modular_config['role'] = st.text_area(
                    translations[lang]['agent_role'],
                    value=st.session_state.modular_config['role'],
                    height=100,
                    help="Define the agent's role and behavior"
                )

                st.markdown("**Context Steps (up to 10):**")
                for i in range(10):
                    st.session_state.modular_config['contexts'][i] = st.text_area(
                        f"{translations[lang]['context_step']} {i+1}",
                        value=st.session_state.modular_config['contexts'][i],
                        height=80,
                        key=f"context_{i}",
                        help=f"Instructions for step {i+1} of the modular process"
                    )

        # Feature Flags
        with st.expander(translations[lang]['advanced'], expanded=False):
            st.session_state.feature_flags['intelligent_analysis'] = st.checkbox(
                "üß† Intelligent Analysis",
                value=st.session_state.feature_flags['intelligent_analysis'],
                help="Automatically calculate KPIs and provide insights"
            )
            st.session_state.feature_flags['auto_comparison'] = st.checkbox(
                "üîÑ Auto Comparison",
                value=st.session_state.feature_flags['auto_comparison'],
                help="Detect temporal queries and generate comparisons"
            )
            st.session_state.feature_flags['smart_visualization'] = st.checkbox(
                "üìä Smart Visualization",
                value=st.session_state.feature_flags['smart_visualization'],
                help="Automatically select best chart types"
            )
            st.session_state.feature_flags['ai_chart_builder'] = st.checkbox(
                "ü§ñ IA para gr√°ficos",
                value=st.session_state.feature_flags['ai_chart_builder'],
                help="Usa a IA para interpretar pedidos de gr√°ficos (tipo, cor, agrega√ß√£o) e montar o Plotly"
            )
            st.session_state.feature_flags['error_correction'] = st.checkbox(
                "üîß Error Correction",
                value=st.session_state.feature_flags['error_correction'],
                help="Automatically retry with corrected SQL"
            )

            st.markdown("---")
            st.markdown("üöÄ **Melhorias BRIAN**")
            st.session_state.feature_flags['context_enrichment'] = st.checkbox(
                "üîß Enriquecimento de Contexto",
                value=st.session_state.feature_flags['context_enrichment'],
                help="Consultar colunas e valores reais da tabela antes de gerar SQL"
            )
            st.session_state.feature_flags['pre_validation'] = st.checkbox(
                "üîç Pr√©-valida√ß√£o de Queries",
                value=st.session_state.feature_flags['pre_validation'],
                help="Executa COUNT(*) antes da query completa para evitar resultados vazios"
            )
            st.session_state.feature_flags['smart_suggestions'] = st.checkbox(
                "üí° Sugest√µes Inteligentes",
                value=st.session_state.feature_flags['smart_suggestions'],
                help="Mostra valores e datas dispon√≠veis quando n√£o h√° resultados"
            )
            st.session_state.feature_flags['query_reasoning'] = st.checkbox(
                "üß† Auto-avalia√ß√£o (Reasoning)",
                value=st.session_state.feature_flags['query_reasoning'],
                help="Avalia a SQL antes de executar e sugere corre√ß√µes"
            )
            st.session_state.feature_flags['sampling_alerts'] = st.checkbox(
                "‚ö†Ô∏è Alertas de Amostragem",
                value=st.session_state.feature_flags['sampling_alerts'],
                help="Detecta LIMIT e alerta sobre amostragem"
            )
            st.session_state.feature_flags['query_judge'] = st.checkbox(
                "üß≠ Judge de Consulta",
                value=st.session_state.feature_flags['query_judge'],
                help="Antes de rodar, valida se a pergunta est√° espec√≠fica (m√©trica e campo de data) e sugere uma formula√ß√£o clara"
            )

            # Vector Database Management
            st.markdown("---")
            st.markdown("üóÇÔ∏è **Vector Database Management**")
            if st.button("üóëÔ∏è Clear All Documents", type="secondary"):
                try:
                    deleted_count = clear_all_documents()
                    st.success(f"‚úÖ Successfully deleted {deleted_count} documents from vector database!")
                except Exception as e:
                    st.error(f"‚ùå Error clearing documents: {str(e)}")

        # Context Configuration
        with st.expander(translations[st.session_state.language]['context'], expanded=False):
            st.markdown("#### üìã Contexto Descritivo")
            st.session_state.user_context = st.text_area(
                "Database Context",
                value=st.session_state.user_context,
                height=300,
                help="Edit the database schema and rules context used for SQL generation. This is pre-filled with the default context. Clear it to reset to default."
            )
            
            st.markdown("---")
            st.markdown("#### üéØ Tabela de Contexto (Enriquecimento Adaptativo)")
            st.markdown("‚ö†Ô∏è **IMPORTANTE:** Esta √© a tabela que o BRIAN vai consultar para obter colunas e valores reais.")
            
            # Campos para configura√ß√£o da tabela
            col1, col2, col3 = st.columns(3)
            with col1:
                catalog_input = st.text_input(
                    "Catalog",
                    value=st.session_state.connection_config.get('catalog', ''),
                    placeholder="prd",
                    help="Nome do cat√°logo Databricks"
                )
            with col2:
                schema_input = st.text_input(
                    "Schema",
                    value=st.session_state.connection_config.get('schema', ''),
                    placeholder="sand_snd_box_cartao",
                    help="Nome do schema"
                )
            with col3:
                table_input = st.text_input(
                    "Tabela",
                    value=st.session_state.ai_config.get('context_table', ''),
                    placeholder="tb_pbi_autz_pampa_diar",
                    help="Nome da tabela (sem catalog.schema)"
                )
            
            # Atualizar configura√ß√µes quando alteradas
            if catalog_input != st.session_state.connection_config.get('catalog'):
                st.session_state.connection_config['catalog'] = catalog_input
            if schema_input != st.session_state.connection_config.get('schema'):
                st.session_state.connection_config['schema'] = schema_input
            if table_input != st.session_state.ai_config.get('context_table'):
                st.session_state.ai_config['context_table'] = table_input
            
            # Mostrar tabela completa e validar
            if catalog_input and schema_input and table_input:
                full_table = f"{catalog_input}.{schema_input}.{table_input}"
                st.success(f"‚úÖ **Tabela configurada:** `{full_table}`")
                st.caption("Esta √© a tabela que ser√° usada para enriquecer o contexto com colunas e valores reais.")

                # Configura√ß√µes adicionais de explora√ß√£o
                st.markdown("---")
                st.markdown("#### ‚öôÔ∏è Configura√ß√µes da Explora√ß√£o")
                col_cfg1, col_cfg2, col_cfg3, col_cfg4 = st.columns(4)
                with col_cfg1:
                    st.session_state.exploration_limit_per_column = st.number_input(
                        "Limite por coluna (distinct)",
                        min_value=10,
                        max_value=100000,
                        value=st.session_state.exploration_limit_per_column,
                        step=100,
                        help="N√∫mero m√°ximo de valores distintos coletados por coluna nas colunas de flag e descri√ß√£o"
                    )
                with col_cfg2:
                    st.session_state.partition_column_name = st.text_input(
                        "Coluna de parti√ß√£o (√∫ltima)",
                        value=st.session_state.partition_column_name,
                        help="Nome da coluna de parti√ß√£o usada para filtrar a √∫ltima carga (ex.: data_refe_crga)"
                    )
                with col_cfg3:
                    st.session_state.exploration_sample_limit = st.number_input(
                        "Limite da amostra",
                        min_value=10,
                        max_value=100000,
                        value=st.session_state.exploration_sample_limit,
                        step=100,
                        help="N√∫mero m√°ximo de linhas na amostra explorat√≥ria"
                    )
                with col_cfg4:
                    st.session_state.query_timeout_seconds = st.number_input(
                        "Timeout (segundos)",
                        min_value=10,
                        max_value=600,
                        value=st.session_state.query_timeout_seconds,
                        step=10,
                        help="Tempo m√°ximo de espera por cada query (padr√£o 60s)"
                    )

                # Aviso se coluna de parti√ß√£o n√£o existir
                try:
                    cols_check = [c['name'] for c in get_table_columns(full_table)]
                    if st.session_state.partition_column_name and st.session_state.partition_column_name not in cols_check:
                        st.warning(f"‚ö†Ô∏è Coluna de parti√ß√£o '{st.session_state.partition_column_name}' n√£o encontrada na tabela. O filtro da √∫ltima parti√ß√£o ser√° ignorado.")
                except Exception:
                    pass
                
                # Se√ß√£o de Explora√ß√£o Autom√°tica
                st.markdown("---")
                st.markdown("#### üîç Explora√ß√£o Autom√°tica da Tabela")
                
                # Status da explora√ß√£o
                exploration = st.session_state.table_exploration_cache.get(full_table, {})
                if exploration:
                    st.success("‚úÖ **Explora√ß√£o dispon√≠vel!**")
                    col_stats1, col_stats2, col_stats3, col_stats4 = st.columns(4)
                    with col_stats1:
                        st.metric("üìä Colunas", len(exploration.get('columns', [])))
                    with col_stats2:
                        st.metric("üö© Flags", len(exploration.get('flag_columns', {})))
                    with col_stats3:
                        st.metric("üìù Descri√ß√µes", len(exploration.get('description_columns', {})))
                    with col_stats4:
                        st.metric("üî¢ C√≥digos", len(exploration.get('code_columns', {})))
                    
                    if exploration.get('exploration_time'):
                        st.caption(f"üïí √öltima explora√ß√£o: {exploration['exploration_time']}")
                    
                    # Mostrar amostra de flags, descri√ß√µes e c√≥digos
                    with st.expander("üîç Ver Valores Mapeados", expanded=False):
                        st.markdown("**üö© Flags:**")
                        if exploration.get('flag_columns'):
                            for col, values in list(exploration.get('flag_columns', {}).items())[:5]:
                                st.code(f"{col}: {values}")
                        else:
                            st.caption("Nenhuma coluna flag_ encontrada")
                        
                        st.markdown("**üìù Descri√ß√µes (amostra):**")
                        if exploration.get('description_columns'):
                            for col, values in list(exploration.get('description_columns', {}).items())[:5]:
                                st.code(f"{col}: {len(values)} valores √∫nicos | Exemplos: {values[:3]}")
                        else:
                            st.caption("Nenhuma coluna ds_/dscr_/desc_ encontrada")
                        
                        st.markdown("**üî¢ C√≥digos (amostra):**")
                        if exploration.get('code_columns'):
                            for col, values in list(exploration.get('code_columns', {}).items())[:5]:
                                st.code(f"{col}: {len(values)} valores √∫nicos | Exemplos: {values[:3]}")
                        else:
                            st.caption("Nenhuma coluna cd_/codigo_/codi_ encontrada")
                else:
                    st.warning("‚ö†Ô∏è Explora√ß√£o n√£o realizada ainda.")
                    st.caption("üí° A explora√ß√£o autom√°tica mapeia colunas, flags e valores dispon√≠veis para aprimorar o assistente e as queries.")
                
                # Bot√£o para explorar/re-explorar
                col_explore_btn, col_explore_info = st.columns([1, 2])
                with col_explore_btn:
                    if st.button("üîç Explorar Tabela", use_container_width=True, type="primary"):
                        with st.spinner("üîç Explorando tabela... Isso pode levar alguns segundos."):
                            exploration_result = explore_table_metadata(full_table, force_refresh=True)
                        
                        if exploration_result.get('columns'):
                            st.success(f"‚úÖ Explora√ß√£o conclu√≠da! {len(exploration_result['columns'])} colunas mapeadas.")
                            st.session_state.auto_exploration_done = True
                            st.rerun()
                        else:
                            st.error("‚ùå Falha na explora√ß√£o. Verifique conex√£o.")
                
                with col_explore_info:
                    st.caption("üîÑ Re-executa explora√ß√£o autom√°tica da tabela")
            else:
                st.warning("‚ö†Ô∏è **Tabela n√£o configurada!** Preencha Catalog, Schema e Tabela.")
                st.caption("O sistema adaptativo n√£o funcionar√° sem a configura√ß√£o completa da tabela.")

        # Language Configuration
        with st.expander(translations[st.session_state.language]['language'], expanded=False):
            lang_options = list(translations[st.session_state.language]['langs'].values())
            lang_map = {v: k for k, v in translations[st.session_state.language]['langs'].items()}
            current_display = translations[st.session_state.language]['langs'][st.session_state.language]
            index = lang_options.index(current_display)
            selected_display = st.selectbox(
                translations[st.session_state.language]['select_lang'],
                lang_options,
                index=index
            )
            st.session_state.language = lang_map[selected_display]

        # Query History
        with st.expander(translations[lang]['history'], expanded=False):
            if st.session_state.query_history:
                for i, query in enumerate(reversed(st.session_state.query_history[-10:])):
                    if st.button(f"üîÑ {query[:50]}...", key=f"history_{i}"):
                        st.session_state.last_query = query
                        st.rerun()
            else:
                st.info("No query history yet")

            if st.button(translations[lang]['clear_history']):
                st.session_state.query_history = []
                st.success("History cleared!")

    # Main Query Interface
    if st.session_state.agent_type == 'chat':
        # Auto-explora√ß√£o ao iniciar (se tabela configurada e ainda n√£o explorada)
        # Usar apenas o cache para evitar re-execu√ß√£o desnecess√°ria
        catalog = st.session_state.connection_config.get('catalog', '')
        schema = st.session_state.connection_config.get('schema', '')
        context_table = st.session_state.ai_config.get('context_table', '')
        
        if catalog and schema and context_table:
            full_table = f"{catalog}.{schema}.{context_table}"
            # Executar explora√ß√£o APENAS se n√£o estiver no cache
            if full_table not in st.session_state.table_exploration_cache:
                with st.spinner("üîç Executando explora√ß√£o autom√°tica da tabela..."): 
                    exploration_result = explore_table_metadata(full_table)
                    if exploration_result.get('columns'):
                        st.success(f"‚úÖ Explora√ß√£o autom√°tica conclu√≠da! {len(exploration_result['columns'])} colunas, "
                                 f"{len(exploration_result.get('flag_columns', {}))} flags, "
                                 f"{len(exploration_result.get('description_columns', {}))} descri√ß√µes, "
                                 f"{len(exploration_result.get('code_columns', {}))} c√≥digos mapeados.")
                        st.session_state.auto_exploration_done = True
        
        st.subheader(translations[lang]['main_question'])
        
        # Garantir que modo 'direct' seja migrado para 'autonomous' (modo desabilitado)
        if st.session_state.query_mode == 'direct':
            st.session_state.query_mode = 'autonomous'
        
        # Sele√ß√£o de modo: Consulta Guiada ou Consulta Aut√¥noma
        mode_options = {
            # 'direct': 'Sem assistente',  # Desabilitado temporariamente
            'assistant': 'ü§ù Consulta Guiada (assistente interativo)',
            'autonomous': 'üöÄ Consulta Aut√¥noma (execu√ß√£o direta)'
        }
        selected_mode = st.selectbox(
            "Modo de consulta",
            list(mode_options.keys()),
            index=list(mode_options.keys()).index(st.session_state.query_mode),
            format_func=lambda x: mode_options[x],
            help="ü§ù Consulta Guiada: conversa com assistente para refinar sua pergunta | üöÄ Consulta Aut√¥noma: executa diretamente sem valida√ß√£o"
        )
        
        # Explica√ß√£o do modo selecionado
        if selected_mode == 'autonomous':
            st.info("üöÄ **Modo Aut√¥nomo:** Sua pergunta ser√° executada diretamente, ideal para consultas r√°pidas e objetivas.")
        elif selected_mode == 'assistant':
            st.info("ü§ù **Modo Guiado:** O assistente ajudar√° a refinar sua pergunta atrav√©s de uma conversa interativa.")
        if selected_mode != st.session_state.query_mode:
            st.session_state.query_mode = selected_mode
            st.session_state.assistant_mode = selected_mode == 'assistant'
            st.session_state.autonomous_mode = selected_mode == 'autonomous'
            st.session_state.auto_execute_from_assistant = False
            if selected_mode != 'assistant':
                st.session_state.assistant_chat_history = []
                st.session_state.assistant_confidence = 0
                st.session_state.final_refined_query = ""
            st.rerun()
        else:
            st.session_state.assistant_mode = selected_mode == 'assistant'
            st.session_state.autonomous_mode = selected_mode == 'autonomous'
        
        # Se modo assistente estiver ativo
        if st.session_state.assistant_mode:
            st.markdown("---")
            st.subheader(translations[lang]['query_assistant'])
            
            # Mostrar hist√≥rico do chat
            if st.session_state.assistant_chat_history:
                with st.container():
                    st.markdown("**" + translations[lang]['assistant_chat'] + "**")
                    for msg in st.session_state.assistant_chat_history:
                        if msg['role'] == 'user':
                            st.markdown(f"**üë§ Voc√™:** {msg['content']}")
                        else:
                            confidence = msg.get('confidence', 0)
                            if confidence >= 80:
                                confidence_label = translations[lang]['confidence_high']
                                emoji = "‚úÖ"
                            elif confidence >= 50:
                                confidence_label = translations[lang]['confidence_medium']
                                emoji = "‚ö†Ô∏è"
                            else:
                                confidence_label = translations[lang]['confidence_low']
                                emoji = "‚ùì"
                            
                            st.markdown(f"**ü§ñ Assistente ({emoji} {confidence}%):** {msg['content']}")
                            if msg.get('suggestions'):
                                st.caption("üí° Sugest√µes: " + ", ".join(msg['suggestions']))
                    st.markdown("---")
            
            # Campo para nova mensagem ao assistente
            # Usar key din√¢mica para for√ßar recria√ß√£o do campo e limpar o valor
            assistant_input = st.text_area(
                translations[lang]['assistant_message'],
                value="",  # Sempre come√ßa vazio
                placeholder="Ex: Quero ver total de vendas do √∫ltimo m√™s por categoria de produto",
                height=80,
                key=f"assistant_input_{st.session_state.assistant_input_counter}"  # Key din√¢mica
            )
            
            col_send, col_finalize = st.columns(2)
            with col_send:
                send_to_assistant = st.button(
                    translations[lang]['send_to_assistant'],
                    type="secondary",
                    use_container_width=True
                )
            with col_finalize:
                finalize_query_btn = st.button(
                    translations[lang]['finalize_query'],
                    type="primary",
                    use_container_width=True,
                    disabled=st.session_state.assistant_confidence < 80
                )
            
            # Processar envio ao assistente
            if send_to_assistant and assistant_input:
                with st.spinner(translations[lang]['assistant_analyzing']):
                    # Adicionar mensagem do usu√°rio ao hist√≥rico
                    st.session_state.assistant_chat_history.append({
                        'role': 'user',
                        'content': assistant_input
                    })
                    
                    # Obter resposta do assistente
                    assistant_response = query_assistant_analyze(
                        assistant_input,
                        st.session_state.assistant_chat_history
                    )
                    
                    # Adicionar resposta ao hist√≥rico
                    st.session_state.assistant_chat_history.append({
                        'role': 'assistant',
                        'content': assistant_response['message'],
                        'confidence': assistant_response['confidence'],
                        'suggestions': assistant_response.get('suggestions', [])
                    })
                    
                    # Atualizar confian√ßa e query refinada
                    st.session_state.assistant_confidence = assistant_response['confidence']
                    st.session_state.final_refined_query = assistant_response['refined_query']
                    
                    # Incrementar contador para for√ßar nova key e limpar campo
                    st.session_state.assistant_input_counter += 1
                    
                    st.rerun()
            
            # Finalizar e executar query AUTOMATICAMENTE
            if finalize_query_btn and st.session_state.final_refined_query:
                # Consolidar prompt final
                consolidated_prompt = "CONSULTA CONSOLIDADA PELO ASSISTENTE:\n\n"
                consolidated_prompt += f"üéØ Objetivo Final: {st.session_state.final_refined_query}\n\n"
                consolidated_prompt += "Detalhes da Conversa:\n"
                for i, msg in enumerate(st.session_state.assistant_chat_history, 1):
                    if msg['role'] == 'user':
                        consolidated_prompt += f"{i}. üë§ Voc√™: {msg['content']}\n"
                    else:
                        consolidated_prompt += f"{i}. ü§ñ Assistente: {msg['content']}\n"
                
                # Armazenar prompt consolidado
                st.session_state.consolidated_assistant_prompt = consolidated_prompt
                
                # Mostrar prompt consolidado em expander
                with st.expander("üìù Ver Prompt Consolidado Final", expanded=True):
                    st.markdown(consolidated_prompt)
                    st.success(f"‚úÖ Confian√ßa Final: {st.session_state.assistant_confidence}%")
                
                # Atualizar campo de input principal
                st.session_state.user_input_value = st.session_state.final_refined_query
                
                # SALVAR dados antes de limpar para exibir no resumo
                st.session_state.last_assistant_confidence = st.session_state.assistant_confidence
                st.session_state.last_refined_query = st.session_state.final_refined_query
                
                # Marcar que deve executar automaticamente
                st.session_state.auto_execute_from_assistant = True
                st.session_state.assistant_mode = False
                
                # Manter hist√≥rico temporariamente para refer√™ncia
                st.session_state.last_assistant_chat = st.session_state.assistant_chat_history.copy()
                st.session_state.assistant_chat_history = []
                st.session_state.assistant_confidence = 0
                st.session_state.final_refined_query = ""
                
                st.success(f"‚úÖ Query consolidada e campo atualizado!")
                st.info("‚è≥ Preparando execu√ß√£o autom√°tica...")
                time.sleep(0.5)
                st.rerun()
            
            # Mostrar query refinada atual
            if st.session_state.final_refined_query:
                st.markdown("---")
                st.markdown("**üìù Query Refinada Atual:**")
                st.info(st.session_state.final_refined_query)
            
            st.markdown("---")
        
        # Query input with enhanced styling - MANTER VALOR ao clicar
        user_query = st.text_area(
            "Enter your question in natural language:",
            value=st.session_state.user_input_value,
            placeholder=translations[lang]['placeholder'],
            height=100,
            key="main_query"
        )
        
        # Atualizar o valor armazenado quando usu√°rio digitar
        if user_query != st.session_state.user_input_value:
            st.session_state.user_input_value = user_query
    else:  # modular_prompt
        st.subheader("ü§ñ " + translations[lang]['modular_prompt_agent'])

        # Show current step and instructions
        current_step = st.session_state.modular_config['current_step']
        if current_step < len(st.session_state.modular_config['contexts']) and st.session_state.modular_config['contexts'][current_step].strip():
            st.markdown(f"### Etapa {current_step + 1}")
            st.info(st.session_state.modular_config['contexts'][current_step])

            # Show previous responses
            if st.session_state.modular_config['responses']:
                st.markdown("### Respostas Anteriores:")
                for i, response in enumerate(st.session_state.modular_config['responses']):
                    st.markdown(f"**Etapa {i+1}:** {response}")

            user_query = st.text_area(
                f"Sua resposta para a Etapa {current_step + 1}:",
                height=100,
                key="modular_query"
            )
        else:
            st.success("Processo modular conclu√≠do!")
            user_query = ""

    if st.session_state.agent_type == 'chat':
        execute_label = translations[lang]['execute']
        if st.session_state.autonomous_mode:
            execute_label = "üöÄ Executar (Consulta Aut√¥noma)"
        elif st.session_state.assistant_mode:
            execute_label = "ü§ù Executar (Consulta Guiada)"
        col1, col2, col3 = st.columns([2, 1, 1])
        with col1:
            execute_button = st.button(execute_label, type="primary", use_container_width=True)
        with col2:
            # Habilitar se tiver resultados dispon√≠veis (current_df ou last_sql)
            refine_enabled = (
                st.session_state.get('current_df') is not None or 
                st.session_state.get('last_sql', '') != '' or
                st.session_state.get('last_query', '') != ''
            )
            if st.button("üîß Refinar", use_container_width=True, disabled=not refine_enabled):
                st.session_state.show_refine_section = not st.session_state.show_refine_section
                st.rerun()
        with col3:
            clear_button = st.button(translations[lang]['clear'], use_container_width=True)
            
    else:  # modular_prompt
        col1, col2, col3 = st.columns([2, 1, 1])
        with col1:
            if st.session_state.modular_config['current_step'] == 0:
                execute_button = st.button(translations[lang]['start_modular'], type="primary", use_container_width=True)
            else:
                execute_button = st.button(translations[lang]['continue_step'], type="primary", use_container_width=True)
        with col2:
            finish_button = st.button(translations[lang]['finish_process'], use_container_width=True)
        with col3:
            clear_button = st.button(translations[lang]['clear'], use_container_width=True)
        refine_button = False
        refine_query_button = False

    # Se√ß√£o de Refinamento Simples - Aparece logo abaixo dos bot√µes
    if st.session_state.show_refine_section and st.session_state.current_df is not None:
        st.markdown("---")
        col_ref1, col_ref2 = st.columns([3, 1])
        with col_ref1:
            refine_input = st.text_input(
                "Inclua aqui o seu pedido de refinamento:",
                placeholder="Ex: filtrar √∫ltimos 30 dias, agrupar por m√™s",
                key="refine_simple_input"
            )
        with col_ref2:
            if st.button("üöÄ Refinar", type="primary", use_container_width=True, disabled=not refine_input):
                with st.spinner("üîÑ Refinando consulta..."):
                    consolidated_query = f"{st.session_state.last_query}. {refine_input}"
                    
                    result = refine_query_with_additional_context(
                        st.session_state.last_query,
                        st.session_state.last_sql,
                        st.session_state.current_df,
                        refine_input,
                        get_bedrock_client(),
                        get_context,
                        st.session_state.table_exploration_cache
                    )
                    
                    if result and result.get('sql'):
                        with st.spinner("‚ö° Executando query refinada..."):
                            df_refined = execute_query(result['sql'])
                        
                        if df_refined is not None and not df_refined.empty:
                            st.session_state.current_df = df_refined
                            st.session_state.last_query = consolidated_query
                            st.session_state.last_sql = result['sql']
                            st.session_state.query_executed_successfully = True
                            st.session_state.show_refine_section = False
                            st.success("‚úÖ Consulta refinada com sucesso!")
                            st.rerun()
                        elif df_refined is not None:
                            st.warning("‚ö†Ô∏è Consulta retornou 0 linhas")
                        else:
                            st.error("‚ùå Erro ao executar consulta refinada")
                    else:
                        st.error("‚ùå Erro ao gerar SQL refinada")
        st.markdown("---")
    
    # Smart Query Suggestions - Geradas por IA baseadas no contexto
    if not st.session_state.first_execution_attempted and st.session_state.agent_type == 'chat':
        st.subheader("üí° Sugest√µes de Consultas")
        
        # Gerar perguntas apenas uma vez quando necess√°rio
        if not st.session_state.questions_generated or not st.session_state.smart_questions:
            try:
                # Verificar se tem token configurado
                if st.session_state.get('aws_bedrock_token', '').strip():
                    with st.spinner("ü§î Gerando sugest√µes inteligentes baseadas no contexto..."):
                        bedrock_client = get_bedrock_client()
                        context = get_context()
                        
                        # Obter cache de explora√ß√£o se dispon√≠vel
                        catalog = st.session_state.connection_config.get('catalog', '')
                        schema = st.session_state.connection_config.get('schema', '')
                        context_table = st.session_state.ai_config.get('context_table', '')
                        exploration_cache = {}
                        if catalog and schema and context_table:
                            full_table = f"{catalog}.{schema}.{context_table}"
                            exploration_cache = st.session_state.table_exploration_cache.get(full_table, {})
                        
                        questions = generate_smart_questions(context, bedrock_client, exploration_cache)
                        st.session_state.smart_questions = questions
                        st.session_state.questions_generated = True
                else:
                    st.info("üí° Configure o token AWS Bedrock no menu lateral para gerar sugest√µes personalizadas")
                    st.session_state.smart_questions = []
                    st.session_state.questions_generated = True
            except Exception as e:
                print(f"Erro ao gerar sugest√µes: {e}")
                st.session_state.smart_questions = []
                st.session_state.questions_generated = True
        
        # Exibir sugest√µes se dispon√≠veis
        if st.session_state.smart_questions:
            col_suggestions_header, col_regenerate = st.columns([3, 1])
            with col_suggestions_header:
                st.caption("Clique em uma pergunta para come√ßar:")
            with col_regenerate:
                if st.button("üîÑ Regerar", key="regenerate_questions", help="Gerar novas sugest√µes"):
                    st.session_state.questions_generated = False
                    st.rerun()
            
            # Organizar em 2 colunas
            col1, col2 = st.columns(2)
            for i, suggestion in enumerate(st.session_state.smart_questions):
                with col1 if i % 2 == 0 else col2:
                    if st.button(f"üí¨ {suggestion}", key=f"suggestion_{i}", use_container_width=True):
                        # Preencher o campo com a sugest√£o e executar automaticamente em modo aut√¥nomo
                        print(f"[BRIAN_SUGGESTION] üéØ Sugest√£o clicada: {suggestion}")
                        st.session_state.user_input_value = suggestion
                        print(f"[BRIAN_SUGGESTION] ‚úÖ Valor salvo em session_state: {st.session_state.user_input_value}")
                        
                        # CR√çTICO: Limpar a key do text_area para for√ßar atualiza√ß√£o do value
                        if 'main_query' in st.session_state:
                            del st.session_state['main_query']
                            print(f"[BRIAN_SUGGESTION] üóëÔ∏è Key 'main_query' deletada para for√ßar atualiza√ß√£o")
                        
                        st.session_state.first_execution_attempted = True  # Ocultar sugest√µes ap√≥s clicar
                        st.session_state.autonomous_mode = True            # Habilitar modo aut√¥nomo
                        st.session_state.auto_execute_from_assistant = True # Reutilizar fluxo de autoexecu√ß√£o
                        print(f"[BRIAN_SUGGESTION] üîÑ Fazendo rerun...")
                        st.rerun()
        
        st.markdown("---")    # Execute Query - Incluir auto-execu√ß√£o do assistente
    auto_execute = st.session_state.get('auto_execute_from_assistant', False)
    
    if (execute_button and user_query) or auto_execute:
        st.session_state.first_execution_attempted = True
        
        # Se veio do auto_execute (assistente ou sugest√£o clicada), usar o valor salvo
        if auto_execute:
            user_query = st.session_state.user_input_value
            st.session_state.auto_execute_from_assistant = False  # Resetar flag
            
            # Validar que temos uma query v√°lida
            if not user_query or not user_query.strip():
                st.warning("‚ö†Ô∏è Nenhuma consulta foi fornecida. Por favor, digite sua pergunta.")
                print(f"[BRIAN_EXEC] ‚ùå Query vazia detectada. session_state.user_input_value = '{st.session_state.user_input_value}'")
                st.stop()
            
            print(f"[BRIAN_EXEC] ‚úÖ Auto-execute com query: {user_query}")
        
        # Garantir que o campo principal reflita a consulta executada
        st.session_state.user_input_value = user_query
        
        # Consolidar contexto do assistente em um √∫nico expander MINIMIZADO
        if auto_execute and st.session_state.last_assistant_chat:
            # Criar resumo usando valores SALVOS
            total_messages = len(st.session_state.last_assistant_chat)
            user_messages = len([m for m in st.session_state.last_assistant_chat if m['role'] == 'user'])
            final_confidence = st.session_state.get('last_assistant_confidence', 0)
            final_query = st.session_state.get('last_refined_query', user_query)
            
            with st.expander(
                f"ü§ù Resumo da Consulta Assistida (Confian√ßa: {final_confidence}% | {user_messages} intera√ß√µes)", 
                expanded=False
            ):
                # Mostrar prompt consolidado
                if st.session_state.get('consolidated_assistant_prompt'):
                    st.markdown("**üéØ Query Final Consolidada:**")
                    st.success(final_query)
                    st.markdown("---")
                    st.markdown("**üìù Detalhes da Conversa:**")
                    st.text(st.session_state.consolidated_assistant_prompt)
                else:
                    st.markdown("**üí¨ Hist√≥rico da conversa:**")
                    for msg in st.session_state.last_assistant_chat:
                        if msg['role'] == 'user':
                            st.markdown(f"üë§ **Voc√™:** {msg['content']}")
                        else:
                            st.markdown(f"ü§ñ **Assistente:** {msg['content']}")
                
                st.markdown("---")
                st.info("‚úÖ Esta query foi refinada atrav√©s de conversa guiada para garantir alta precis√£o!")
        
        if st.session_state.agent_type == 'chat':
            sql_override = st.session_state.get('sql_override', "")
            # Pular o judge se veio do assistente (j√° foi refinado) OU se modo aut√¥nomo est√° ativo
            if not auto_execute and not st.session_state.get('autonomous_mode', False) and st.session_state.feature_flags.get('query_judge', True):
                full_table = f"{st.session_state.connection_config.get('catalog', 'prd')}.{st.session_state.connection_config.get('schema', 'sand_snd_box_cartao')}.{st.session_state.ai_config.get('context_table', 'tb_pbi_autz_pampa_diar')}"
                judge = judge_user_query_intent(user_query, full_table)
                if judge.get('needs_clarification'):
                    st.warning(judge.get('message', 'A consulta parece amb√≠gua.'))
                    suggested = judge.get('suggested_query')
                    if suggested:
                        st.info(f"Sugest√£o: {suggested}")
                    st.stop()

            if sql_override:
                sql_query = sql_override
                interpretation_data = None
                st.session_state.sql_override = ""
                st.info("‚úÖ SQL corrigida a partir da rean√°lise de erros ser√° executada.")
            else:
                with st.spinner("ü§ñ Gerando SQL query otimizada..."):
                    # Generate SQL based on selected model
                    if st.session_state.ai_config['model'] == "databricks":
                        sql_query = generate_sql_with_databricks_llama(user_query)
                        interpretation_data = None
                    else:
                        sql_query, interpretation_data = generate_sql_with_bedrock_claude(user_query)
                
                # Exibir racioc√≠nio da IA - APENAS INTERPRETA√á√ÉO PRINCIPAL
                if interpretation_data and st.session_state.feature_flags.get('query_reasoning', True):
                    st.markdown("### üß† Racioc√≠nio da IA")
                    
                    # Interpreta√ß√£o principal - sempre vis√≠vel (COMPACTA)
                    st.info(f"**üìù Como entendi sua pergunta:** {interpretation_data.get('interpretation', 'N/A')} üí° *Saiba mais em 'Detalhes da Query'*")
        else:  # modular_prompt
            with st.spinner("ü§ñ Processando etapa modular..."):
                sql_query = process_modular_step(user_query)

        if sql_query:
            # TUDO em um √∫nico expander colaps√°vel
            query_origin = "ü§ù Assistente" if auto_execute else "üîç Direta"
            
            # Preparar auto-avalia√ß√£o ANTES do expander
            reasoning_info = None
            if st.session_state.feature_flags.get('query_reasoning', True):
                reasoning = validate_query_with_reasoning(sql_query, get_context())
                reasoning_text = reasoning.get('reasoning', 'Valida√ß√£o conclu√≠da')
                confidence = reasoning.get('confidence')
                is_valid = reasoning.get('is_valid', True)
                reasoning_info = {'text': reasoning_text, 'confidence': confidence, 'is_valid': is_valid, 'suggestions': reasoning.get('suggestions')}
            
            # T√≠tulo do expander com resumo
            expander_title = f"üìã Detalhes da Query ({query_origin}"
            if reasoning_info:
                expander_title += f" | Confian√ßa: {reasoning_info['confidence']}"
            expander_title += ")"
            
            with st.expander(expander_title, expanded=False):
                # Origem
                if auto_execute:
                    st.success("‚úÖ Query consolidada pelo Assistente de Consulta")
                
                # DETALHAMENTO DO RACIOC√çNIO DA IA (movido para c√°)
                if interpretation_data and st.session_state.feature_flags.get('query_reasoning', True):
                    st.markdown("---")
                    st.markdown("### üß† Racioc√≠nio Detalhado da IA")
                    
                    # Determinar se deve expandir por padr√£o (sempre no modo aut√¥nomo)
                    is_autonomous = st.session_state.get('autonomous_mode', False)
                    
                    # Decis√µes tomadas
                    decisions = interpretation_data.get('decisions', {})
                    if decisions:
                        with st.expander("üéØ Decis√µes Tomadas pela IA", expanded=True):
                            if decisions.get('period'):
                                st.markdown(f"**üìÖ Per√≠odo:** {decisions['period']}")
                            if decisions.get('metrics'):
                                st.markdown(f"**üìä M√©tricas calculadas:**")
                                for metric in decisions['metrics']:
                                    st.markdown(f"  - {metric}")
                            if decisions.get('filters'):
                                st.markdown(f"**üîç Filtros aplicados:**")
                                for filter_item in decisions['filters']:
                                    st.markdown(f"  - {filter_item}")
                            if decisions.get('grouping'):
                                st.markdown(f"**üìÇ Agrupamento:** {decisions['grouping']}")
                            if decisions.get('formulas'):
                                st.markdown(f"**üßÆ F√≥rmulas utilizadas:**")
                                for formula in decisions['formulas']:
                                    st.markdown(f"  - {formula}")
                    
                    # Racioc√≠nio passo-a-passo
                    reasoning = interpretation_data.get('reasoning', '')
                    if reasoning:
                        with st.expander("üí≠ Racioc√≠nio Passo-a-Passo", expanded=False):
                            st.markdown(reasoning)
                    
                    # Clarifica√ß√µes/Suposi√ß√µes
                    clarifications = interpretation_data.get('clarifications', [])
                    if clarifications:
                        with st.expander("‚ö†Ô∏è Suposi√ß√µes e Decis√µes Aut√¥nomas", expanded=True):
                            if is_autonomous:
                                st.warning("ü§ñ **Modo Aut√¥nomo Ativo:** A IA tomou as seguintes decis√µes por conta pr√≥pria:")
                            for clarification in clarifications:
                                st.markdown(f"  - {clarification}")
                            if is_autonomous:
                                st.info("‚úÖ **Modo Aut√¥nomo:** Revise as decis√µes acima antes de prosseguir!")
                    
                    st.markdown("---")
                
                # SQL completa
                st.markdown("**üìù SQL Gerada:**")
                st.code(sql_query, language="sql")
                
                # Auto-avalia√ß√£o
                if reasoning_info:
                    prefix = "‚úÖ" if reasoning_info['is_valid'] else "‚ö†Ô∏è"
                    st.markdown(f"**{prefix} Auto-avalia√ß√£o (confian√ßa: {reasoning_info['confidence']}):**")
                    st.info(reasoning_info['text'])
                    if reasoning_info.get('suggestions'):
                        st.caption("üí° Sugest√µes: " + "; ".join(reasoning_info['suggestions']))
                
                # Prompt Final (avan√ßado) - colaps√°vel
                if st.session_state.get('last_generation_prompt'):
                    with st.expander("üî¨ [Avan√ßado] Prompt Final Enviado √† IA", expanded=False):
                        st.markdown("Este √© o prompt completo que foi enviado √† IA para gerar a SQL:")
                        st.code(st.session_state['last_generation_prompt'], language="text")
                        st.caption("‚ÑπÔ∏è O prompt inclui o contexto da tabela, valores reais das colunas e regras de gera√ß√£o.")
                
                # Tempo de execu√ß√£o (se dispon√≠vel)
                exec_time = st.session_state.get('last_execution_time')
                if exec_time:
                    if exec_time > 30:
                        st.warning(f"‚è±Ô∏è Tempo de execu√ß√£o: {exec_time:.1f}s (considere adicionar filtros)")
                    else:
                        st.info(f"‚è±Ô∏è Tempo de execu√ß√£o: {exec_time:.1f}s")
            
            st.session_state.last_sql = sql_query

            run_query = True
            if st.session_state.feature_flags.get('pre_validation', True):
                pre_val = pre_validate_query_with_count(sql_query)
                if pre_val.get('is_empty'):
                    default_table = f"{st.session_state.connection_config.get('catalog', 'prd')}.{st.session_state.connection_config.get('schema', 'sand_snd_box_cartao')}.{st.session_state.ai_config.get('context_table', 'tb_pbi_autz_pampa_diar')}"
                    table_for_meta = pre_val.get('table_name', default_table)
                    # OTIMIZA√á√ÉO: Usar cache de explora√ß√£o ao inv√©s de executar queries
                    metadata = st.session_state.table_exploration_cache.get(table_for_meta, {})
                    suggestion_text = generate_smart_suggestions_for_empty_result(user_query, table_for_meta, metadata)
                    st.warning(f"Pr√©-valida√ß√£o identificou 0 linhas.\n\n{suggestion_text}")
                    update_last_query_context(user_query, sql_query, pd.DataFrame())
                    run_query = False

            if run_query:
                # Execute query with retry logic
                max_retries = st.session_state.ai_config['max_retries'] if st.session_state.feature_flags['error_correction'] else 1
                st.session_state.execution_errors = []
                df = None

                for attempt in range(max_retries):
                    with st.spinner(f"‚ö° Executing query (attempt {attempt + 1})..."):
                        df = execute_query(sql_query)

                    if df is not None:
                        break
                    elif attempt < max_retries - 1 and st.session_state.feature_flags['error_correction']:
                        st.warning(f"Query failed, attempting correction (attempt {attempt + 2})...")
                        # Here you could implement SQL correction logic

                if df is not None and not df.empty:
                    st.session_state.current_df = df
                    st.session_state.query_history.append(user_query)
                    update_last_query_context(user_query, sql_query, df)
                    
                    # Reset par√¢metros do gr√°fico manual quando uma nova query √© executada
                    st.session_state.manual_chart_generated = False
                    st.session_state.manual_chart_option = None
                    st.session_state.manual_chart_type = 'Autom√°tico'
                    st.session_state.manual_color = '#5470c6'
                    st.session_state.manual_x = None
                    st.session_state.manual_y = []
                    st.session_state.manual_agg = 'sum'
                    
                    # Flag para indicar execu√ß√£o bem-sucedida
                    st.session_state.query_executed_successfully = True

                    # Mensagem de sucesso compacta
                    st.success("‚úÖ Query executada com sucesso!")

                    # Alertas compactos
                    if st.session_state.feature_flags.get('sampling_alerts', True):
                        alert = check_sampling_alert(sql_query, df)
                        if alert:
                            st.info(alert)

                    # Detectar pedido focado em gr√°fico para reduzir an√°lises
                    chart_only = False
                    chart_keywords = ['gr√°fico', 'grafico', 'chart', 'plot', 'barra', 'barras', 'linha', 'linhas', 'funil']
                    analysis_keywords = ['an√°lise', 'analise', 'analysis', 'explicar', 'causa', 'por que', 'porque']
                    if any(k in user_query.lower() for k in chart_keywords) and not any(k in user_query.lower() for k in analysis_keywords):
                        chart_only = True

                    # Structured Analysis Output com Auto-Avalia√ß√£o
                    st.markdown("### " + translations[lang]['direct'])
                    
                    # Se pede gr√°fico, renderizar primeiro o gr√°fico e depois texto breve
                    if chart_only:
                        # Renderizar gr√°fico primeiro
                        if st.session_state.feature_flags['smart_visualization']:
                            generate_smart_visualization(df, user_query)
                        # Texto breve de contexto
                        with st.spinner(translations[lang]['generating_direct']):
                            direct_response = generate_direct_response(df, user_query)
                        st.write(direct_response)
                    else:
                        # Fluxo normal: texto primeiro
                        with st.spinner(translations[lang]['generating_direct']):
                            direct_response = generate_direct_response(df, user_query)
                        st.write(direct_response)

                        # Visualiza√ß√£o autom√°tica junto da resposta direta
                        if st.session_state.feature_flags.get('smart_visualization', True):
                            st.markdown("### üìä Visualiza√ß√£o autom√°tica")
                            generate_smart_visualization(df, user_query)
                    
                    if not chart_only:
                        # Auto-Avalia√ß√£o
                        with st.spinner("üîç Avaliando qualidade da resposta..."):
                            eval_direct = auto_evaluate_analysis(direct_response, df, "Resposta Direta")
                            st.session_state.analysis_scores['direct'] = eval_direct
                        
                        # Exibir score e avalia√ß√£o - COLAPS√ÅVEL
                        score = eval_direct.get('score', 0)
                        if score >= 80:
                            score_emoji = "‚úÖ"
                            score_color = "green"
                        elif score >= 60:
                            score_emoji = "‚ö†Ô∏è"
                            score_color = "orange"
                        else:
                            score_emoji = "‚ùå"
                            score_color = "red"
                        
                        with st.expander(f"{score_emoji} Score de Qualidade: {score}/100 - {eval_direct.get('summary', '')}", expanded=False):
                            if eval_direct.get('issues'):
                                st.warning("‚ö†Ô∏è Problemas identificados: " + ", ".join(eval_direct['issues']))
                            if eval_direct.get('suggestions'):
                                st.info("üí° Sugest√µes: " + ", ".join(eval_direct['suggestions']))
                            
                            if st.button("üìù Reportar Problema", key="feedback_direct", use_container_width=True):
                                st.session_state.show_feedback_form['direct'] = True
                        
                        # Formul√°rio de Feedback
                        if st.session_state.show_feedback_form.get('direct', False):
                            with st.form(key="form_feedback_direct"):
                                st.markdown("**üêõ Qual problema voc√™ identificou?**")
                                feedback = st.text_area(
                                    "Descreva o problema",
                                    placeholder="Ex: Os n√∫meros n√£o batem com o esperado, faltou considerar X, etc.",
                                    height=100
                                )
                                col_submit, col_cancel = st.columns(2)
                                with col_submit:
                                    submit_feedback = st.form_submit_button("üîÑ Gerar Corre√ß√£o", type="primary")
                                with col_cancel:
                                    cancel_feedback = st.form_submit_button("‚ùå Cancelar")
                                
                                if submit_feedback and feedback:
                                    with st.spinner("ü§ñ Processando feedback e gerando corre√ß√£o..."):
                                            bedrock = get_bedrock_client()
                                            correction = regenerate_analysis_with_feedback(
                                                "direct",
                                                user_query,
                                                df,
                                                direct_response,
                                                feedback,
                                                bedrock
                                            )
                                    # Mostrar corre√ß√£o
                                    st.success("‚úÖ An√°lise do problema conclu√≠da!")
                                    st.markdown("**üîç Problemas Identificados:**")
                                    for issue in correction.get('issues_found', []):
                                        st.markdown(f"- {issue}")

                                    st.markdown("**üí° Explica√ß√£o da Corre√ß√£o:**")
                                    st.info(correction['explanation'])

                                    st.markdown("**‚úÖ An√°lise Corrigida:**")
                                    st.markdown(correction.get('corrected_analysis', 'N√£o foi poss√≠vel gerar an√°lise corrigida.'))

                                    st.session_state.show_feedback_form['direct'] = False
                                
                                if cancel_feedback:
                                    st.session_state.show_feedback_form['direct'] = False
                                    st.rerun()
                    
                    st.dataframe(df, use_container_width=True)

                    if not chart_only:
                        st.markdown("### " + translations[lang]['additional'])
                        with st.spinner(translations[lang]['generating_analysis']):
                            additional_analyses = generate_additional_analyses(df, user_query)
                        st.markdown(additional_analyses)
                        
                        # Auto-Avalia√ß√£o para Additional Analyses
                        with st.spinner("üîç Avaliando an√°lises adicionais..."):
                            eval_additional = auto_evaluate_analysis(additional_analyses, df, "An√°lises Adicionais")
                            st.session_state.analysis_scores['additional'] = eval_additional
                        
                        score_add = eval_additional.get('score', 0)
                        score_emoji_add = "‚úÖ" if score_add >= 80 else ("‚ö†Ô∏è" if score_add >= 60 else "‚ùå")
                        
                        with st.expander(f"{score_emoji_add} Score: {score_add}/100 - {eval_additional.get('summary', '')}", expanded=False):
                            if eval_additional.get('issues'):
                                st.warning("‚ö†Ô∏è " + ", ".join(eval_additional['issues']))
                            
                            if st.button("üìù Reportar Problema", key="feedback_additional", use_container_width=True):
                                st.session_state.show_feedback_form['additional'] = True
                        
                            # Formul√°rio de Feedback para Additional
                            if st.session_state.show_feedback_form.get('additional', False):
                                with st.form(key="form_feedback_additional"):
                                    st.markdown("**üêõ Qual problema voc√™ identificou nas an√°lises adicionais?**")
                                    feedback_add = st.text_area(
                                        "Descreva o problema",
                                        placeholder="Ex: Faltou analisar X, os insights est√£o incorretos, etc.",
                                        height=100
                                    )
                                    col_submit_add, col_cancel_add = st.columns(2)
                                    with col_submit_add:
                                        submit_feedback_add = st.form_submit_button("üîÑ Gerar Corre√ß√£o", type="primary")
                                    with col_cancel_add:
                                        cancel_feedback_add = st.form_submit_button("‚ùå Cancelar")
                                
                                    if submit_feedback_add and feedback_add:
                                        with st.spinner("ü§ñ Regenerando an√°lises adicionais..."):
                                            bedrock = get_bedrock_client()
                                            correction_add = regenerate_analysis_with_feedback(
                                                "additional",
                                                user_query,
                                                df,
                                                additional_analyses,
                                                feedback_add,
                                                bedrock
                                            )
                                
                                        st.success("‚úÖ An√°lise corrigida!")
                                        st.markdown("**üîç Problemas Identificados:**")
                                        for issue in correction_add.get('issues_found', []):
                                            st.markdown(f"- {issue}")
                                
                                        st.markdown("**üí° Explica√ß√£o:**")
                                        st.info(correction_add['explanation'])
                                
                                        st.markdown("**‚úÖ An√°lises Adicionais Corrigidas:**")
                                        st.markdown(correction_add.get('corrected_analysis', ''))
                                
                                        st.session_state.show_feedback_form['additional'] = False
                                
                                    if cancel_feedback_add:
                                        st.session_state.show_feedback_form['additional'] = False
                                        st.rerun()

                    st.markdown("### " + translations[lang]['causes'])
                    with st.spinner(translations[lang]['generating_causes']):
                        causes = generate_causes_and_new_analyses(df, user_query)
                    st.markdown(causes)
                    
                    # Auto-Avalia√ß√£o para Causes
                    with st.spinner("üîç Avaliando causas e hip√≥teses..."):
                        eval_causes = auto_evaluate_analysis(causes, df, "Causas e Hip√≥teses")
                        st.session_state.analysis_scores['causes'] = eval_causes
                    
                    score_causes = eval_causes.get('score', 0)
                    score_emoji_causes = "‚úÖ" if score_causes >= 80 else ("‚ö†Ô∏è" if score_causes >= 60 else "‚ùå")
                    
                    with st.expander(f"{score_emoji_causes} Score: {score_causes}/100 - {eval_causes.get('summary', '')}", expanded=False):
                        if eval_causes.get('issues'):
                            st.warning("‚ö†Ô∏è " + ", ".join(eval_causes['issues']))
                        
                        if st.button("üìù Reportar Problema", key="feedback_causes", use_container_width=True):
                            st.session_state.show_feedback_form['causes'] = True
                    
                        # Formul√°rio de Feedback para Causes
                        if st.session_state.show_feedback_form.get('causes', False):
                            with st.form(key="form_feedback_causes"):
                                st.markdown("**üêõ Qual problema voc√™ identificou nas causas e hip√≥teses?**")
                                feedback_causes = st.text_area(
                                    "Descreva o problema",
                                    placeholder="Ex: As causas propostas n√£o fazem sentido, faltou considerar Y, etc.",
                                    height=100
                                )
                                col_submit_causes, col_cancel_causes = st.columns(2)
                                with col_submit_causes:
                                    submit_feedback_causes = st.form_submit_button("üîÑ Gerar Corre√ß√£o", type="primary")
                                with col_cancel_causes:
                                    cancel_feedback_causes = st.form_submit_button("‚ùå Cancelar")
                            
                                if submit_feedback_causes and feedback_causes:
                                    with st.spinner("ü§ñ Regenerando causas e an√°lises..."):
                                        bedrock = get_bedrock_client()
                                        correction_causes = regenerate_analysis_with_feedback(
                                            "causes",
                                            user_query,
                                            df,
                                            causes,
                                            feedback_causes,
                                            bedrock
                                        )
                                
                                    st.success("‚úÖ An√°lise corrigida!")
                                    st.markdown("**üîç Problemas Identificados:**")
                                    for issue in correction_causes.get('issues_found', []):
                                        st.markdown(f"- {issue}")
                                
                                    st.markdown("**üí° Explica√ß√£o:**")
                                    st.info(correction_causes['explanation'])
                                
                                    st.markdown("**‚úÖ Causas e An√°lises Corrigidas:**")
                                    st.markdown(correction_causes.get('corrected_analysis', ''))
                                
                                    st.session_state.show_feedback_form['causes'] = False
                            
                                if cancel_feedback_causes:
                                    st.session_state.show_feedback_form['causes'] = False
                                    st.rerun()

                    st.markdown("### " + translations[lang]['code'])
                    # Exibir SQL de forma colaps√°vel
                    sql_lines = sql_query.split('\n')
                    preview_lines = sql_lines[:4]
                    preview_sql = '\n'.join(preview_lines)
                    
                    if len(sql_lines) <= 4:
                        st.code(sql_query, language="sql")
                    else:
                        st.code(preview_sql + '\n...', language="sql")
                        with st.expander("üìñ Ver SQL Completa", expanded=False):
                            st.code(sql_query, language="sql")

                    st.markdown("### " + translations[lang]['note'])
                    with st.spinner(translations[lang]['generating_note']):
                        note = generate_note(df, user_query, sql_query)
                    st.write(note)
                    
                    # Resumo Consolidado de Qualidade
                    if st.session_state.analysis_scores:
                        st.markdown("---")
                        st.markdown("### üìä Resumo de Qualidade da An√°lise")
                        
                        scores_data = []
                        for section, eval_data in st.session_state.analysis_scores.items():
                            scores_data.append({
                                "Se√ß√£o": section.title(),
                                "Score": eval_data.get('score', 0),
                                "Confian√ßa": eval_data.get('confidence', 'N/A'),
                                "Problemas": len(eval_data.get('issues', []))
                            })
                        
                        if scores_data:
                            scores_df = pd.DataFrame(scores_data)
                            
                            col1, col2 = st.columns(2)
                            with col1:
                                st.dataframe(scores_df, use_container_width=True, hide_index=True)
                            
                            with col2:
                                avg_score = scores_df['Score'].mean()
                                if avg_score >= 80:
                                    st.success(f"‚úÖ **Score M√©dio: {avg_score:.1f}/100**\n\nAn√°lise de alta qualidade!")
                                elif avg_score >= 60:
                                    st.warning(f"‚ö†Ô∏è **Score M√©dio: {avg_score:.1f}/100**\n\nAlguns pontos precisam aten√ß√£o.")
                                else:
                                    st.error(f"‚ùå **Score M√©dio: {avg_score:.1f}/100**\n\nRecomenda-se revisar a an√°lise.")
                                
                                total_issues = scores_df['Problemas'].sum()
                                if total_issues > 0:
                                    st.info(f"‚ö†Ô∏è {total_issues} problema(s) identificado(s) no total")

                    # Painel manual no final das an√°lises
                    if st.session_state.feature_flags.get('smart_visualization', True):
                        render_manual_chart_panel(df, user_query)
                    
                elif df is not None and df.empty:
                    st.warning("‚ùå Query executada, mas nenhum dado retornado.")
                    default_table = f"{st.session_state.connection_config.get('catalog', 'prd')}.{st.session_state.connection_config.get('schema', 'sand_snd_box_cartao')}.{st.session_state.ai_config.get('context_table', 'tb_pbi_autz_pampa_diar')}"
                    # OTIMIZA√á√ÉO: Usar cache de explora√ß√£o ao inv√©s de executar queries
                    metadata = st.session_state.table_exploration_cache.get(default_table, {})
                    if st.session_state.feature_flags.get('smart_suggestions', True):
                        suggestion_text = generate_smart_suggestions_for_empty_result(user_query, default_table, metadata)
                        st.info(suggestion_text)
                    update_last_query_context(user_query, sql_query, df)
                elif df is None:
                    st.markdown('<div class="error-message">‚ùå Failed to execute query after all retries.</div>', unsafe_allow_html=True)

                    if st.session_state.execution_errors:
                        with st.expander("üìÑ Detalhes dos erros", expanded=False):
                            for i, err in enumerate(st.session_state.execution_errors, 1):
                                st.code(f"{i}. {err}", language="text")

                    col_reanalyze_msg, col_reanalyze_btn = st.columns([3, 1])
                    with col_reanalyze_msg:
                        st.info("A IA atingiu o limite de tentativas. Reanalise automaticamente a solicita√ß√£o e os erros para gerar uma SQL corrigida.")
                    with col_reanalyze_btn:
                        trigger_reanalyze = st.button("ü§ù Reanalisar e corrigir SQL", key="reanalyze_after_fail", use_container_width=True)

                    if trigger_reanalyze:
                        with st.spinner("üîÅ Reanalisando erros e criando SQL efetiva..."):
                            analysis = reanalyze_failed_query(user_query, sql_query, st.session_state.execution_errors)

                        st.session_state.last_error_analysis = analysis

                        if analysis.get('fixed_sql'):
                            st.success(analysis.get('reason', 'SQL corrigida gerada.'))
                            if analysis.get('changes'):
                                st.caption("Ajustes aplicados: " + "; ".join(analysis['changes']))
                            
                            # Exibir SQL de forma colaps√°vel
                            fixed_sql = analysis['fixed_sql']
                            sql_lines = fixed_sql.split('\n')
                            preview_lines = sql_lines[:4]
                            preview_sql = '\n'.join(preview_lines)
                            
                            if len(sql_lines) <= 4:
                                st.code(fixed_sql, language="sql")
                            else:
                                st.code(preview_sql + '\n...', language="sql")
                                with st.expander("üìñ Ver SQL Completa", expanded=False):
                                    st.code(fixed_sql, language="sql")

                            # Preparar execu√ß√£o autom√°tica da SQL corrigida
                            st.session_state.sql_override = analysis['fixed_sql']
                            st.session_state.user_input_value = user_query
                            st.session_state.last_query = user_query
                            st.session_state.last_sql = analysis['fixed_sql']
                            st.session_state.execution_errors = []
                            st.session_state.auto_execute_from_assistant = True
                            st.info("A SQL corrigida ser√° executada automaticamente.")
                            st.rerun()
                        else:
                            st.warning(analysis.get('reason', 'N√£o foi poss√≠vel gerar uma SQL corrigida.'))

            elif st.session_state.agent_type == 'modular_prompt':
                # Display AI response for modular step
                st.markdown("### Resposta da IA - Etapa " + str(st.session_state.modular_config['current_step']))
                st.write(sql_query)

                # Show next step if available
                next_step = st.session_state.modular_config['current_step']
                if next_step < len(st.session_state.modular_config['contexts']) and st.session_state.modular_config['contexts'][next_step].strip():
                    st.info(f"Pr√≥xima etapa ({next_step + 1}): {st.session_state.modular_config['contexts'][next_step][:100]}...")
                else:
                    st.success("Todas as etapas foram conclu√≠das!")

            else:
                st.markdown('<div class="error-message">‚ùå Failed to execute query after all retries.</div>', unsafe_allow_html=True)


    # Display current results if available
    if st.session_state.current_df is not None and not execute_button:
        # Garantir que o bot√£o refinar esteja habilitado quando h√° resultados
        st.session_state.query_executed_successfully = True
        
        st.subheader(translations[lang]['current_results'])
        df = st.session_state.current_df
        user_query = st.session_state.last_query or "Previous query"
        
        # Exibir apenas tabela (an√°lises j√° foram mostradas durante execu√ß√£o)
        st.info("‚ÑπÔ∏è As an√°lises detalhadas j√° foram exibidas acima ap√≥s a execu√ß√£o da query.")
        st.dataframe(df, use_container_width=True)

        # Query Refinement Section for current results
        st.subheader("üîß " + translations[lang]['refine_query'])
        refine_input_current = st.text_input(
            translations[lang]['refine_instructions'],
            placeholder="e.g., Add filter by date, group by region, etc.",
            key="refine_input_current"
        )
        refine_query_button_current = st.button(translations[lang]['refine_query'], key="refine_current", use_container_width=True)

        if refine_query_button_current and refine_input_current:
            with st.spinner(translations[lang]['refining_query']):
                refined_sql = generate_refined_sql(user_query, df, refine_input_current)

            if refined_sql:
                st.markdown("### üîç SQL Refinada")
                # Exibir SQL de forma colaps√°vel
                sql_lines = refined_sql.split('\n')
                preview_lines = sql_lines[:4]
                preview_sql = '\n'.join(preview_lines)
                
                if len(sql_lines) <= 4:
                    st.code(refined_sql, language="sql")
                else:
                    st.code(preview_sql + '\n...', language="sql")
                    with st.expander("üìñ Ver SQL Completa", expanded=False):
                        st.code(refined_sql, language="sql")
                
                st.session_state.last_sql = refined_sql

                # Execute the refined query
                st.session_state.execution_errors = []
                df_refined = None
                for attempt in range(st.session_state.ai_config['max_retries']):
                    with st.spinner(f"‚ö° Executing refined query (attempt {attempt + 1})..."):
                        df_refined = execute_query(refined_sql)

                    if df_refined is not None:
                        break

                if df_refined is not None:
                    st.session_state.current_df = df_refined
                    st.session_state.query_history.append(f"Refined: {refine_input_current}")
                    update_last_query_context(f"{user_query} - Refined: {refine_input_current}", refined_sql, df_refined)

                    st.markdown('<div class="success-message">‚úÖ Refined query executed successfully!</div>', unsafe_allow_html=True)

                    if st.session_state.feature_flags.get('sampling_alerts', True):
                        alert = check_sampling_alert(refined_sql, df_refined)
                        if alert:
                            st.warning(alert)

                    # Update the display with refined results
                    st.rerun()
                elif df_refined is not None and df_refined.empty:
                    st.warning("‚ùå Query refinada executada, mas sem resultados.")
                    default_table = f"{st.session_state.connection_config.get('catalog', 'prd')}.{st.session_state.connection_config.get('schema', 'sand_snd_box_cartao')}.{st.session_state.ai_config.get('context_table', 'tb_pbi_autz_pampa_diar')}"
                    # OTIMIZA√á√ÉO: Usar cache de explora√ß√£o ao inv√©s de executar queries
                    metadata = st.session_state.table_exploration_cache.get(default_table, {})
                    if st.session_state.feature_flags.get('smart_suggestions', True):
                        st.info(generate_smart_suggestions_for_empty_result(refine_input_current, default_table, metadata))
                    update_last_query_context(f"{user_query} - Refined: {refine_input_current}", refined_sql, df_refined)

                else:
                    st.markdown('<div class="error-message">‚ùå Failed to execute refined query after all retries.</div>', unsafe_allow_html=True)

    # Handle Finish Process for Modular Prompt
    if 'finish_button' in locals() and finish_button:
        st.session_state.modular_config['current_step'] = 0
        st.session_state.modular_config['responses'] = []
        st.success("Processo modular finalizado!")
        st.rerun()

    # Clear results
    if clear_button:
        st.session_state.current_df = None
        st.session_state.last_query = ""
        st.session_state.last_sql = ""
        st.session_state.first_execution_attempted = False
        st.session_state.user_input_value = ""  # Limpar tamb√©m o campo de input
        st.session_state.refinement_history = []  # Limpar hist√≥rico de refinamentos
        st.session_state.assistant_mode = False  # Desativar modo assistente
        st.session_state.assistant_chat_history = []  # Limpar chat do assistente
        st.session_state.assistant_confidence = 0
        st.session_state.final_refined_query = ""
        st.session_state.show_refine_section = False  # Limpar se√ß√£o de refinamento
        st.session_state.query_executed_successfully = False  # Resetar flag de execu√ß√£o
        st.session_state.analysis_scores = {}  # Limpar scores de an√°lise
        st.session_state.execution_errors = []  # Limpar erros armazenados
        st.session_state.sql_override = ""  # Limpar SQL corrigida pendente
        st.session_state.last_error_analysis = {}
        st.session_state.table_exploration_cache = {}  # Limpar cache de explora√ß√£o
        st.session_state.auto_exploration_done = False  # Resetar flag de explora√ß√£o
        if st.session_state.agent_type == 'modular_prompt':
            st.session_state.modular_config['current_step'] = 0
            st.session_state.modular_config['responses'] = []
        st.rerun()

    # Footer
    st.markdown("---")
    st.markdown(translations[lang]['footer'])

if __name__ == "__main__":
    main()
